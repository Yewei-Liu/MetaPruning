[2025-04-27 17:30:03,744][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index:
- 10
- 20
- 30
- 40

[2025-04-27 17:30:03,872][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:30:03,872][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:30:03,872][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:30:14,591][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 17:30:26,024][train][INFO] - Epoch 1/100, Val Acc=0.4532, Val Loss=1.9536, lr=0.0100
[2025-04-27 17:30:37,831][train][INFO] - Epoch 2/100, Val Acc=0.6632, Val Loss=1.0106, lr=0.0100
[2025-04-27 17:30:48,133][train][INFO] - Epoch 3/100, Val Acc=0.7501, Val Loss=0.7448, lr=0.0100
[2025-04-27 17:31:00,481][train][INFO] - Epoch 4/100, Val Acc=0.7137, Val Loss=0.8998, lr=0.0100
[2025-04-27 17:31:12,449][train][INFO] - Epoch 5/100, Val Acc=0.7853, Val Loss=0.6651, lr=0.0100
[2025-04-27 17:31:25,375][train][INFO] - Epoch 6/100, Val Acc=0.7517, Val Loss=0.7408, lr=0.0100
[2025-04-27 17:31:56,773][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 10

[2025-04-27 17:31:56,898][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:31:56,899][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:31:56,899][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:02,456][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 20

[2025-04-27 17:32:02,595][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:02,595][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:02,595][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:32:05,715][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 30

[2025-04-27 17:32:05,856][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:05,857][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:05,857][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:32:07,278][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:09,392][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 40

[2025-04-27 17:32:09,538][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:09,539][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:09,539][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:13,337][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 17:32:17,469][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:19,791][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 15

[2025-04-27 17:32:19,953][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:19,953][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:19,953][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:32:22,175][train][INFO] - Epoch 1/100, Val Acc=0.4170, Val Loss=2.1906, lr=0.0100
[2025-04-27 17:32:23,399][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 25

[2025-04-27 17:32:23,563][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:23,563][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:23,564][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:32:26,653][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 17:32:28,278][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 35

[2025-04-27 17:32:28,426][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:28,426][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:28,426][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:29,559][train][INFO] - Epoch 1/100, Val Acc=0.4138, Val Loss=2.0898, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:32,398][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 5

[2025-04-27 17:32:32,552][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:32:32,552][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:32:32,553][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:32:34,769][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 17:32:35,503][train][INFO] - Epoch 1/100, Val Acc=0.3620, Val Loss=1.7415, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 17:32:37,311][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Error executing job with overrides: ['index=5']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 117, in main
    metanetwork = load_metanetwork(index)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 103, in load_metanetwork
    raise ValueError(f"More than one metanetwork found with index {index}")
ValueError: More than one metanetwork found with index 5

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 17:32:41,114][train][INFO] - Epoch 2/100, Val Acc=0.6714, Val Loss=0.9429, lr=0.0100
[2025-04-27 17:32:42,947][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 17:32:46,539][train][INFO] - Epoch 1/100, Val Acc=0.2125, Val Loss=2.6794, lr=0.0100
[2025-04-27 17:32:48,530][train][INFO] - Epoch 2/100, Val Acc=0.5438, Val Loss=1.5330, lr=0.0100
[2025-04-27 17:32:48,907][train][INFO] - Epoch 1/100, Val Acc=0.4845, Val Loss=1.5017, lr=0.0100
[2025-04-27 17:32:52,101][train][INFO] - Epoch 1/100, Val Acc=0.3019, Val Loss=2.5306, lr=0.0100
[2025-04-27 17:32:54,540][train][INFO] - Epoch 2/100, Val Acc=0.3995, Val Loss=1.8009, lr=0.0100
[2025-04-27 17:32:58,819][train][INFO] - Epoch 1/100, Val Acc=0.3361, Val Loss=1.8299, lr=0.0100
[2025-04-27 17:33:00,661][train][INFO] - Epoch 3/100, Val Acc=0.7329, Val Loss=0.7967, lr=0.0100
[2025-04-27 17:33:03,590][train][INFO] - Epoch 2/100, Val Acc=0.5129, Val Loss=1.7437, lr=0.0100
[2025-04-27 17:33:05,629][train][INFO] - Epoch 2/100, Val Acc=0.4543, Val Loss=1.4666, lr=0.0100
[2025-04-27 17:33:07,186][train][INFO] - Epoch 2/100, Val Acc=0.5024, Val Loss=1.5702, lr=0.0100
[2025-04-27 17:33:07,718][train][INFO] - Epoch 3/100, Val Acc=0.6047, Val Loss=1.2382, lr=0.0100
[2025-04-27 17:33:13,524][train][INFO] - Epoch 3/100, Val Acc=0.5609, Val Loss=1.2559, lr=0.0100
[2025-04-27 17:33:14,160][train][INFO] - Epoch 2/100, Val Acc=0.4155, Val Loss=1.7553, lr=0.0100
[2025-04-27 17:33:18,385][train][INFO] - Epoch 3/100, Val Acc=0.5294, Val Loss=1.7108, lr=0.0100
[2025-04-27 17:33:20,162][train][INFO] - Epoch 4/100, Val Acc=0.7599, Val Loss=0.7195, lr=0.0100
[2025-04-27 17:33:22,393][train][INFO] - Epoch 3/100, Val Acc=0.5192, Val Loss=1.6866, lr=0.0100
[2025-04-27 17:33:24,739][train][INFO] - Epoch 3/100, Val Acc=0.4674, Val Loss=1.5731, lr=0.0100
[2025-04-27 17:33:26,877][train][INFO] - Epoch 4/100, Val Acc=0.6711, Val Loss=0.9941, lr=0.0100
[2025-04-27 17:33:29,609][train][INFO] - Epoch 3/100, Val Acc=0.4579, Val Loss=1.6233, lr=0.0100
[2025-04-27 17:33:32,566][train][INFO] - Epoch 4/100, Val Acc=0.6293, Val Loss=1.0528, lr=0.0100
[2025-04-27 17:33:33,172][train][INFO] - Epoch 4/100, Val Acc=0.6789, Val Loss=0.9909, lr=0.0100
[2025-04-27 17:33:37,692][train][INFO] - Epoch 4/100, Val Acc=0.5446, Val Loss=1.5895, lr=0.0100
[2025-04-27 17:33:39,538][train][INFO] - Epoch 5/100, Val Acc=0.7776, Val Loss=0.6541, lr=0.0100
[2025-04-27 17:33:43,889][train][INFO] - Epoch 4/100, Val Acc=0.5100, Val Loss=1.5469, lr=0.0100
[2025-04-27 17:33:45,083][train][INFO] - Epoch 4/100, Val Acc=0.6215, Val Loss=1.1233, lr=0.0100
[2025-04-27 17:33:45,926][train][INFO] - Epoch 5/100, Val Acc=0.7436, Val Loss=0.7340, lr=0.0100
[2025-04-27 17:33:48,069][train][INFO] - Epoch 5/100, Val Acc=0.6794, Val Loss=1.0092, lr=0.0100
[2025-04-27 17:33:51,551][train][INFO] - Epoch 5/100, Val Acc=0.6435, Val Loss=1.0279, lr=0.0100
[2025-04-27 17:33:52,814][train][INFO] - Epoch 5/100, Val Acc=0.6451, Val Loss=1.1634, lr=0.0100
[2025-04-27 17:33:59,032][train][INFO] - Epoch 6/100, Val Acc=0.7877, Val Loss=0.6360, lr=0.0100
[2025-04-27 17:34:00,602][train][INFO] - Epoch 5/100, Val Acc=0.5233, Val Loss=1.4846, lr=0.0100
[2025-04-27 17:34:02,986][train][INFO] - Epoch 5/100, Val Acc=0.5906, Val Loss=1.1759, lr=0.0100
[2025-04-27 17:34:03,033][train][INFO] - Epoch 6/100, Val Acc=0.7636, Val Loss=0.7007, lr=0.0100
[2025-04-27 17:34:04,926][train][INFO] - Epoch 6/100, Val Acc=0.7242, Val Loss=0.8166, lr=0.0100
[2025-04-27 17:34:07,997][train][INFO] - Epoch 6/100, Val Acc=0.7614, Val Loss=0.6905, lr=0.0100
[2025-04-27 17:34:10,589][train][INFO] - Epoch 6/100, Val Acc=0.6769, Val Loss=0.9155, lr=0.0100
[2025-04-27 17:34:15,978][train][INFO] - Epoch 6/100, Val Acc=0.6133, Val Loss=1.1533, lr=0.0100
[2025-04-27 17:34:17,874][train][INFO] - Epoch 7/100, Val Acc=0.7469, Val Loss=0.7745, lr=0.0100
[2025-04-27 17:34:18,483][train][INFO] - Epoch 7/100, Val Acc=0.7836, Val Loss=0.6620, lr=0.0100
[2025-04-27 17:34:22,061][train][INFO] - Epoch 6/100, Val Acc=0.6061, Val Loss=1.1780, lr=0.0100
[2025-04-27 17:34:23,212][train][INFO] - Epoch 7/100, Val Acc=0.7275, Val Loss=0.8092, lr=0.0100
[2025-04-27 17:34:24,125][train][INFO] - Epoch 7/100, Val Acc=0.7760, Val Loss=0.6757, lr=0.0100
[2025-04-27 17:34:29,588][train][INFO] - Epoch 7/100, Val Acc=0.6141, Val Loss=1.2162, lr=0.0100
[2025-04-27 17:34:31,379][train][INFO] - Epoch 7/100, Val Acc=0.6990, Val Loss=0.8778, lr=0.0100
[2025-04-27 17:34:32,805][train][INFO] - Epoch 8/100, Val Acc=0.7589, Val Loss=0.7480, lr=0.0100
[2025-04-27 17:34:37,910][train][INFO] - Epoch 8/100, Val Acc=0.7727, Val Loss=0.7345, lr=0.0100
[2025-04-27 17:34:38,105][train][INFO] - Epoch 8/100, Val Acc=0.6616, Val Loss=1.1837, lr=0.0100
[2025-04-27 17:34:41,179][train][INFO] - Epoch 7/100, Val Acc=0.6022, Val Loss=1.1723, lr=0.0100
[2025-04-27 17:34:43,118][train][INFO] - Epoch 8/100, Val Acc=0.7449, Val Loss=0.8173, lr=0.0100
[2025-04-27 17:34:47,011][train][INFO] - Epoch 8/100, Val Acc=0.5934, Val Loss=1.3360, lr=0.0100
[2025-04-27 17:34:47,642][train][INFO] - Epoch 9/100, Val Acc=0.7998, Val Loss=0.5953, lr=0.0100
[2025-04-27 17:34:48,666][train][INFO] - Epoch 8/100, Val Acc=0.6558, Val Loss=1.0088, lr=0.0100
[2025-04-27 17:34:53,498][train][INFO] - Epoch 9/100, Val Acc=0.7720, Val Loss=0.6746, lr=0.0100
[2025-04-27 17:34:57,440][train][INFO] - Epoch 9/100, Val Acc=0.7791, Val Loss=0.7001, lr=0.0100
[2025-04-27 17:35:00,260][train][INFO] - Epoch 8/100, Val Acc=0.6658, Val Loss=1.0108, lr=0.0100
[2025-04-27 17:35:02,243][train][INFO] - Epoch 9/100, Val Acc=0.7627, Val Loss=0.7729, lr=0.0100
[2025-04-27 17:35:02,604][train][INFO] - Epoch 9/100, Val Acc=0.5777, Val Loss=1.4231, lr=0.0100
[2025-04-27 17:35:02,935][train][INFO] - Epoch 10/100, Val Acc=0.7624, Val Loss=0.7006, lr=0.0100
[2025-04-27 17:35:07,714][train][INFO] - Epoch 9/100, Val Acc=0.6835, Val Loss=0.9264, lr=0.0100
[2025-04-27 17:35:08,641][train][INFO] - Epoch 10/100, Val Acc=0.7705, Val Loss=0.6694, lr=0.0100
[2025-04-27 17:35:16,873][train][INFO] - Epoch 10/100, Val Acc=0.8161, Val Loss=0.5594, lr=0.0100
[2025-04-27 17:35:18,197][train][INFO] - Epoch 10/100, Val Acc=0.6973, Val Loss=0.8910, lr=0.0100
[2025-04-27 17:35:18,205][train][INFO] - Epoch 11/100, Val Acc=0.8052, Val Loss=0.5800, lr=0.0100
[2025-04-27 17:35:19,360][train][INFO] - Epoch 9/100, Val Acc=0.6841, Val Loss=0.9175, lr=0.0100
[2025-04-27 17:35:21,364][train][INFO] - Epoch 10/100, Val Acc=0.8253, Val Loss=0.5027, lr=0.0100
[2025-04-27 17:35:23,827][train][INFO] - Epoch 11/100, Val Acc=0.6470, Val Loss=1.1990, lr=0.0100
[2025-04-27 17:35:26,826][train][INFO] - Epoch 10/100, Val Acc=0.7144, Val Loss=0.8610, lr=0.0100
[2025-04-27 17:35:28,636][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 5

[2025-04-27 17:35:28,791][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:35:28,791][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:35:28,792][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:35:33,324][train][INFO] - Epoch 12/100, Val Acc=0.8005, Val Loss=0.5701, lr=0.0100
[2025-04-27 17:35:33,885][train][INFO] - Epoch 11/100, Val Acc=0.7438, Val Loss=0.7275, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Error executing job with overrides: ['index=5']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 117, in main
    metanetwork = load_metanetwork(index)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 103, in load_metanetwork
    raise ValueError(f"More than one metanetwork found with index {index}")
ValueError: More than one metanetwork found with index 5

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 17:35:36,380][train][INFO] - Epoch 11/100, Val Acc=0.8254, Val Loss=0.5160, lr=0.0100
[2025-04-27 17:35:38,444][train][INFO] - Epoch 10/100, Val Acc=0.6056, Val Loss=1.4249, lr=0.0100
[2025-04-27 17:35:39,218][train][INFO] - Epoch 12/100, Val Acc=0.7938, Val Loss=0.6154, lr=0.0100
[2025-04-27 17:35:40,518][train][INFO] - Epoch 11/100, Val Acc=0.7897, Val Loss=0.6233, lr=0.0100
[2025-04-27 17:35:46,045][train][INFO] - Epoch 11/100, Val Acc=0.7330, Val Loss=0.8406, lr=0.0100
[2025-04-27 17:35:48,393][train][INFO] - Epoch 13/100, Val Acc=0.7921, Val Loss=0.6591, lr=0.0100
[2025-04-27 17:35:49,529][train][INFO] - Epoch 12/100, Val Acc=0.7417, Val Loss=0.7510, lr=0.0100
[2025-04-27 17:35:54,400][train][INFO] - Epoch 13/100, Val Acc=0.7265, Val Loss=0.8373, lr=0.0100
[2025-04-27 17:35:55,896][train][INFO] - Epoch 12/100, Val Acc=0.8351, Val Loss=0.5097, lr=0.0100
[2025-04-27 17:35:57,575][train][INFO] - Epoch 11/100, Val Acc=0.7393, Val Loss=0.7465, lr=0.0100
[2025-04-27 17:35:59,689][train][INFO] - Epoch 12/100, Val Acc=0.8165, Val Loss=0.5593, lr=0.0100
[2025-04-27 17:36:03,396][train][INFO] - Epoch 14/100, Val Acc=0.8035, Val Loss=0.5953, lr=0.0100
[2025-04-27 17:36:04,954][train][INFO] - Epoch 13/100, Val Acc=0.7455, Val Loss=0.7566, lr=0.0100
[2025-04-27 17:36:05,110][train][INFO] - Epoch 12/100, Val Acc=0.7416, Val Loss=0.7739, lr=0.0100
[2025-04-27 17:36:09,675][train][INFO] - Epoch 14/100, Val Acc=0.7690, Val Loss=0.7288, lr=0.0100
[2025-04-27 17:36:15,342][train][INFO] - Epoch 13/100, Val Acc=0.8455, Val Loss=0.4758, lr=0.0100
[2025-04-27 17:36:16,619][train][INFO] - Epoch 12/100, Val Acc=0.7259, Val Loss=0.8096, lr=0.0100
[2025-04-27 17:36:18,045][train][INFO] - Epoch 15/100, Val Acc=0.8408, Val Loss=0.4670, lr=0.0100
[2025-04-27 17:36:18,836][train][INFO] - Epoch 13/100, Val Acc=0.8328, Val Loss=0.5047, lr=0.0100
[2025-04-27 17:36:20,587][train][INFO] - Epoch 14/100, Val Acc=0.7572, Val Loss=0.7217, lr=0.0100
[2025-04-27 17:36:24,196][train][INFO] - Epoch 13/100, Val Acc=0.7212, Val Loss=0.8584, lr=0.0100
[2025-04-27 17:36:24,975][train][INFO] - Epoch 15/100, Val Acc=0.7875, Val Loss=0.6155, lr=0.0100
[2025-04-27 17:36:32,901][train][INFO] - Epoch 16/100, Val Acc=0.8342, Val Loss=0.5100, lr=0.0100
[2025-04-27 17:36:34,892][train][INFO] - Epoch 14/100, Val Acc=0.8444, Val Loss=0.4736, lr=0.0100
[2025-04-27 17:36:35,926][train][INFO] - Epoch 13/100, Val Acc=0.7618, Val Loss=0.6890, lr=0.0100
[2025-04-27 17:36:36,186][train][INFO] - Epoch 15/100, Val Acc=0.7544, Val Loss=0.7257, lr=0.0100
[2025-04-27 17:36:37,945][train][INFO] - Epoch 14/100, Val Acc=0.8184, Val Loss=0.5301, lr=0.0100
[2025-04-27 17:36:40,386][train][INFO] - Epoch 16/100, Val Acc=0.7685, Val Loss=0.7628, lr=0.0100
[2025-04-27 17:36:43,312][train][INFO] - Epoch 14/100, Val Acc=0.7467, Val Loss=0.7913, lr=0.0100
[2025-04-27 17:36:47,850][train][INFO] - Epoch 17/100, Val Acc=0.8276, Val Loss=0.5156, lr=0.0100
[2025-04-27 17:36:51,635][train][INFO] - Epoch 16/100, Val Acc=0.7276, Val Loss=0.7959, lr=0.0100
[2025-04-27 17:36:54,394][train][INFO] - Epoch 15/100, Val Acc=0.8235, Val Loss=0.5157, lr=0.0100
[2025-04-27 17:36:55,110][train][INFO] - Epoch 14/100, Val Acc=0.6761, Val Loss=1.0170, lr=0.0100
[2025-04-27 17:36:55,699][train][INFO] - Epoch 17/100, Val Acc=0.8303, Val Loss=0.5045, lr=0.0100
[2025-04-27 17:36:57,063][train][INFO] - Epoch 15/100, Val Acc=0.8445, Val Loss=0.4548, lr=0.0100
[2025-04-27 17:37:02,371][train][INFO] - Epoch 15/100, Val Acc=0.7854, Val Loss=0.6369, lr=0.0100
[2025-04-27 17:37:02,703][train][INFO] - Epoch 18/100, Val Acc=0.8249, Val Loss=0.5236, lr=0.0100
[2025-04-27 17:37:07,247][train][INFO] - Epoch 17/100, Val Acc=0.7021, Val Loss=0.8887, lr=0.0100
[2025-04-27 17:37:10,863][train][INFO] - Epoch 18/100, Val Acc=0.8270, Val Loss=0.5320, lr=0.0100
[2025-04-27 17:37:11,649][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
  meta_train:
    epochs: 60
    lr: 0.001
    lr_decay_milestones: '12'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Lewis
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
index: 5

[2025-04-27 17:37:11,799][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 17:37:11,799][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 17:37:11,799][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 17:37:14,054][train][INFO] - Epoch 16/100, Val Acc=0.8532, Val Loss=0.4412, lr=0.0100
[2025-04-27 17:37:14,366][train][INFO] - Epoch 15/100, Val Acc=0.7721, Val Loss=0.6879, lr=0.0100
[2025-04-27 17:37:16,284][train][INFO] - Epoch 16/100, Val Acc=0.7805, Val Loss=0.6857, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Error executing job with overrides: ['index=5']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 117, in main
    metanetwork = load_metanetwork(index)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 103, in load_metanetwork
    raise ValueError(f"More than one metanetwork found with index {index}")
ValueError: More than one metanetwork found with index 5

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 17:37:17,679][train][INFO] - Epoch 19/100, Val Acc=0.8197, Val Loss=0.5562, lr=0.0100
[2025-04-27 17:37:21,473][train][INFO] - Epoch 16/100, Val Acc=0.7445, Val Loss=0.7995, lr=0.0100
[2025-04-27 17:37:23,015][train][INFO] - Epoch 18/100, Val Acc=0.7582, Val Loss=0.7271, lr=0.0100
[2025-04-27 17:37:26,072][train][INFO] - Epoch 19/100, Val Acc=0.8120, Val Loss=0.5686, lr=0.0100
[2025-04-27 17:37:32,363][train][INFO] - Epoch 20/100, Val Acc=0.8442, Val Loss=0.4741, lr=0.0100
[2025-04-27 17:37:33,766][train][INFO] - Epoch 17/100, Val Acc=0.8264, Val Loss=0.5583, lr=0.0100
[2025-04-27 17:37:33,783][train][INFO] - Epoch 16/100, Val Acc=0.7402, Val Loss=0.8090, lr=0.0100
[2025-04-27 17:37:35,520][train][INFO] - Epoch 17/100, Val Acc=0.8529, Val Loss=0.4403, lr=0.0100
[2025-04-27 17:37:38,717][train][INFO] - Epoch 19/100, Val Acc=0.7821, Val Loss=0.6355, lr=0.0100
[2025-04-27 17:37:40,647][train][INFO] - Epoch 17/100, Val Acc=0.7551, Val Loss=0.7294, lr=0.0100
[2025-04-27 17:37:41,370][train][INFO] - Epoch 20/100, Val Acc=0.8260, Val Loss=0.5158, lr=0.0100
[2025-04-27 17:37:47,556][train][INFO] - Epoch 21/100, Val Acc=0.8296, Val Loss=0.5239, lr=0.0100
[2025-04-27 17:37:53,289][train][INFO] - Epoch 17/100, Val Acc=0.7342, Val Loss=0.8073, lr=0.0100
[2025-04-27 17:37:53,421][train][INFO] - Epoch 18/100, Val Acc=0.8411, Val Loss=0.4979, lr=0.0100
[2025-04-27 17:37:54,271][train][INFO] - Epoch 20/100, Val Acc=0.7873, Val Loss=0.6179, lr=0.0100
[2025-04-27 17:37:54,848][train][INFO] - Epoch 18/100, Val Acc=0.8232, Val Loss=0.5259, lr=0.0100
[2025-04-27 17:37:56,574][train][INFO] - Epoch 21/100, Val Acc=0.8218, Val Loss=0.5355, lr=0.0100
[2025-04-27 17:37:59,798][train][INFO] - Epoch 18/100, Val Acc=0.7601, Val Loss=0.7174, lr=0.0100
[2025-04-27 17:38:02,630][train][INFO] - Epoch 22/100, Val Acc=0.8464, Val Loss=0.4637, lr=0.0100
[2025-04-27 17:38:09,738][train][INFO] - Epoch 21/100, Val Acc=0.7864, Val Loss=0.6281, lr=0.0100
[2025-04-27 17:38:11,761][train][INFO] - Epoch 22/100, Val Acc=0.8215, Val Loss=0.5289, lr=0.0100
[2025-04-27 17:38:12,555][train][INFO] - Epoch 18/100, Val Acc=0.7519, Val Loss=0.7753, lr=0.0100
[2025-04-27 17:38:13,025][train][INFO] - Epoch 19/100, Val Acc=0.8650, Val Loss=0.4074, lr=0.0100
[2025-04-27 17:38:14,159][train][INFO] - Epoch 19/100, Val Acc=0.8566, Val Loss=0.4395, lr=0.0100
[2025-04-27 17:38:17,643][train][INFO] - Epoch 23/100, Val Acc=0.8506, Val Loss=0.4481, lr=0.0100
[2025-04-27 17:38:18,868][train][INFO] - Epoch 19/100, Val Acc=0.7109, Val Loss=0.9186, lr=0.0100
[2025-04-27 17:38:25,269][train][INFO] - Epoch 22/100, Val Acc=0.7778, Val Loss=0.6661, lr=0.0100
[2025-04-27 17:38:26,952][train][INFO] - Epoch 23/100, Val Acc=0.8486, Val Loss=0.4437, lr=0.0100
[2025-04-27 17:38:31,755][train][INFO] - Epoch 19/100, Val Acc=0.7412, Val Loss=0.7623, lr=0.0100
[2025-04-27 17:38:32,641][train][INFO] - Epoch 20/100, Val Acc=0.8578, Val Loss=0.4295, lr=0.0100
[2025-04-27 17:38:32,679][train][INFO] - Epoch 24/100, Val Acc=0.8510, Val Loss=0.4357, lr=0.0100
[2025-04-27 17:38:33,546][train][INFO] - Epoch 20/100, Val Acc=0.8305, Val Loss=0.4984, lr=0.0100
[2025-04-27 17:38:38,063][train][INFO] - Epoch 20/100, Val Acc=0.7906, Val Loss=0.6407, lr=0.0100
[2025-04-27 17:38:40,706][train][INFO] - Epoch 23/100, Val Acc=0.7757, Val Loss=0.6643, lr=0.0100
[2025-04-27 17:38:42,179][train][INFO] - Epoch 24/100, Val Acc=0.8406, Val Loss=0.4703, lr=0.0100
[2025-04-27 17:38:47,538][train][INFO] - Epoch 25/100, Val Acc=0.8357, Val Loss=0.4960, lr=0.0100
[2025-04-27 17:38:51,112][train][INFO] - Epoch 20/100, Val Acc=0.7396, Val Loss=0.7919, lr=0.0100
[2025-04-27 17:38:52,305][train][INFO] - Epoch 21/100, Val Acc=0.8634, Val Loss=0.4108, lr=0.0100
[2025-04-27 17:38:52,701][train][INFO] - Epoch 21/100, Val Acc=0.8310, Val Loss=0.5163, lr=0.0100
[2025-04-27 17:38:56,135][train][INFO] - Epoch 24/100, Val Acc=0.7844, Val Loss=0.6509, lr=0.0100
[2025-04-27 17:38:57,042][train][INFO] - Epoch 21/100, Val Acc=0.7964, Val Loss=0.6003, lr=0.0100
[2025-04-27 17:38:57,691][train][INFO] - Epoch 25/100, Val Acc=0.8433, Val Loss=0.4755, lr=0.0100
[2025-04-27 17:39:02,861][train][INFO] - Epoch 26/100, Val Acc=0.8410, Val Loss=0.4863, lr=0.0100
[2025-04-27 17:39:10,333][train][INFO] - Epoch 21/100, Val Acc=0.7948, Val Loss=0.5962, lr=0.0100
[2025-04-27 17:39:11,991][train][INFO] - Epoch 22/100, Val Acc=0.8715, Val Loss=0.3804, lr=0.0100
[2025-04-27 17:39:12,062][train][INFO] - Epoch 22/100, Val Acc=0.8368, Val Loss=0.4844, lr=0.0100
[2025-04-27 17:39:13,669][train][INFO] - Epoch 25/100, Val Acc=0.7465, Val Loss=0.7455, lr=0.0100
[2025-04-27 17:39:15,255][train][INFO] - Epoch 26/100, Val Acc=0.8380, Val Loss=0.4972, lr=0.0100
[2025-04-27 17:39:16,146][train][INFO] - Epoch 22/100, Val Acc=0.8026, Val Loss=0.6006, lr=0.0100
[2025-04-27 17:39:21,249][train][INFO] - Epoch 27/100, Val Acc=0.8446, Val Loss=0.4878, lr=0.0100
[2025-04-27 17:39:29,564][train][INFO] - Epoch 22/100, Val Acc=0.7748, Val Loss=0.6530, lr=0.0100
[2025-04-27 17:39:31,599][train][INFO] - Epoch 23/100, Val Acc=0.8437, Val Loss=0.4726, lr=0.0100
[2025-04-27 17:39:31,767][train][INFO] - Epoch 23/100, Val Acc=0.8414, Val Loss=0.4779, lr=0.0100
[2025-04-27 17:39:33,074][train][INFO] - Epoch 26/100, Val Acc=0.7694, Val Loss=0.7006, lr=0.0100
[2025-04-27 17:39:34,584][train][INFO] - Epoch 27/100, Val Acc=0.8185, Val Loss=0.5517, lr=0.0100
[2025-04-27 17:39:35,187][train][INFO] - Epoch 23/100, Val Acc=0.8007, Val Loss=0.6012, lr=0.0100
[2025-04-27 17:39:40,243][train][INFO] - Epoch 28/100, Val Acc=0.8674, Val Loss=0.3980, lr=0.0100
[2025-04-27 17:39:48,805][train][INFO] - Epoch 23/100, Val Acc=0.7719, Val Loss=0.6820, lr=0.0100
[2025-04-27 17:39:50,939][train][INFO] - Epoch 24/100, Val Acc=0.8339, Val Loss=0.5257, lr=0.0100
[2025-04-27 17:39:51,455][train][INFO] - Epoch 24/100, Val Acc=0.8477, Val Loss=0.4873, lr=0.0100
[2025-04-27 17:39:52,513][train][INFO] - Epoch 27/100, Val Acc=0.7673, Val Loss=0.6811, lr=0.0100
[2025-04-27 17:39:53,943][train][INFO] - Epoch 28/100, Val Acc=0.8248, Val Loss=0.5399, lr=0.0100
[2025-04-27 17:39:54,343][train][INFO] - Epoch 24/100, Val Acc=0.7650, Val Loss=0.7169, lr=0.0100
[2025-04-27 17:39:59,273][train][INFO] - Epoch 29/100, Val Acc=0.8546, Val Loss=0.4350, lr=0.0100
[2025-04-27 17:40:08,000][train][INFO] - Epoch 24/100, Val Acc=0.7855, Val Loss=0.6280, lr=0.0100
[2025-04-27 17:40:10,199][train][INFO] - Epoch 25/100, Val Acc=0.8466, Val Loss=0.4580, lr=0.0100
[2025-04-27 17:40:11,066][train][INFO] - Epoch 25/100, Val Acc=0.8334, Val Loss=0.4934, lr=0.0100
[2025-04-27 17:40:12,047][train][INFO] - Epoch 28/100, Val Acc=0.7668, Val Loss=0.7321, lr=0.0100
[2025-04-27 17:40:13,346][train][INFO] - Epoch 29/100, Val Acc=0.8579, Val Loss=0.4264, lr=0.0100
[2025-04-27 17:40:13,534][train][INFO] - Epoch 25/100, Val Acc=0.8130, Val Loss=0.5723, lr=0.0100
[2025-04-27 17:40:18,316][train][INFO] - Epoch 30/100, Val Acc=0.8593, Val Loss=0.4183, lr=0.0100
[2025-04-27 17:40:27,176][train][INFO] - Epoch 25/100, Val Acc=0.7913, Val Loss=0.6312, lr=0.0100
[2025-04-27 17:40:29,445][train][INFO] - Epoch 26/100, Val Acc=0.8303, Val Loss=0.5248, lr=0.0100
[2025-04-27 17:40:30,695][train][INFO] - Epoch 26/100, Val Acc=0.8728, Val Loss=0.3900, lr=0.0100
[2025-04-27 17:40:31,527][train][INFO] - Epoch 29/100, Val Acc=0.7666, Val Loss=0.7149, lr=0.0100
[2025-04-27 17:40:32,648][train][INFO] - Epoch 26/100, Val Acc=0.8104, Val Loss=0.5594, lr=0.0100
[2025-04-27 17:40:32,799][train][INFO] - Epoch 30/100, Val Acc=0.8464, Val Loss=0.4618, lr=0.0100
[2025-04-27 17:40:37,315][train][INFO] - Epoch 31/100, Val Acc=0.8531, Val Loss=0.4596, lr=0.0100
[2025-04-27 17:40:46,360][train][INFO] - Epoch 26/100, Val Acc=0.7814, Val Loss=0.6669, lr=0.0100
[2025-04-27 17:40:48,640][train][INFO] - Epoch 27/100, Val Acc=0.8495, Val Loss=0.4558, lr=0.0100
[2025-04-27 17:40:50,316][train][INFO] - Epoch 27/100, Val Acc=0.8539, Val Loss=0.4461, lr=0.0100
[2025-04-27 17:40:51,211][train][INFO] - Epoch 30/100, Val Acc=0.7942, Val Loss=0.6150, lr=0.0100
[2025-04-27 17:40:51,828][train][INFO] - Epoch 27/100, Val Acc=0.8287, Val Loss=0.5227, lr=0.0100
[2025-04-27 17:40:52,261][train][INFO] - Epoch 31/100, Val Acc=0.8398, Val Loss=0.5074, lr=0.0100
[2025-04-27 17:40:56,289][train][INFO] - Epoch 32/100, Val Acc=0.8658, Val Loss=0.4016, lr=0.0100
[2025-04-27 17:41:05,648][train][INFO] - Epoch 27/100, Val Acc=0.7873, Val Loss=0.6253, lr=0.0100
[2025-04-27 17:41:07,836][train][INFO] - Epoch 28/100, Val Acc=0.8544, Val Loss=0.4553, lr=0.0100
[2025-04-27 17:41:09,925][train][INFO] - Epoch 28/100, Val Acc=0.8531, Val Loss=0.4529, lr=0.0100
[2025-04-27 17:41:10,725][train][INFO] - Epoch 31/100, Val Acc=0.7956, Val Loss=0.6138, lr=0.0100
[2025-04-27 17:41:11,040][train][INFO] - Epoch 28/100, Val Acc=0.8199, Val Loss=0.5323, lr=0.0100
[2025-04-27 17:41:11,669][train][INFO] - Epoch 32/100, Val Acc=0.8280, Val Loss=0.5455, lr=0.0100
[2025-04-27 17:41:15,277][train][INFO] - Epoch 33/100, Val Acc=0.8510, Val Loss=0.4456, lr=0.0100
[2025-04-27 17:41:24,922][train][INFO] - Epoch 28/100, Val Acc=0.7911, Val Loss=0.6474, lr=0.0100
[2025-04-27 17:41:27,075][train][INFO] - Epoch 29/100, Val Acc=0.8329, Val Loss=0.5301, lr=0.0100
[2025-04-27 17:41:29,582][train][INFO] - Epoch 29/100, Val Acc=0.8382, Val Loss=0.5121, lr=0.0100
[2025-04-27 17:41:30,240][train][INFO] - Epoch 32/100, Val Acc=0.7794, Val Loss=0.6573, lr=0.0100
[2025-04-27 17:41:30,376][train][INFO] - Epoch 29/100, Val Acc=0.7733, Val Loss=0.6993, lr=0.0100
[2025-04-27 17:41:31,070][train][INFO] - Epoch 33/100, Val Acc=0.8601, Val Loss=0.4204, lr=0.0100
[2025-04-27 17:41:34,286][train][INFO] - Epoch 34/100, Val Acc=0.8640, Val Loss=0.4157, lr=0.0100
[2025-04-27 17:41:44,189][train][INFO] - Epoch 29/100, Val Acc=0.7792, Val Loss=0.6860, lr=0.0100
[2025-04-27 17:41:46,370][train][INFO] - Epoch 30/100, Val Acc=0.8453, Val Loss=0.4676, lr=0.0100
[2025-04-27 17:41:49,166][train][INFO] - Epoch 30/100, Val Acc=0.8725, Val Loss=0.3930, lr=0.0100
[2025-04-27 17:41:49,637][train][INFO] - Epoch 30/100, Val Acc=0.8030, Val Loss=0.5823, lr=0.0100
[2025-04-27 17:41:49,757][train][INFO] - Epoch 33/100, Val Acc=0.8005, Val Loss=0.5892, lr=0.0100
[2025-04-27 17:41:50,502][train][INFO] - Epoch 34/100, Val Acc=0.8493, Val Loss=0.4745, lr=0.0100
[2025-04-27 17:41:53,263][train][INFO] - Epoch 35/100, Val Acc=0.8558, Val Loss=0.4481, lr=0.0100
[2025-04-27 17:42:03,416][train][INFO] - Epoch 30/100, Val Acc=0.7935, Val Loss=0.6169, lr=0.0100
[2025-04-27 17:42:05,712][train][INFO] - Epoch 31/100, Val Acc=0.8609, Val Loss=0.4308, lr=0.0100
[2025-04-27 17:42:08,815][train][INFO] - Epoch 31/100, Val Acc=0.8681, Val Loss=0.3943, lr=0.0100
[2025-04-27 17:42:09,035][train][INFO] - Epoch 31/100, Val Acc=0.8088, Val Loss=0.5918, lr=0.0100
[2025-04-27 17:42:09,268][train][INFO] - Epoch 34/100, Val Acc=0.7679, Val Loss=0.7413, lr=0.0100
[2025-04-27 17:42:10,028][train][INFO] - Epoch 35/100, Val Acc=0.8406, Val Loss=0.4766, lr=0.0100
[2025-04-27 17:42:12,321][train][INFO] - Epoch 36/100, Val Acc=0.8559, Val Loss=0.4344, lr=0.0100
[2025-04-27 17:42:22,805][train][INFO] - Epoch 31/100, Val Acc=0.7985, Val Loss=0.5940, lr=0.0100
[2025-04-27 17:42:24,876][train][INFO] - Epoch 32/100, Val Acc=0.8695, Val Loss=0.4032, lr=0.0100
[2025-04-27 17:42:28,615][train][INFO] - Epoch 32/100, Val Acc=0.8113, Val Loss=0.6043, lr=0.0100
[2025-04-27 17:42:28,774][train][INFO] - Epoch 32/100, Val Acc=0.8703, Val Loss=0.3992, lr=0.0100
[2025-04-27 17:42:28,774][train][INFO] - Epoch 35/100, Val Acc=0.7986, Val Loss=0.5851, lr=0.0100
[2025-04-27 17:42:29,471][train][INFO] - Epoch 36/100, Val Acc=0.8532, Val Loss=0.4671, lr=0.0100
[2025-04-27 17:42:31,379][train][INFO] - Epoch 37/100, Val Acc=0.8551, Val Loss=0.4655, lr=0.0100
[2025-04-27 17:42:41,948][train][INFO] - Epoch 32/100, Val Acc=0.7701, Val Loss=0.7353, lr=0.0100
[2025-04-27 17:42:44,065][train][INFO] - Epoch 33/100, Val Acc=0.8601, Val Loss=0.4341, lr=0.0100
[2025-04-27 17:42:48,114][train][INFO] - Epoch 33/100, Val Acc=0.8146, Val Loss=0.5769, lr=0.0100
[2025-04-27 17:42:48,380][train][INFO] - Epoch 36/100, Val Acc=0.7512, Val Loss=0.8080, lr=0.0100
[2025-04-27 17:42:48,587][train][INFO] - Epoch 33/100, Val Acc=0.8722, Val Loss=0.3881, lr=0.0100
[2025-04-27 17:42:48,966][train][INFO] - Epoch 37/100, Val Acc=0.8451, Val Loss=0.4682, lr=0.0100
[2025-04-27 17:42:50,479][train][INFO] - Epoch 38/100, Val Acc=0.8556, Val Loss=0.4523, lr=0.0100
[2025-04-27 17:43:01,202][train][INFO] - Epoch 33/100, Val Acc=0.8060, Val Loss=0.5913, lr=0.0100
[2025-04-27 17:43:03,332][train][INFO] - Epoch 34/100, Val Acc=0.8668, Val Loss=0.3993, lr=0.0100
[2025-04-27 17:43:07,351][train][INFO] - Epoch 34/100, Val Acc=0.7969, Val Loss=0.6754, lr=0.0100
[2025-04-27 17:43:07,878][train][INFO] - Epoch 37/100, Val Acc=0.8077, Val Loss=0.5868, lr=0.0100
[2025-04-27 17:43:08,341][train][INFO] - Epoch 34/100, Val Acc=0.8801, Val Loss=0.3647, lr=0.0100
[2025-04-27 17:43:08,426][train][INFO] - Epoch 38/100, Val Acc=0.8548, Val Loss=0.4496, lr=0.0100
[2025-04-27 17:43:09,704][train][INFO] - Epoch 39/100, Val Acc=0.8489, Val Loss=0.4717, lr=0.0100
[2025-04-27 17:43:20,533][train][INFO] - Epoch 34/100, Val Acc=0.7412, Val Loss=0.8659, lr=0.0100
[2025-04-27 17:43:22,603][train][INFO] - Epoch 35/100, Val Acc=0.8400, Val Loss=0.4993, lr=0.0100
[2025-04-27 17:43:26,757][train][INFO] - Epoch 35/100, Val Acc=0.8103, Val Loss=0.5790, lr=0.0100
[2025-04-27 17:43:27,486][train][INFO] - Epoch 38/100, Val Acc=0.8016, Val Loss=0.5875, lr=0.0100
[2025-04-27 17:43:27,912][train][INFO] - Epoch 39/100, Val Acc=0.8570, Val Loss=0.4282, lr=0.0100
[2025-04-27 17:43:28,207][train][INFO] - Epoch 35/100, Val Acc=0.8779, Val Loss=0.3901, lr=0.0100
[2025-04-27 17:43:28,979][train][INFO] - Epoch 40/100, Val Acc=0.8606, Val Loss=0.4302, lr=0.0100
[2025-04-27 17:43:39,777][train][INFO] - Epoch 35/100, Val Acc=0.7673, Val Loss=0.7492, lr=0.0100
[2025-04-27 17:43:41,949][train][INFO] - Epoch 36/100, Val Acc=0.8662, Val Loss=0.4040, lr=0.0100
[2025-04-27 17:43:46,021][train][INFO] - Epoch 36/100, Val Acc=0.8448, Val Loss=0.4556, lr=0.0100
[2025-04-27 17:43:47,038][train][INFO] - Epoch 39/100, Val Acc=0.8181, Val Loss=0.5350, lr=0.0100
[2025-04-27 17:43:47,504][train][INFO] - Epoch 40/100, Val Acc=0.8330, Val Loss=0.5252, lr=0.0100
[2025-04-27 17:43:47,742][train][INFO] - Epoch 36/100, Val Acc=0.8566, Val Loss=0.4592, lr=0.0100
[2025-04-27 17:43:48,335][train][INFO] - Epoch 41/100, Val Acc=0.8653, Val Loss=0.4066, lr=0.0100
[2025-04-27 17:43:59,064][train][INFO] - Epoch 36/100, Val Acc=0.8131, Val Loss=0.5562, lr=0.0100
[2025-04-27 17:44:01,176][train][INFO] - Epoch 37/100, Val Acc=0.8474, Val Loss=0.4762, lr=0.0100
[2025-04-27 17:44:05,247][train][INFO] - Epoch 37/100, Val Acc=0.8138, Val Loss=0.5749, lr=0.0100
[2025-04-27 17:44:06,630][train][INFO] - Epoch 40/100, Val Acc=0.7867, Val Loss=0.6489, lr=0.0100
[2025-04-27 17:44:07,056][train][INFO] - Epoch 41/100, Val Acc=0.8363, Val Loss=0.5318, lr=0.0100
[2025-04-27 17:44:07,377][train][INFO] - Epoch 37/100, Val Acc=0.8675, Val Loss=0.4227, lr=0.0100
[2025-04-27 17:44:07,625][train][INFO] - Epoch 42/100, Val Acc=0.8449, Val Loss=0.4710, lr=0.0100
[2025-04-27 17:44:18,239][train][INFO] - Epoch 37/100, Val Acc=0.8029, Val Loss=0.5897, lr=0.0100
[2025-04-27 17:44:20,430][train][INFO] - Epoch 38/100, Val Acc=0.8640, Val Loss=0.4136, lr=0.0100
[2025-04-27 17:44:24,511][train][INFO] - Epoch 38/100, Val Acc=0.8381, Val Loss=0.4811, lr=0.0100
[2025-04-27 17:44:26,334][train][INFO] - Epoch 41/100, Val Acc=0.8055, Val Loss=0.5812, lr=0.0100
[2025-04-27 17:44:26,774][train][INFO] - Epoch 42/100, Val Acc=0.8449, Val Loss=0.4945, lr=0.0100
[2025-04-27 17:44:26,994][train][INFO] - Epoch 38/100, Val Acc=0.8711, Val Loss=0.4061, lr=0.0100
[2025-04-27 17:44:27,027][train][INFO] - Epoch 43/100, Val Acc=0.8705, Val Loss=0.3988, lr=0.0100
[2025-04-27 17:44:37,402][train][INFO] - Epoch 38/100, Val Acc=0.8009, Val Loss=0.5978, lr=0.0100
[2025-04-27 17:44:39,790][train][INFO] - Epoch 39/100, Val Acc=0.8598, Val Loss=0.4319, lr=0.0100
[2025-04-27 17:44:43,639][train][INFO] - Epoch 39/100, Val Acc=0.8173, Val Loss=0.5348, lr=0.0100
[2025-04-27 17:44:46,012][train][INFO] - Epoch 42/100, Val Acc=0.7831, Val Loss=0.6414, lr=0.0100
[2025-04-27 17:44:46,356][train][INFO] - Epoch 43/100, Val Acc=0.8425, Val Loss=0.4949, lr=0.0100
[2025-04-27 17:44:46,521][train][INFO] - Epoch 44/100, Val Acc=0.8719, Val Loss=0.3896, lr=0.0100
[2025-04-27 17:44:46,553][train][INFO] - Epoch 39/100, Val Acc=0.8855, Val Loss=0.3633, lr=0.0100
[2025-04-27 17:44:56,652][train][INFO] - Epoch 39/100, Val Acc=0.8110, Val Loss=0.5726, lr=0.0100
[2025-04-27 17:44:59,122][train][INFO] - Epoch 40/100, Val Acc=0.8610, Val Loss=0.4237, lr=0.0100
[2025-04-27 17:45:02,965][train][INFO] - Epoch 40/100, Val Acc=0.8503, Val Loss=0.4430, lr=0.0100
[2025-04-27 17:45:05,806][train][INFO] - Epoch 43/100, Val Acc=0.8045, Val Loss=0.6101, lr=0.0100
[2025-04-27 17:45:06,159][train][INFO] - Epoch 45/100, Val Acc=0.8756, Val Loss=0.3825, lr=0.0100
[2025-04-27 17:45:06,184][train][INFO] - Epoch 44/100, Val Acc=0.8464, Val Loss=0.4649, lr=0.0100
[2025-04-27 17:45:06,307][train][INFO] - Epoch 40/100, Val Acc=0.8835, Val Loss=0.3707, lr=0.0100
[2025-04-27 17:45:15,865][train][INFO] - Epoch 40/100, Val Acc=0.8020, Val Loss=0.5948, lr=0.0100
[2025-04-27 17:45:18,383][train][INFO] - Epoch 41/100, Val Acc=0.8713, Val Loss=0.4021, lr=0.0100
[2025-04-27 17:45:22,203][train][INFO] - Epoch 41/100, Val Acc=0.8332, Val Loss=0.5190, lr=0.0100
[2025-04-27 17:45:25,442][train][INFO] - Epoch 44/100, Val Acc=0.8210, Val Loss=0.5278, lr=0.0100
[2025-04-27 17:45:25,615][train][INFO] - Epoch 46/100, Val Acc=0.8539, Val Loss=0.4412, lr=0.0100
[2025-04-27 17:45:25,855][train][INFO] - Epoch 45/100, Val Acc=0.8585, Val Loss=0.4310, lr=0.0100
[2025-04-27 17:45:25,982][train][INFO] - Epoch 41/100, Val Acc=0.8657, Val Loss=0.4095, lr=0.0100
[2025-04-27 17:45:35,116][train][INFO] - Epoch 41/100, Val Acc=0.8259, Val Loss=0.5075, lr=0.0100
[2025-04-27 17:45:37,693][train][INFO] - Epoch 42/100, Val Acc=0.8603, Val Loss=0.4191, lr=0.0100
[2025-04-27 17:45:41,436][train][INFO] - Epoch 42/100, Val Acc=0.8459, Val Loss=0.4785, lr=0.0100
[2025-04-27 17:45:45,096][train][INFO] - Epoch 47/100, Val Acc=0.8689, Val Loss=0.4081, lr=0.0100
[2025-04-27 17:45:45,186][train][INFO] - Epoch 45/100, Val Acc=0.7924, Val Loss=0.6419, lr=0.0100
[2025-04-27 17:45:45,664][train][INFO] - Epoch 46/100, Val Acc=0.8699, Val Loss=0.4093, lr=0.0100
[2025-04-27 17:45:45,712][train][INFO] - Epoch 42/100, Val Acc=0.8797, Val Loss=0.3740, lr=0.0100
[2025-04-27 17:45:54,502][train][INFO] - Epoch 42/100, Val Acc=0.7902, Val Loss=0.6021, lr=0.0100
[2025-04-27 17:45:56,979][train][INFO] - Epoch 43/100, Val Acc=0.8428, Val Loss=0.4766, lr=0.0100
[2025-04-27 17:46:00,543][train][INFO] - Epoch 43/100, Val Acc=0.8343, Val Loss=0.5047, lr=0.0100
[2025-04-27 17:46:04,551][train][INFO] - Epoch 48/100, Val Acc=0.8661, Val Loss=0.4139, lr=0.0100
[2025-04-27 17:46:04,909][train][INFO] - Epoch 46/100, Val Acc=0.8076, Val Loss=0.5816, lr=0.0100
[2025-04-27 17:46:05,348][train][INFO] - Epoch 47/100, Val Acc=0.8482, Val Loss=0.4776, lr=0.0100
[2025-04-27 17:46:05,471][train][INFO] - Epoch 43/100, Val Acc=0.8701, Val Loss=0.4116, lr=0.0100
[2025-04-27 17:46:13,673][train][INFO] - Epoch 43/100, Val Acc=0.7338, Val Loss=0.9538, lr=0.0100
[2025-04-27 17:46:16,235][train][INFO] - Epoch 44/100, Val Acc=0.8643, Val Loss=0.4222, lr=0.0100
[2025-04-27 17:46:19,702][train][INFO] - Epoch 44/100, Val Acc=0.8268, Val Loss=0.5369, lr=0.0100
[2025-04-27 17:46:23,801][train][INFO] - Epoch 49/100, Val Acc=0.8681, Val Loss=0.4057, lr=0.0100
[2025-04-27 17:46:24,521][train][INFO] - Epoch 47/100, Val Acc=0.8052, Val Loss=0.5796, lr=0.0100
[2025-04-27 17:46:24,965][train][INFO] - Epoch 48/100, Val Acc=0.8478, Val Loss=0.4786, lr=0.0100
[2025-04-27 17:46:25,174][train][INFO] - Epoch 44/100, Val Acc=0.8739, Val Loss=0.3890, lr=0.0100
[2025-04-27 17:46:32,925][train][INFO] - Epoch 44/100, Val Acc=0.8022, Val Loss=0.5960, lr=0.0100
[2025-04-27 17:46:35,528][train][INFO] - Epoch 45/100, Val Acc=0.8697, Val Loss=0.3856, lr=0.0100
[2025-04-27 17:46:38,873][train][INFO] - Epoch 45/100, Val Acc=0.8324, Val Loss=0.4855, lr=0.0100
[2025-04-27 17:46:43,105][train][INFO] - Epoch 50/100, Val Acc=0.8486, Val Loss=0.4965, lr=0.0100
[2025-04-27 17:46:44,227][train][INFO] - Epoch 48/100, Val Acc=0.7803, Val Loss=0.7060, lr=0.0100
[2025-04-27 17:46:44,482][train][INFO] - Epoch 49/100, Val Acc=0.8306, Val Loss=0.5476, lr=0.0100
[2025-04-27 17:46:44,841][train][INFO] - Epoch 45/100, Val Acc=0.8739, Val Loss=0.3991, lr=0.0100
[2025-04-27 17:46:52,241][train][INFO] - Epoch 45/100, Val Acc=0.8201, Val Loss=0.5406, lr=0.0100
[2025-04-27 17:46:54,774][train][INFO] - Epoch 46/100, Val Acc=0.8546, Val Loss=0.4568, lr=0.0100
[2025-04-27 17:46:58,074][train][INFO] - Epoch 46/100, Val Acc=0.8098, Val Loss=0.5954, lr=0.0100
[2025-04-27 17:47:02,426][train][INFO] - Epoch 51/100, Val Acc=0.8799, Val Loss=0.3723, lr=0.0100
[2025-04-27 17:47:03,879][train][INFO] - Epoch 49/100, Val Acc=0.7879, Val Loss=0.6365, lr=0.0100
[2025-04-27 17:47:04,037][train][INFO] - Epoch 50/100, Val Acc=0.8633, Val Loss=0.4198, lr=0.0100
[2025-04-27 17:47:04,523][train][INFO] - Epoch 46/100, Val Acc=0.8852, Val Loss=0.3602, lr=0.0100
[2025-04-27 17:47:11,544][train][INFO] - Epoch 46/100, Val Acc=0.7577, Val Loss=0.7732, lr=0.0100
[2025-04-27 17:47:14,103][train][INFO] - Epoch 47/100, Val Acc=0.8745, Val Loss=0.3851, lr=0.0100
[2025-04-27 17:47:17,343][train][INFO] - Epoch 47/100, Val Acc=0.8290, Val Loss=0.5203, lr=0.0100
[2025-04-27 17:47:21,543][train][INFO] - Epoch 52/100, Val Acc=0.8794, Val Loss=0.3711, lr=0.0100
[2025-04-27 17:47:23,587][train][INFO] - Epoch 50/100, Val Acc=0.8068, Val Loss=0.5958, lr=0.0100
[2025-04-27 17:47:23,632][train][INFO] - Epoch 51/100, Val Acc=0.8435, Val Loss=0.4994, lr=0.0100
[2025-04-27 17:47:24,193][train][INFO] - Epoch 47/100, Val Acc=0.8429, Val Loss=0.5104, lr=0.0100
[2025-04-27 17:47:30,775][train][INFO] - Epoch 47/100, Val Acc=0.8159, Val Loss=0.5703, lr=0.0100
[2025-04-27 17:47:33,330][train][INFO] - Epoch 48/100, Val Acc=0.8676, Val Loss=0.4113, lr=0.0100
[2025-04-27 17:47:36,583][train][INFO] - Epoch 48/100, Val Acc=0.8125, Val Loss=0.5831, lr=0.0100
[2025-04-27 17:47:40,424][train][INFO] - Epoch 53/100, Val Acc=0.8645, Val Loss=0.4327, lr=0.0100
[2025-04-27 17:47:43,175][train][INFO] - Epoch 52/100, Val Acc=0.8581, Val Loss=0.4384, lr=0.0100
[2025-04-27 17:47:43,269][train][INFO] - Epoch 51/100, Val Acc=0.8325, Val Loss=0.5025, lr=0.0100
[2025-04-27 17:47:43,817][train][INFO] - Epoch 48/100, Val Acc=0.8739, Val Loss=0.4005, lr=0.0100
[2025-04-27 17:47:50,004][train][INFO] - Epoch 48/100, Val Acc=0.7940, Val Loss=0.6325, lr=0.0100
[2025-04-27 17:47:52,562][train][INFO] - Epoch 49/100, Val Acc=0.8760, Val Loss=0.3968, lr=0.0100
[2025-04-27 17:47:55,753][train][INFO] - Epoch 49/100, Val Acc=0.8192, Val Loss=0.5580, lr=0.0100
[2025-04-27 17:47:59,273][train][INFO] - Epoch 54/100, Val Acc=0.8633, Val Loss=0.4007, lr=0.0100
[2025-04-27 17:48:02,687][train][INFO] - Epoch 53/100, Val Acc=0.8728, Val Loss=0.4035, lr=0.0100
[2025-04-27 17:48:02,825][train][INFO] - Epoch 52/100, Val Acc=0.8173, Val Loss=0.5650, lr=0.0100
[2025-04-27 17:48:03,483][train][INFO] - Epoch 49/100, Val Acc=0.8777, Val Loss=0.3821, lr=0.0100
[2025-04-27 17:48:09,276][train][INFO] - Epoch 49/100, Val Acc=0.8121, Val Loss=0.5680, lr=0.0100
[2025-04-27 17:48:11,998][train][INFO] - Epoch 50/100, Val Acc=0.8749, Val Loss=0.3878, lr=0.0100
[2025-04-27 17:48:14,917][train][INFO] - Epoch 50/100, Val Acc=0.8526, Val Loss=0.4347, lr=0.0100
[2025-04-27 17:48:18,263][train][INFO] - Epoch 55/100, Val Acc=0.8739, Val Loss=0.3867, lr=0.0100
[2025-04-27 17:48:22,131][train][INFO] - Epoch 54/100, Val Acc=0.8703, Val Loss=0.3853, lr=0.0100
[2025-04-27 17:48:22,452][train][INFO] - Epoch 53/100, Val Acc=0.7888, Val Loss=0.6769, lr=0.0100
[2025-04-27 17:48:23,348][train][INFO] - Epoch 50/100, Val Acc=0.8782, Val Loss=0.3714, lr=0.0100
[2025-04-27 17:48:28,515][train][INFO] - Epoch 50/100, Val Acc=0.8148, Val Loss=0.5500, lr=0.0100
[2025-04-27 17:48:31,165][train][INFO] - Epoch 51/100, Val Acc=0.8566, Val Loss=0.4278, lr=0.0100
[2025-04-27 17:48:34,040][train][INFO] - Epoch 51/100, Val Acc=0.8471, Val Loss=0.4723, lr=0.0100
[2025-04-27 17:48:37,016][train][INFO] - Epoch 56/100, Val Acc=0.8667, Val Loss=0.3976, lr=0.0100
[2025-04-27 17:48:41,587][train][INFO] - Epoch 55/100, Val Acc=0.8653, Val Loss=0.4231, lr=0.0100
[2025-04-27 17:48:42,007][train][INFO] - Epoch 54/100, Val Acc=0.7944, Val Loss=0.6496, lr=0.0100
[2025-04-27 17:48:43,041][train][INFO] - Epoch 51/100, Val Acc=0.8776, Val Loss=0.3676, lr=0.0100
[2025-04-27 17:48:47,858][train][INFO] - Epoch 51/100, Val Acc=0.7329, Val Loss=0.8863, lr=0.0100
[2025-04-27 17:48:50,537][train][INFO] - Epoch 52/100, Val Acc=0.8752, Val Loss=0.3888, lr=0.0100
[2025-04-27 17:48:53,188][train][INFO] - Epoch 52/100, Val Acc=0.8140, Val Loss=0.5950, lr=0.0100
[2025-04-27 17:48:56,033][train][INFO] - Epoch 57/100, Val Acc=0.8805, Val Loss=0.3645, lr=0.0100
[2025-04-27 17:49:00,940][train][INFO] - Epoch 56/100, Val Acc=0.8714, Val Loss=0.3985, lr=0.0100
[2025-04-27 17:49:01,496][train][INFO] - Epoch 55/100, Val Acc=0.8087, Val Loss=0.5850, lr=0.0100
[2025-04-27 17:49:02,738][train][INFO] - Epoch 52/100, Val Acc=0.8524, Val Loss=0.4801, lr=0.0100
[2025-04-27 17:49:07,084][train][INFO] - Epoch 52/100, Val Acc=0.8192, Val Loss=0.5583, lr=0.0100
[2025-04-27 17:49:09,807][train][INFO] - Epoch 53/100, Val Acc=0.8214, Val Loss=0.5986, lr=0.0100
[2025-04-27 17:49:12,301][train][INFO] - Epoch 53/100, Val Acc=0.8264, Val Loss=0.5204, lr=0.0100
[2025-04-27 17:49:15,029][train][INFO] - Epoch 58/100, Val Acc=0.8735, Val Loss=0.3823, lr=0.0100
[2025-04-27 17:49:20,187][train][INFO] - Epoch 57/100, Val Acc=0.8763, Val Loss=0.3843, lr=0.0100
[2025-04-27 17:49:21,096][train][INFO] - Epoch 56/100, Val Acc=0.8032, Val Loss=0.6128, lr=0.0100
[2025-04-27 17:49:22,496][train][INFO] - Epoch 53/100, Val Acc=0.8863, Val Loss=0.3607, lr=0.0100
[2025-04-27 17:49:26,399][train][INFO] - Epoch 53/100, Val Acc=0.8083, Val Loss=0.5864, lr=0.0100
[2025-04-27 17:49:29,139][train][INFO] - Epoch 54/100, Val Acc=0.8660, Val Loss=0.4220, lr=0.0100
[2025-04-27 17:49:31,496][train][INFO] - Epoch 54/100, Val Acc=0.8456, Val Loss=0.4589, lr=0.0100
[2025-04-27 17:49:33,999][train][INFO] - Epoch 59/100, Val Acc=0.8710, Val Loss=0.4016, lr=0.0100
[2025-04-27 17:49:39,538][train][INFO] - Epoch 58/100, Val Acc=0.8667, Val Loss=0.3956, lr=0.0100
[2025-04-27 17:49:40,678][train][INFO] - Epoch 57/100, Val Acc=0.8182, Val Loss=0.5658, lr=0.0100
[2025-04-27 17:49:42,202][train][INFO] - Epoch 54/100, Val Acc=0.8803, Val Loss=0.3806, lr=0.0100
[2025-04-27 17:49:45,580][train][INFO] - Epoch 54/100, Val Acc=0.8234, Val Loss=0.5246, lr=0.0100
[2025-04-27 17:49:48,447][train][INFO] - Epoch 55/100, Val Acc=0.8756, Val Loss=0.3754, lr=0.0100
[2025-04-27 17:49:50,708][train][INFO] - Epoch 55/100, Val Acc=0.8546, Val Loss=0.4361, lr=0.0100
[2025-04-27 17:49:52,990][train][INFO] - Epoch 60/100, Val Acc=0.8553, Val Loss=0.4530, lr=0.0100
[2025-04-27 17:49:58,940][train][INFO] - Epoch 59/100, Val Acc=0.8561, Val Loss=0.4558, lr=0.0100
[2025-04-27 17:50:00,222][train][INFO] - Epoch 58/100, Val Acc=0.8291, Val Loss=0.5252, lr=0.0100
[2025-04-27 17:50:01,823][train][INFO] - Epoch 55/100, Val Acc=0.8791, Val Loss=0.3959, lr=0.0100
[2025-04-27 17:50:04,775][train][INFO] - Epoch 55/100, Val Acc=0.7925, Val Loss=0.6300, lr=0.0100
[2025-04-27 17:50:07,629][train][INFO] - Epoch 56/100, Val Acc=0.8519, Val Loss=0.4781, lr=0.0100
[2025-04-27 17:50:09,841][train][INFO] - Epoch 56/100, Val Acc=0.8623, Val Loss=0.4119, lr=0.0100
[2025-04-27 17:50:11,973][train][INFO] - Epoch 61/100, Val Acc=0.9079, Val Loss=0.2815, lr=0.0010
[2025-04-27 17:50:18,253][train][INFO] - Epoch 60/100, Val Acc=0.8534, Val Loss=0.4541, lr=0.0100
[2025-04-27 17:50:19,794][train][INFO] - Epoch 59/100, Val Acc=0.7952, Val Loss=0.6376, lr=0.0100
[2025-04-27 17:50:21,471][train][INFO] - Epoch 56/100, Val Acc=0.8820, Val Loss=0.3621, lr=0.0100
[2025-04-27 17:50:24,080][train][INFO] - Epoch 56/100, Val Acc=0.8321, Val Loss=0.4937, lr=0.0100
[2025-04-27 17:50:26,918][train][INFO] - Epoch 57/100, Val Acc=0.8765, Val Loss=0.3838, lr=0.0100
[2025-04-27 17:50:28,948][train][INFO] - Epoch 57/100, Val Acc=0.8562, Val Loss=0.4425, lr=0.0100
[2025-04-27 17:50:30,986][train][INFO] - Epoch 62/100, Val Acc=0.9061, Val Loss=0.2824, lr=0.0010
[2025-04-27 17:50:37,582][train][INFO] - Epoch 61/100, Val Acc=0.9049, Val Loss=0.2909, lr=0.0010
[2025-04-27 17:50:39,480][train][INFO] - Epoch 60/100, Val Acc=0.7904, Val Loss=0.6663, lr=0.0100
[2025-04-27 17:50:41,174][train][INFO] - Epoch 57/100, Val Acc=0.8672, Val Loss=0.4123, lr=0.0100
[2025-04-27 17:50:43,265][train][INFO] - Epoch 57/100, Val Acc=0.7292, Val Loss=0.9164, lr=0.0100
[2025-04-27 17:50:46,159][train][INFO] - Epoch 58/100, Val Acc=0.8648, Val Loss=0.4165, lr=0.0100
[2025-04-27 17:50:48,072][train][INFO] - Epoch 58/100, Val Acc=0.8487, Val Loss=0.4694, lr=0.0100
[2025-04-27 17:50:49,947][train][INFO] - Epoch 63/100, Val Acc=0.9093, Val Loss=0.2775, lr=0.0010
[2025-04-27 17:50:57,012][train][INFO] - Epoch 62/100, Val Acc=0.9078, Val Loss=0.2856, lr=0.0010
[2025-04-27 17:50:59,204][train][INFO] - Epoch 61/100, Val Acc=0.8710, Val Loss=0.3810, lr=0.0010
[2025-04-27 17:51:00,750][train][INFO] - Epoch 58/100, Val Acc=0.8739, Val Loss=0.4021, lr=0.0100
[2025-04-27 17:51:02,473][train][INFO] - Epoch 58/100, Val Acc=0.7888, Val Loss=0.6876, lr=0.0100
[2025-04-27 17:51:05,377][train][INFO] - Epoch 59/100, Val Acc=0.8590, Val Loss=0.4585, lr=0.0100
[2025-04-27 17:51:07,125][train][INFO] - Epoch 59/100, Val Acc=0.8581, Val Loss=0.4328, lr=0.0100
[2025-04-27 17:51:08,910][train][INFO] - Epoch 64/100, Val Acc=0.9081, Val Loss=0.2745, lr=0.0010
[2025-04-27 17:51:16,367][train][INFO] - Epoch 63/100, Val Acc=0.9090, Val Loss=0.2818, lr=0.0010
[2025-04-27 17:51:18,884][train][INFO] - Epoch 62/100, Val Acc=0.8700, Val Loss=0.3782, lr=0.0010
[2025-04-27 17:51:20,377][train][INFO] - Epoch 59/100, Val Acc=0.8801, Val Loss=0.3649, lr=0.0100
[2025-04-27 17:51:21,742][train][INFO] - Epoch 59/100, Val Acc=0.8370, Val Loss=0.5129, lr=0.0100
[2025-04-27 17:51:24,565][train][INFO] - Epoch 60/100, Val Acc=0.8764, Val Loss=0.3873, lr=0.0100
[2025-04-27 17:51:26,283][train][INFO] - Epoch 60/100, Val Acc=0.8481, Val Loss=0.4543, lr=0.0100
[2025-04-27 17:51:27,962][train][INFO] - Epoch 65/100, Val Acc=0.9097, Val Loss=0.2807, lr=0.0010
[2025-04-27 17:51:35,585][train][INFO] - Epoch 64/100, Val Acc=0.9078, Val Loss=0.2868, lr=0.0010
[2025-04-27 17:51:38,495][train][INFO] - Epoch 63/100, Val Acc=0.8708, Val Loss=0.3819, lr=0.0010
[2025-04-27 17:51:39,919][train][INFO] - Epoch 60/100, Val Acc=0.8806, Val Loss=0.3765, lr=0.0100
[2025-04-27 17:51:41,059][train][INFO] - Epoch 60/100, Val Acc=0.8005, Val Loss=0.6204, lr=0.0100
[2025-04-27 17:51:43,814][train][INFO] - Epoch 61/100, Val Acc=0.9046, Val Loss=0.2928, lr=0.0010
[2025-04-27 17:51:45,522][train][INFO] - Epoch 61/100, Val Acc=0.8904, Val Loss=0.3305, lr=0.0010
[2025-04-27 17:51:47,132][train][INFO] - Epoch 66/100, Val Acc=0.9090, Val Loss=0.2783, lr=0.0010
[2025-04-27 17:51:54,796][train][INFO] - Epoch 65/100, Val Acc=0.9102, Val Loss=0.2828, lr=0.0010
[2025-04-27 17:51:57,946][train][INFO] - Epoch 64/100, Val Acc=0.8721, Val Loss=0.3775, lr=0.0010
[2025-04-27 17:51:59,633][train][INFO] - Epoch 61/100, Val Acc=0.9111, Val Loss=0.2692, lr=0.0010
[2025-04-27 17:52:00,419][train][INFO] - Epoch 61/100, Val Acc=0.8665, Val Loss=0.4022, lr=0.0010
[2025-04-27 17:52:03,119][train][INFO] - Epoch 62/100, Val Acc=0.9057, Val Loss=0.2874, lr=0.0010
[2025-04-27 17:52:04,680][train][INFO] - Epoch 62/100, Val Acc=0.8898, Val Loss=0.3238, lr=0.0010
[2025-04-27 17:52:06,102][train][INFO] - Epoch 67/100, Val Acc=0.9103, Val Loss=0.2787, lr=0.0010
[2025-04-27 17:52:14,015][train][INFO] - Epoch 66/100, Val Acc=0.9122, Val Loss=0.2843, lr=0.0010
[2025-04-27 17:52:17,417][train][INFO] - Epoch 65/100, Val Acc=0.8745, Val Loss=0.3705, lr=0.0010
[2025-04-27 17:52:19,174][train][INFO] - Epoch 62/100, Val Acc=0.9114, Val Loss=0.2701, lr=0.0010
[2025-04-27 17:52:19,711][train][INFO] - Epoch 62/100, Val Acc=0.8650, Val Loss=0.3947, lr=0.0010
[2025-04-27 17:52:22,421][train][INFO] - Epoch 63/100, Val Acc=0.9078, Val Loss=0.2879, lr=0.0010
[2025-04-27 17:52:23,867][train][INFO] - Epoch 63/100, Val Acc=0.8927, Val Loss=0.3229, lr=0.0010
[2025-04-27 17:52:25,013][train][INFO] - Epoch 68/100, Val Acc=0.9101, Val Loss=0.2783, lr=0.0010
[2025-04-27 17:52:33,110][train][INFO] - Epoch 67/100, Val Acc=0.9114, Val Loss=0.2862, lr=0.0010
[2025-04-27 17:52:36,954][train][INFO] - Epoch 66/100, Val Acc=0.8717, Val Loss=0.3763, lr=0.0010
[2025-04-27 17:52:38,799][train][INFO] - Epoch 63/100, Val Acc=0.9131, Val Loss=0.2704, lr=0.0010
[2025-04-27 17:52:39,014][train][INFO] - Epoch 63/100, Val Acc=0.8680, Val Loss=0.3923, lr=0.0010
[2025-04-27 17:52:41,658][train][INFO] - Epoch 64/100, Val Acc=0.9085, Val Loss=0.2866, lr=0.0010
[2025-04-27 17:52:43,072][train][INFO] - Epoch 64/100, Val Acc=0.8924, Val Loss=0.3199, lr=0.0010
[2025-04-27 17:52:44,159][train][INFO] - Epoch 69/100, Val Acc=0.9113, Val Loss=0.2809, lr=0.0010
[2025-04-27 17:52:52,296][train][INFO] - Epoch 68/100, Val Acc=0.9106, Val Loss=0.2815, lr=0.0010
[2025-04-27 17:52:56,537][train][INFO] - Epoch 67/100, Val Acc=0.8724, Val Loss=0.3761, lr=0.0010
[2025-04-27 17:52:58,566][train][INFO] - Epoch 64/100, Val Acc=0.9126, Val Loss=0.2711, lr=0.0010
[2025-04-27 17:52:58,582][train][INFO] - Epoch 64/100, Val Acc=0.8681, Val Loss=0.3917, lr=0.0010
[2025-04-27 17:53:00,968][train][INFO] - Epoch 65/100, Val Acc=0.9070, Val Loss=0.2881, lr=0.0010
[2025-04-27 17:53:02,331][train][INFO] - Epoch 65/100, Val Acc=0.8916, Val Loss=0.3261, lr=0.0010
[2025-04-27 17:53:03,159][train][INFO] - Epoch 70/100, Val Acc=0.9075, Val Loss=0.2839, lr=0.0010
[2025-04-27 17:53:11,594][train][INFO] - Epoch 69/100, Val Acc=0.9114, Val Loss=0.2848, lr=0.0010
[2025-04-27 17:53:16,017][train][INFO] - Epoch 68/100, Val Acc=0.8718, Val Loss=0.3777, lr=0.0010
[2025-04-27 17:53:18,151][train][INFO] - Epoch 65/100, Val Acc=0.8696, Val Loss=0.3898, lr=0.0010
[2025-04-27 17:53:18,256][train][INFO] - Epoch 65/100, Val Acc=0.9125, Val Loss=0.2704, lr=0.0010
[2025-04-27 17:53:20,165][train][INFO] - Epoch 66/100, Val Acc=0.9075, Val Loss=0.2872, lr=0.0010
[2025-04-27 17:53:21,497][train][INFO] - Epoch 66/100, Val Acc=0.8927, Val Loss=0.3244, lr=0.0010
[2025-04-27 17:53:22,161][train][INFO] - Epoch 71/100, Val Acc=0.9087, Val Loss=0.2811, lr=0.0010
[2025-04-27 17:53:30,940][train][INFO] - Epoch 70/100, Val Acc=0.9115, Val Loss=0.2843, lr=0.0010
[2025-04-27 17:53:35,537][train][INFO] - Epoch 69/100, Val Acc=0.8704, Val Loss=0.3819, lr=0.0010
[2025-04-27 17:53:37,617][train][INFO] - Epoch 66/100, Val Acc=0.8700, Val Loss=0.3950, lr=0.0010
[2025-04-27 17:53:37,920][train][INFO] - Epoch 66/100, Val Acc=0.9128, Val Loss=0.2722, lr=0.0010
[2025-04-27 17:53:39,385][train][INFO] - Epoch 67/100, Val Acc=0.9073, Val Loss=0.2889, lr=0.0010
[2025-04-27 17:53:40,749][train][INFO] - Epoch 67/100, Val Acc=0.8918, Val Loss=0.3290, lr=0.0010
[2025-04-27 17:53:41,193][train][INFO] - Epoch 72/100, Val Acc=0.9078, Val Loss=0.2864, lr=0.0010
[2025-04-27 17:53:50,209][train][INFO] - Epoch 71/100, Val Acc=0.9101, Val Loss=0.2861, lr=0.0010
[2025-04-27 17:53:54,875][train][INFO] - Epoch 70/100, Val Acc=0.8739, Val Loss=0.3761, lr=0.0010
[2025-04-27 17:53:57,081][train][INFO] - Epoch 67/100, Val Acc=0.8719, Val Loss=0.3906, lr=0.0010
[2025-04-27 17:53:57,419][train][INFO] - Epoch 67/100, Val Acc=0.9140, Val Loss=0.2687, lr=0.0010
[2025-04-27 17:53:58,676][train][INFO] - Epoch 68/100, Val Acc=0.9071, Val Loss=0.2879, lr=0.0010
[2025-04-27 17:53:59,955][train][INFO] - Epoch 68/100, Val Acc=0.8931, Val Loss=0.3235, lr=0.0010
[2025-04-27 17:54:00,334][train][INFO] - Epoch 73/100, Val Acc=0.9090, Val Loss=0.2871, lr=0.0010
[2025-04-27 17:54:09,517][train][INFO] - Epoch 72/100, Val Acc=0.9091, Val Loss=0.2858, lr=0.0010
[2025-04-27 17:54:14,264][train][INFO] - Epoch 71/100, Val Acc=0.8719, Val Loss=0.3805, lr=0.0010
[2025-04-27 17:54:16,348][train][INFO] - Epoch 68/100, Val Acc=0.8698, Val Loss=0.3938, lr=0.0010
[2025-04-27 17:54:17,196][train][INFO] - Epoch 68/100, Val Acc=0.9144, Val Loss=0.2710, lr=0.0010
[2025-04-27 17:54:18,121][train][INFO] - Epoch 69/100, Val Acc=0.9081, Val Loss=0.2851, lr=0.0010
[2025-04-27 17:54:19,246][train][INFO] - Epoch 74/100, Val Acc=0.9082, Val Loss=0.2885, lr=0.0010
[2025-04-27 17:54:19,291][train][INFO] - Epoch 69/100, Val Acc=0.8913, Val Loss=0.3273, lr=0.0010
[2025-04-27 17:54:28,881][train][INFO] - Epoch 73/100, Val Acc=0.9107, Val Loss=0.2886, lr=0.0010
[2025-04-27 17:54:33,762][train][INFO] - Epoch 72/100, Val Acc=0.8761, Val Loss=0.3715, lr=0.0010
[2025-04-27 17:54:35,855][train][INFO] - Epoch 69/100, Val Acc=0.8714, Val Loss=0.3904, lr=0.0010
[2025-04-27 17:54:37,022][train][INFO] - Epoch 69/100, Val Acc=0.9146, Val Loss=0.2665, lr=0.0010
[2025-04-27 17:54:37,581][train][INFO] - Epoch 70/100, Val Acc=0.9111, Val Loss=0.2834, lr=0.0010
[2025-04-27 17:54:38,393][train][INFO] - Epoch 75/100, Val Acc=0.9096, Val Loss=0.2862, lr=0.0010
[2025-04-27 17:54:38,695][train][INFO] - Epoch 70/100, Val Acc=0.8928, Val Loss=0.3240, lr=0.0010
[2025-04-27 17:54:48,123][train][INFO] - Epoch 74/100, Val Acc=0.9113, Val Loss=0.2849, lr=0.0010
[2025-04-27 17:54:53,115][train][INFO] - Epoch 73/100, Val Acc=0.8732, Val Loss=0.3791, lr=0.0010
[2025-04-27 17:54:55,100][train][INFO] - Epoch 70/100, Val Acc=0.8710, Val Loss=0.3933, lr=0.0010
[2025-04-27 17:54:56,747][train][INFO] - Epoch 70/100, Val Acc=0.9161, Val Loss=0.2700, lr=0.0010
[2025-04-27 17:54:56,957][train][INFO] - Epoch 71/100, Val Acc=0.9095, Val Loss=0.2862, lr=0.0010
[2025-04-27 17:54:57,430][train][INFO] - Epoch 76/100, Val Acc=0.9094, Val Loss=0.2861, lr=0.0010
[2025-04-27 17:54:58,054][train][INFO] - Epoch 71/100, Val Acc=0.8928, Val Loss=0.3294, lr=0.0010
[2025-04-27 17:55:07,414][train][INFO] - Epoch 75/100, Val Acc=0.9089, Val Loss=0.2915, lr=0.0010
[2025-04-27 17:55:12,568][train][INFO] - Epoch 74/100, Val Acc=0.8748, Val Loss=0.3767, lr=0.0010
[2025-04-27 17:55:14,261][train][INFO] - Epoch 71/100, Val Acc=0.8724, Val Loss=0.3889, lr=0.0010
[2025-04-27 17:55:16,559][train][INFO] - Epoch 71/100, Val Acc=0.9135, Val Loss=0.2731, lr=0.0010
[2025-04-27 17:55:16,584][train][INFO] - Epoch 72/100, Val Acc=0.9091, Val Loss=0.2945, lr=0.0010
[2025-04-27 17:55:16,619][train][INFO] - Epoch 77/100, Val Acc=0.9106, Val Loss=0.2848, lr=0.0010
[2025-04-27 17:55:17,587][train][INFO] - Epoch 72/100, Val Acc=0.8940, Val Loss=0.3284, lr=0.0010
[2025-04-27 17:55:26,779][train][INFO] - Epoch 76/100, Val Acc=0.9094, Val Loss=0.2923, lr=0.0010
[2025-04-27 17:55:31,997][train][INFO] - Epoch 75/100, Val Acc=0.8763, Val Loss=0.3701, lr=0.0010
[2025-04-27 17:55:33,674][train][INFO] - Epoch 72/100, Val Acc=0.8734, Val Loss=0.3907, lr=0.0010
[2025-04-27 17:55:35,814][train][INFO] - Epoch 78/100, Val Acc=0.9116, Val Loss=0.2882, lr=0.0010
[2025-04-27 17:55:36,183][train][INFO] - Epoch 73/100, Val Acc=0.9081, Val Loss=0.2921, lr=0.0010
[2025-04-27 17:55:36,453][train][INFO] - Epoch 72/100, Val Acc=0.9141, Val Loss=0.2785, lr=0.0010
[2025-04-27 17:55:36,898][train][INFO] - Epoch 73/100, Val Acc=0.8937, Val Loss=0.3251, lr=0.0010
[2025-04-27 17:55:46,207][train][INFO] - Epoch 77/100, Val Acc=0.9093, Val Loss=0.2888, lr=0.0010
[2025-04-27 17:55:51,426][train][INFO] - Epoch 76/100, Val Acc=0.8743, Val Loss=0.3754, lr=0.0010
[2025-04-27 17:55:52,988][train][INFO] - Epoch 73/100, Val Acc=0.8684, Val Loss=0.3996, lr=0.0010
[2025-04-27 17:55:54,953][train][INFO] - Epoch 79/100, Val Acc=0.9089, Val Loss=0.2919, lr=0.0010
[2025-04-27 17:55:55,653][train][INFO] - Epoch 74/100, Val Acc=0.9102, Val Loss=0.2918, lr=0.0010
[2025-04-27 17:55:56,229][train][INFO] - Epoch 73/100, Val Acc=0.9157, Val Loss=0.2743, lr=0.0010
[2025-04-27 17:55:56,342][train][INFO] - Epoch 74/100, Val Acc=0.8915, Val Loss=0.3394, lr=0.0010
[2025-04-27 17:56:05,474][train][INFO] - Epoch 78/100, Val Acc=0.9103, Val Loss=0.2898, lr=0.0010
[2025-04-27 17:56:10,819][train][INFO] - Epoch 77/100, Val Acc=0.8784, Val Loss=0.3715, lr=0.0010
[2025-04-27 17:56:12,271][train][INFO] - Epoch 74/100, Val Acc=0.8736, Val Loss=0.3910, lr=0.0010
[2025-04-27 17:56:14,092][train][INFO] - Epoch 80/100, Val Acc=0.9089, Val Loss=0.2899, lr=0.0010
[2025-04-27 17:56:15,054][train][INFO] - Epoch 75/100, Val Acc=0.9071, Val Loss=0.2952, lr=0.0010
[2025-04-27 17:56:15,694][train][INFO] - Epoch 75/100, Val Acc=0.8939, Val Loss=0.3279, lr=0.0010
[2025-04-27 17:56:16,038][train][INFO] - Epoch 74/100, Val Acc=0.9163, Val Loss=0.2755, lr=0.0010
[2025-04-27 17:56:24,697][train][INFO] - Epoch 79/100, Val Acc=0.9093, Val Loss=0.2929, lr=0.0010
[2025-04-27 17:56:30,160][train][INFO] - Epoch 78/100, Val Acc=0.8780, Val Loss=0.3710, lr=0.0010
[2025-04-27 17:56:31,493][train][INFO] - Epoch 75/100, Val Acc=0.8724, Val Loss=0.3896, lr=0.0010
[2025-04-27 17:56:33,206][train][INFO] - Epoch 81/100, Val Acc=0.9096, Val Loss=0.2954, lr=0.0010
[2025-04-27 17:56:34,514][train][INFO] - Epoch 76/100, Val Acc=0.9068, Val Loss=0.2974, lr=0.0010
[2025-04-27 17:56:35,012][train][INFO] - Epoch 76/100, Val Acc=0.8944, Val Loss=0.3276, lr=0.0010
[2025-04-27 17:56:35,859][train][INFO] - Epoch 75/100, Val Acc=0.9168, Val Loss=0.2772, lr=0.0010
[2025-04-27 17:56:44,011][train][INFO] - Epoch 80/100, Val Acc=0.9076, Val Loss=0.2948, lr=0.0010
[2025-04-27 17:56:49,446][train][INFO] - Epoch 79/100, Val Acc=0.8766, Val Loss=0.3755, lr=0.0010
[2025-04-27 17:56:50,687][train][INFO] - Epoch 76/100, Val Acc=0.8724, Val Loss=0.3891, lr=0.0010
[2025-04-27 17:56:52,201][train][INFO] - Epoch 82/100, Val Acc=0.9094, Val Loss=0.2893, lr=0.0010
[2025-04-27 17:56:53,791][train][INFO] - Epoch 77/100, Val Acc=0.9088, Val Loss=0.2933, lr=0.0010
[2025-04-27 17:56:54,321][train][INFO] - Epoch 77/100, Val Acc=0.8942, Val Loss=0.3238, lr=0.0010
[2025-04-27 17:56:55,528][train][INFO] - Epoch 76/100, Val Acc=0.9145, Val Loss=0.2802, lr=0.0010
[2025-04-27 17:57:03,225][train][INFO] - Epoch 81/100, Val Acc=0.9101, Val Loss=0.2946, lr=0.0010
[2025-04-27 17:57:08,938][train][INFO] - Epoch 80/100, Val Acc=0.8727, Val Loss=0.3776, lr=0.0010
[2025-04-27 17:57:09,903][train][INFO] - Epoch 77/100, Val Acc=0.8697, Val Loss=0.3986, lr=0.0010
[2025-04-27 17:57:11,334][train][INFO] - Epoch 83/100, Val Acc=0.9073, Val Loss=0.2997, lr=0.0010
[2025-04-27 17:57:12,955][train][INFO] - Epoch 78/100, Val Acc=0.9078, Val Loss=0.2983, lr=0.0010
[2025-04-27 17:57:13,636][train][INFO] - Epoch 78/100, Val Acc=0.8946, Val Loss=0.3241, lr=0.0010
[2025-04-27 17:57:15,046][train][INFO] - Epoch 77/100, Val Acc=0.9171, Val Loss=0.2776, lr=0.0010
[2025-04-27 17:57:22,433][train][INFO] - Epoch 82/100, Val Acc=0.9091, Val Loss=0.2974, lr=0.0010
[2025-04-27 17:57:28,428][train][INFO] - Epoch 81/100, Val Acc=0.8753, Val Loss=0.3733, lr=0.0010
[2025-04-27 17:57:29,043][train][INFO] - Epoch 78/100, Val Acc=0.8719, Val Loss=0.3911, lr=0.0010
[2025-04-27 17:57:30,305][train][INFO] - Epoch 84/100, Val Acc=0.9088, Val Loss=0.3001, lr=0.0010
[2025-04-27 17:57:32,148][train][INFO] - Epoch 79/100, Val Acc=0.9102, Val Loss=0.2968, lr=0.0010
[2025-04-27 17:57:32,848][train][INFO] - Epoch 79/100, Val Acc=0.8964, Val Loss=0.3297, lr=0.0010
[2025-04-27 17:57:34,643][train][INFO] - Epoch 78/100, Val Acc=0.9164, Val Loss=0.2736, lr=0.0010
[2025-04-27 17:57:41,742][train][INFO] - Epoch 83/100, Val Acc=0.9092, Val Loss=0.2916, lr=0.0010
[2025-04-27 17:57:47,830][train][INFO] - Epoch 82/100, Val Acc=0.8730, Val Loss=0.3756, lr=0.0010
[2025-04-27 17:57:48,254][train][INFO] - Epoch 79/100, Val Acc=0.8718, Val Loss=0.3950, lr=0.0010
[2025-04-27 17:57:49,356][train][INFO] - Epoch 85/100, Val Acc=0.9095, Val Loss=0.2983, lr=0.0010
[2025-04-27 17:57:51,433][train][INFO] - Epoch 80/100, Val Acc=0.9074, Val Loss=0.2992, lr=0.0010
[2025-04-27 17:57:52,103][train][INFO] - Epoch 80/100, Val Acc=0.8940, Val Loss=0.3279, lr=0.0010
[2025-04-27 17:57:54,197][train][INFO] - Epoch 79/100, Val Acc=0.9124, Val Loss=0.2864, lr=0.0010
[2025-04-27 17:58:01,038][train][INFO] - Epoch 84/100, Val Acc=0.9087, Val Loss=0.2920, lr=0.0010
[2025-04-27 17:58:07,222][train][INFO] - Epoch 83/100, Val Acc=0.8737, Val Loss=0.3694, lr=0.0010
[2025-04-27 17:58:07,467][train][INFO] - Epoch 80/100, Val Acc=0.8712, Val Loss=0.3977, lr=0.0010
[2025-04-27 17:58:08,429][train][INFO] - Epoch 86/100, Val Acc=0.9063, Val Loss=0.3017, lr=0.0010
[2025-04-27 17:58:10,682][train][INFO] - Epoch 81/100, Val Acc=0.9098, Val Loss=0.2970, lr=0.0010
[2025-04-27 17:58:11,269][train][INFO] - Epoch 81/100, Val Acc=0.8973, Val Loss=0.3263, lr=0.0010
[2025-04-27 17:58:13,854][train][INFO] - Epoch 80/100, Val Acc=0.9143, Val Loss=0.2838, lr=0.0010
[2025-04-27 17:58:20,444][train][INFO] - Epoch 85/100, Val Acc=0.9122, Val Loss=0.2971, lr=0.0010
[2025-04-27 17:58:26,724][train][INFO] - Epoch 84/100, Val Acc=0.8767, Val Loss=0.3742, lr=0.0010
[2025-04-27 17:58:26,743][train][INFO] - Epoch 81/100, Val Acc=0.8728, Val Loss=0.3971, lr=0.0010
[2025-04-27 17:58:27,617][train][INFO] - Epoch 87/100, Val Acc=0.9090, Val Loss=0.2936, lr=0.0010
[2025-04-27 17:58:29,986][train][INFO] - Epoch 82/100, Val Acc=0.9073, Val Loss=0.3051, lr=0.0010
[2025-04-27 17:58:30,417][train][INFO] - Epoch 82/100, Val Acc=0.8962, Val Loss=0.3349, lr=0.0010
[2025-04-27 17:58:33,305][train][INFO] - Epoch 81/100, Val Acc=0.9150, Val Loss=0.2787, lr=0.0010
[2025-04-27 17:58:39,790][train][INFO] - Epoch 86/100, Val Acc=0.9083, Val Loss=0.2965, lr=0.0010
[2025-04-27 17:58:46,026][train][INFO] - Epoch 82/100, Val Acc=0.8723, Val Loss=0.3907, lr=0.0010
[2025-04-27 17:58:46,184][train][INFO] - Epoch 85/100, Val Acc=0.8762, Val Loss=0.3731, lr=0.0010
[2025-04-27 17:58:46,665][train][INFO] - Epoch 88/100, Val Acc=0.9077, Val Loss=0.3020, lr=0.0010
[2025-04-27 17:58:49,430][train][INFO] - Epoch 83/100, Val Acc=0.9095, Val Loss=0.3013, lr=0.0010
[2025-04-27 17:58:49,697][train][INFO] - Epoch 83/100, Val Acc=0.8965, Val Loss=0.3289, lr=0.0010
[2025-04-27 17:58:52,952][train][INFO] - Epoch 82/100, Val Acc=0.9149, Val Loss=0.2832, lr=0.0010
[2025-04-27 17:58:59,049][train][INFO] - Epoch 87/100, Val Acc=0.9086, Val Loss=0.2998, lr=0.0010
[2025-04-27 17:59:05,279][train][INFO] - Epoch 83/100, Val Acc=0.8734, Val Loss=0.3922, lr=0.0010
[2025-04-27 17:59:05,716][train][INFO] - Epoch 86/100, Val Acc=0.8751, Val Loss=0.3753, lr=0.0010
[2025-04-27 17:59:05,911][train][INFO] - Epoch 89/100, Val Acc=0.9107, Val Loss=0.3009, lr=0.0010
[2025-04-27 17:59:08,828][train][INFO] - Epoch 84/100, Val Acc=0.9093, Val Loss=0.3066, lr=0.0010
[2025-04-27 17:59:08,991][train][INFO] - Epoch 84/100, Val Acc=0.8936, Val Loss=0.3330, lr=0.0010
[2025-04-27 17:59:12,467][train][INFO] - Epoch 83/100, Val Acc=0.9162, Val Loss=0.2822, lr=0.0010
[2025-04-27 17:59:18,352][train][INFO] - Epoch 88/100, Val Acc=0.9091, Val Loss=0.2991, lr=0.0010
[2025-04-27 17:59:24,565][train][INFO] - Epoch 84/100, Val Acc=0.8712, Val Loss=0.3952, lr=0.0010
[2025-04-27 17:59:25,331][train][INFO] - Epoch 87/100, Val Acc=0.8734, Val Loss=0.3836, lr=0.0010
[2025-04-27 17:59:25,350][train][INFO] - Epoch 90/100, Val Acc=0.9098, Val Loss=0.2981, lr=0.0010
[2025-04-27 17:59:28,162][train][INFO] - Epoch 85/100, Val Acc=0.9090, Val Loss=0.3015, lr=0.0010
[2025-04-27 17:59:28,304][train][INFO] - Epoch 85/100, Val Acc=0.8926, Val Loss=0.3381, lr=0.0010
[2025-04-27 17:59:31,998][train][INFO] - Epoch 84/100, Val Acc=0.9167, Val Loss=0.2840, lr=0.0010
[2025-04-27 17:59:37,582][train][INFO] - Epoch 89/100, Val Acc=0.9081, Val Loss=0.3006, lr=0.0010
[2025-04-27 17:59:43,894][train][INFO] - Epoch 85/100, Val Acc=0.8725, Val Loss=0.3932, lr=0.0010
[2025-04-27 17:59:44,713][train][INFO] - Epoch 91/100, Val Acc=0.9113, Val Loss=0.2949, lr=0.0001
[2025-04-27 17:59:44,961][train][INFO] - Epoch 88/100, Val Acc=0.8733, Val Loss=0.3833, lr=0.0010
[2025-04-27 17:59:47,534][train][INFO] - Epoch 86/100, Val Acc=0.9088, Val Loss=0.3017, lr=0.0010
[2025-04-27 17:59:47,671][train][INFO] - Epoch 86/100, Val Acc=0.8927, Val Loss=0.3340, lr=0.0010
[2025-04-27 17:59:51,543][train][INFO] - Epoch 85/100, Val Acc=0.9155, Val Loss=0.2883, lr=0.0010
[2025-04-27 17:59:56,910][train][INFO] - Epoch 90/100, Val Acc=0.9112, Val Loss=0.3018, lr=0.0010
[2025-04-27 18:00:03,219][train][INFO] - Epoch 86/100, Val Acc=0.8750, Val Loss=0.3904, lr=0.0010
[2025-04-27 18:00:04,132][train][INFO] - Epoch 92/100, Val Acc=0.9113, Val Loss=0.2910, lr=0.0001
[2025-04-27 18:00:04,571][train][INFO] - Epoch 89/100, Val Acc=0.8745, Val Loss=0.3748, lr=0.0010
[2025-04-27 18:00:07,077][train][INFO] - Epoch 87/100, Val Acc=0.9080, Val Loss=0.3080, lr=0.0010
[2025-04-27 18:00:07,110][train][INFO] - Epoch 87/100, Val Acc=0.8954, Val Loss=0.3345, lr=0.0010
[2025-04-27 18:00:11,128][train][INFO] - Epoch 86/100, Val Acc=0.9170, Val Loss=0.2825, lr=0.0010
[2025-04-27 18:00:16,223][train][INFO] - Epoch 91/100, Val Acc=0.9128, Val Loss=0.2970, lr=0.0001
[2025-04-27 18:00:22,554][train][INFO] - Epoch 87/100, Val Acc=0.8751, Val Loss=0.3867, lr=0.0010
[2025-04-27 18:00:23,235][train][INFO] - Epoch 93/100, Val Acc=0.9111, Val Loss=0.2934, lr=0.0001
[2025-04-27 18:00:23,985][train][INFO] - Epoch 90/100, Val Acc=0.8747, Val Loss=0.3762, lr=0.0010
[2025-04-27 18:00:26,512][train][INFO] - Epoch 88/100, Val Acc=0.8929, Val Loss=0.3354, lr=0.0010
[2025-04-27 18:00:26,613][train][INFO] - Epoch 88/100, Val Acc=0.9094, Val Loss=0.3009, lr=0.0010
[2025-04-27 18:00:30,745][train][INFO] - Epoch 87/100, Val Acc=0.9173, Val Loss=0.2847, lr=0.0010
[2025-04-27 18:00:35,449][train][INFO] - Epoch 92/100, Val Acc=0.9102, Val Loss=0.2990, lr=0.0001
[2025-04-27 18:00:41,879][train][INFO] - Epoch 88/100, Val Acc=0.8748, Val Loss=0.3890, lr=0.0010
[2025-04-27 18:00:42,344][train][INFO] - Epoch 94/100, Val Acc=0.9113, Val Loss=0.2913, lr=0.0001
[2025-04-27 18:00:43,592][train][INFO] - Epoch 91/100, Val Acc=0.8787, Val Loss=0.3643, lr=0.0001
[2025-04-27 18:00:45,874][train][INFO] - Epoch 89/100, Val Acc=0.8946, Val Loss=0.3421, lr=0.0010
[2025-04-27 18:00:46,084][train][INFO] - Epoch 89/100, Val Acc=0.9072, Val Loss=0.3082, lr=0.0010
[2025-04-27 18:00:50,341][train][INFO] - Epoch 88/100, Val Acc=0.9173, Val Loss=0.2815, lr=0.0010
[2025-04-27 18:00:54,836][train][INFO] - Epoch 93/100, Val Acc=0.9108, Val Loss=0.2970, lr=0.0001
[2025-04-27 18:01:01,140][train][INFO] - Epoch 89/100, Val Acc=0.8733, Val Loss=0.3925, lr=0.0010
[2025-04-27 18:01:01,390][train][INFO] - Epoch 95/100, Val Acc=0.9107, Val Loss=0.2933, lr=0.0001
[2025-04-27 18:01:03,074][train][INFO] - Epoch 92/100, Val Acc=0.8789, Val Loss=0.3657, lr=0.0001
[2025-04-27 18:01:05,176][train][INFO] - Epoch 90/100, Val Acc=0.8948, Val Loss=0.3359, lr=0.0010
[2025-04-27 18:01:05,426][train][INFO] - Epoch 90/100, Val Acc=0.9085, Val Loss=0.3058, lr=0.0010
[2025-04-27 18:01:09,925][train][INFO] - Epoch 89/100, Val Acc=0.9157, Val Loss=0.2842, lr=0.0010
[2025-04-27 18:01:14,232][train][INFO] - Epoch 94/100, Val Acc=0.9118, Val Loss=0.2930, lr=0.0001
[2025-04-27 18:01:20,391][train][INFO] - Epoch 96/100, Val Acc=0.9100, Val Loss=0.2910, lr=0.0001
[2025-04-27 18:01:20,410][train][INFO] - Epoch 90/100, Val Acc=0.8702, Val Loss=0.3969, lr=0.0010
[2025-04-27 18:01:22,573][train][INFO] - Epoch 93/100, Val Acc=0.8793, Val Loss=0.3643, lr=0.0001
[2025-04-27 18:01:24,451][train][INFO] - Epoch 91/100, Val Acc=0.8962, Val Loss=0.3310, lr=0.0001
[2025-04-27 18:01:24,742][train][INFO] - Epoch 91/100, Val Acc=0.9101, Val Loss=0.3029, lr=0.0001
[2025-04-27 18:01:29,501][train][INFO] - Epoch 90/100, Val Acc=0.9161, Val Loss=0.2844, lr=0.0010
[2025-04-27 18:01:33,594][train][INFO] - Epoch 95/100, Val Acc=0.9109, Val Loss=0.2956, lr=0.0001
[2025-04-27 18:01:39,461][train][INFO] - Epoch 97/100, Val Acc=0.9117, Val Loss=0.2939, lr=0.0001
[2025-04-27 18:01:39,722][train][INFO] - Epoch 91/100, Val Acc=0.8750, Val Loss=0.3875, lr=0.0001
[2025-04-27 18:01:41,952][train][INFO] - Epoch 94/100, Val Acc=0.8785, Val Loss=0.3677, lr=0.0001
[2025-04-27 18:01:43,751][train][INFO] - Epoch 92/100, Val Acc=0.8955, Val Loss=0.3295, lr=0.0001
[2025-04-27 18:01:44,084][train][INFO] - Epoch 92/100, Val Acc=0.9098, Val Loss=0.3031, lr=0.0001
[2025-04-27 18:01:49,134][train][INFO] - Epoch 91/100, Val Acc=0.9175, Val Loss=0.2849, lr=0.0001
[2025-04-27 18:01:52,988][train][INFO] - Epoch 96/100, Val Acc=0.9116, Val Loss=0.2948, lr=0.0001
[2025-04-27 18:01:58,404][train][INFO] - Epoch 98/100, Val Acc=0.9108, Val Loss=0.2927, lr=0.0001
[2025-04-27 18:01:59,005][train][INFO] - Epoch 92/100, Val Acc=0.8763, Val Loss=0.3854, lr=0.0001
[2025-04-27 18:02:01,345][train][INFO] - Epoch 95/100, Val Acc=0.8784, Val Loss=0.3667, lr=0.0001
[2025-04-27 18:02:02,796][train][INFO] - Epoch 93/100, Val Acc=0.8962, Val Loss=0.3286, lr=0.0001
[2025-04-27 18:02:03,376][train][INFO] - Epoch 93/100, Val Acc=0.9084, Val Loss=0.3011, lr=0.0001
[2025-04-27 18:02:08,759][train][INFO] - Epoch 92/100, Val Acc=0.9162, Val Loss=0.2822, lr=0.0001
[2025-04-27 18:02:12,316][train][INFO] - Epoch 97/100, Val Acc=0.9118, Val Loss=0.2964, lr=0.0001
[2025-04-27 18:02:17,424][train][INFO] - Epoch 99/100, Val Acc=0.9104, Val Loss=0.2934, lr=0.0001
[2025-04-27 18:02:18,147][train][INFO] - Epoch 93/100, Val Acc=0.8757, Val Loss=0.3872, lr=0.0001
[2025-04-27 18:02:20,764][train][INFO] - Epoch 96/100, Val Acc=0.8771, Val Loss=0.3669, lr=0.0001
[2025-04-27 18:02:21,969][train][INFO] - Epoch 94/100, Val Acc=0.8953, Val Loss=0.3293, lr=0.0001
[2025-04-27 18:02:22,692][train][INFO] - Epoch 94/100, Val Acc=0.9083, Val Loss=0.3002, lr=0.0001
[2025-04-27 18:02:28,359][train][INFO] - Epoch 93/100, Val Acc=0.9166, Val Loss=0.2823, lr=0.0001
[2025-04-27 18:02:31,560][train][INFO] - Epoch 98/100, Val Acc=0.9125, Val Loss=0.2934, lr=0.0001
[2025-04-27 18:02:36,450][train][INFO] - Epoch 100/100, Val Acc=0.9109, Val Loss=0.2965, lr=0.0001
[2025-04-27 18:02:37,404][train][INFO] - Epoch 94/100, Val Acc=0.8749, Val Loss=0.3848, lr=0.0001
[2025-04-27 18:02:39,828][train][INFO] - Epoch 97/100, Val Acc=0.8800, Val Loss=0.3655, lr=0.0001
[2025-04-27 18:02:41,235][train][INFO] - Epoch 95/100, Val Acc=0.8965, Val Loss=0.3309, lr=0.0001
[2025-04-27 18:02:42,004][train][INFO] - Epoch 95/100, Val Acc=0.9095, Val Loss=0.2999, lr=0.0001
[2025-04-27 18:02:43,514][train][INFO] - After training : Train Acc=0.9818  Val Acc=0.9117
[2025-04-27 18:02:43,525][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:02:48,075][train][INFO] - Epoch 94/100, Val Acc=0.9159, Val Loss=0.2842, lr=0.0001
[2025-04-27 18:02:49,232][train][INFO] - Epoch 99/100, Val Acc=0.9108, Val Loss=0.2949, lr=0.0001
[2025-04-27 18:02:56,735][train][INFO] - Epoch 95/100, Val Acc=0.8757, Val Loss=0.3865, lr=0.0001
[2025-04-27 18:02:57,073][train][INFO] - Epoch 98/100, Val Acc=0.8788, Val Loss=0.3625, lr=0.0001
[2025-04-27 18:03:00,517][train][INFO] - Epoch 96/100, Val Acc=0.8961, Val Loss=0.3286, lr=0.0001
[2025-04-27 18:03:01,369][train][INFO] - Epoch 96/100, Val Acc=0.9089, Val Loss=0.3007, lr=0.0001
[2025-04-27 18:03:05,934][train][INFO] - Epoch 100/100, Val Acc=0.9120, Val Loss=0.2949, lr=0.0001
[2025-04-27 18:03:07,846][train][INFO] - Epoch 95/100, Val Acc=0.9148, Val Loss=0.2834, lr=0.0001
[2025-04-27 18:03:12,329][train][INFO] - After training : Train Acc=0.9739  Val Acc=0.9128
[2025-04-27 18:03:12,336][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:03:13,397][train][INFO] - Epoch 99/100, Val Acc=0.8786, Val Loss=0.3637, lr=0.0001
[2025-04-27 18:03:16,092][train][INFO] - Epoch 96/100, Val Acc=0.8767, Val Loss=0.3817, lr=0.0001
[2025-04-27 18:03:19,974][train][INFO] - Epoch 97/100, Val Acc=0.8957, Val Loss=0.3309, lr=0.0001
[2025-04-27 18:03:20,837][train][INFO] - Epoch 97/100, Val Acc=0.9104, Val Loss=0.2996, lr=0.0001
[2025-04-27 18:03:27,503][train][INFO] - Epoch 96/100, Val Acc=0.9160, Val Loss=0.2834, lr=0.0001
[2025-04-27 18:03:28,912][train][INFO] - Epoch 100/100, Val Acc=0.8804, Val Loss=0.3637, lr=0.0001
[2025-04-27 18:03:35,184][train][INFO] - After training : Train Acc=0.9244  Val Acc=0.8804
[2025-04-27 18:03:35,191][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:03:35,350][train][INFO] - Epoch 97/100, Val Acc=0.8759, Val Loss=0.3853, lr=0.0001
[2025-04-27 18:03:39,360][train][INFO] - Epoch 98/100, Val Acc=0.8958, Val Loss=0.3285, lr=0.0001
[2025-04-27 18:03:40,267][train][INFO] - Epoch 98/100, Val Acc=0.9089, Val Loss=0.3011, lr=0.0001
[2025-04-27 18:03:47,134][train][INFO] - Epoch 97/100, Val Acc=0.9172, Val Loss=0.2842, lr=0.0001
[2025-04-27 18:03:54,798][train][INFO] - Epoch 98/100, Val Acc=0.8773, Val Loss=0.3835, lr=0.0001
[2025-04-27 18:03:58,805][train][INFO] - Epoch 99/100, Val Acc=0.8954, Val Loss=0.3314, lr=0.0001
[2025-04-27 18:03:59,839][train][INFO] - Epoch 99/100, Val Acc=0.9096, Val Loss=0.3015, lr=0.0001
[2025-04-27 18:04:06,834][train][INFO] - Epoch 98/100, Val Acc=0.9164, Val Loss=0.2824, lr=0.0001
[2025-04-27 18:04:14,395][train][INFO] - Epoch 99/100, Val Acc=0.8774, Val Loss=0.3815, lr=0.0001
[2025-04-27 18:04:18,358][train][INFO] - Epoch 100/100, Val Acc=0.8968, Val Loss=0.3294, lr=0.0001
[2025-04-27 18:04:19,454][train][INFO] - Epoch 100/100, Val Acc=0.9108, Val Loss=0.3003, lr=0.0001
[2025-04-27 18:04:25,118][train][INFO] - After training : Train Acc=0.9490  Val Acc=0.8973
[2025-04-27 18:04:25,124][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:04:25,526][train][INFO] - Epoch 99/100, Val Acc=0.9148, Val Loss=0.2874, lr=0.0001
[2025-04-27 18:04:26,212][train][INFO] - After training : Train Acc=0.9724  Val Acc=0.9111
[2025-04-27 18:04:26,223][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:04:31,460][train][INFO] - Epoch 100/100, Val Acc=0.8787, Val Loss=0.3833, lr=0.0001
[2025-04-27 18:04:34,670][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:04:38,103][train][INFO] - After training : Train Acc=0.9165  Val Acc=0.8787
[2025-04-27 18:04:38,111][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:04:41,845][train][INFO] - Epoch 100/100, Val Acc=0.9161, Val Loss=0.2855, lr=0.0001
[2025-04-27 18:04:49,063][train][INFO] - After training : Train Acc=0.9837  Val Acc=0.9175
[2025-04-27 18:04:49,070][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 18:05:00,716][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:05:59,227][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:05:59,421][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:06:01,605][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:06:36,261][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:06:40,083][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 18:08:02,076][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:08:03,024][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:08:41,253][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:08:41,878][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:08:58,749][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:08:59,281][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:09:01,822][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:09:02,314][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:09:23,375][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:09:23,847][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:09:44,220][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:09:44,696][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 18:09:46,772][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 18:09:47,237][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 22:10:55,892][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 100

Error executing job with overrides: ['run=pretrain_final', 'index=100']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 78, in main
    seed = cfg.seed
omegaconf.errors.ConfigAttributeError: Key 'seed' is not in struct
    full_key: task.seed
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 22:11:16,870][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 101

Error executing job with overrides: ['run=pretrain_final', 'index=101']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 78, in main
    seed = cfg.seed
omegaconf.errors.ConfigAttributeError: Key 'seed' is not in struct
    full_key: task.seed
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 22:13:13,279][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 101

[2025-04-27 22:13:13,443][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:13:13,444][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:13:13,444][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:13:17,119][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 100

[2025-04-27 22:13:17,257][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:13:17,257][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:13:17,257][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:13:24,788][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Error executing job with overrides: ['run=pretrain_final', 'index=101']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 203, in main
    train(model, small_train_loader, big_test_loader, 200, 0.1, "100, 150, 180", log=log)
  File "/home/liuyewei/metanetwork/meta-pruning/utils/train.py", line 86, in train
    loss.backward()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 22:13:32,110][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Error executing job with overrides: ['run=pretrain_final', 'index=100']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 203, in main
    train(model, small_train_loader, big_test_loader, 200, 0.1, "100, 150, 180", log=log)
  File "/home/liuyewei/metanetwork/meta-pruning/utils/train.py", line 86, in train
    loss.backward()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2025-04-27 22:14:40,915][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 100

[2025-04-27 22:14:41,051][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:14:41,051][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:14:41,051][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:14:52,911][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
Error executing job with overrides: ['run=pretrain_final', 'index=100']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 218, in <module>
    main()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 203, in main
    train(model, small_train_loader, big_test_loader, 200, 0.1, "100, 150, 180", log=log)
  File "/home/liuyewei/metanetwork/meta-pruning/utils/train.py", line 86, in train
    loss.backward()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
[2025-04-27 22:24:37,017][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 100

[2025-04-27 22:24:37,151][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:24:37,152][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:24:37,152][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:24:49,483][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:25:07,726][train][INFO] - Epoch 1/200, Val Acc=0.4063, Val Loss=1.5948, lr=0.1000
[2025-04-27 22:25:16,932][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: pretrain_final
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 101

[2025-04-27 22:25:17,077][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:25:17,077][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:25:17,077][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:25:25,883][train][INFO] - Epoch 2/200, Val Acc=0.5384, Val Loss=1.2947, lr=0.1000
[2025-04-27 22:25:35,885][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:25:47,415][train][INFO] - Epoch 3/200, Val Acc=0.6384, Val Loss=1.0788, lr=0.1000
[2025-04-27 22:25:59,827][train][INFO] - Epoch 1/200, Val Acc=0.4063, Val Loss=1.5948, lr=0.1000
[2025-04-27 22:26:10,070][train][INFO] - Epoch 4/200, Val Acc=0.6495, Val Loss=1.0515, lr=0.1000
[2025-04-27 22:26:23,419][train][INFO] - Epoch 2/200, Val Acc=0.5384, Val Loss=1.2947, lr=0.1000
[2025-04-27 22:26:33,230][train][INFO] - Epoch 5/200, Val Acc=0.6678, Val Loss=1.0129, lr=0.1000
[2025-04-27 22:26:45,811][train][INFO] - Epoch 3/200, Val Acc=0.6384, Val Loss=1.0788, lr=0.1000
[2025-04-27 22:26:56,438][train][INFO] - Epoch 6/200, Val Acc=0.7251, Val Loss=0.8540, lr=0.1000
[2025-04-27 22:27:09,368][train][INFO] - Epoch 4/200, Val Acc=0.6495, Val Loss=1.0515, lr=0.1000
[2025-04-27 22:30:01,780][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 10

[2025-04-27 22:30:01,940][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:30:01,940][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:30:01,940][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:30:13,156][train][INFO] - Before training : Train Acc=0.1237  Val Acc=0.1256
[2025-04-27 22:30:14,046][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 20

[2025-04-27 22:30:14,178][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:30:14,178][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:30:14,178][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:30:16,826][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 30

[2025-04-27 22:30:16,976][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:30:16,976][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:30:16,976][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:30:19,585][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 40

[2025-04-27 22:30:19,723][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:30:19,723][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:30:19,723][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Error executing job with overrides: ['index=30']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 218, in <module>
    main()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 122, in main
    new_model = get_new_model(metanetwork)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 111, in get_new_model
    node_pred, edge_pred = metanetwork.forward(node_features.to(device), edge_index.to(device), edge_features.to(device))
  File "/home/liuyewei/metanetwork/meta-pruning/nn/GNN.py", line 47, in forward
    ret_node1, ret_edge1 = self.convs[i].forward(self.norm(hidden), edge_index, edge_attr) 
  File "/home/liuyewei/metanetwork/meta-pruning/nn/GNN.py", line 17, in forward
    ret_node = self.downlin(self.propagate(edge_index, x=self.lin1(x), z=self.lin2(x), edge_attr=edge_attr))
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 484, in propagate
    out = self.aggregate(out, **aggr_kwargs)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 608, in aggregate
    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py", line 116, in __call__
    raise e
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py", line 109, in __call__
    return super().__call__(x, index, ptr, dim_size, dim, **kwargs)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/multi.py", line 163, in forward
    fused_outs = self.fused_aggr(x, index, ptr, dim_size, dim)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py", line 116, in __call__
    raise e
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/base.py", line 109, in __call__
    return super().__call__(x, index, ptr, dim_size, dim, **kwargs)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/nn/aggr/fused.py", line 230, in forward
    out = scatter(x, index, 0, dim_size, reduce=reduce)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch_geometric/utils/scatter.py", line 74, in scatter
    return src.new_zeros(size).scatter_add_(dim, index, src)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB (GPU 0; 23.65 GiB total capacity; 3.77 GiB already allocated; 10.50 MiB free; 3.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Error executing job with overrides: ['index=40']
Traceback (most recent call last):
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 218, in <module>
    main()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 121, in main
    metanetwork = load_metanetwork(index)
  File "/home/liuyewei/metanetwork/meta-pruning/main/main.py", line 108, in load_metanetwork
    metanetwork = torch.load(matching_files[0])
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 809, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 1172, in _load
    result = unpickler.load()
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 1142, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 1116, in load_tensor
    wrap_storage=restore_location(storage, location),
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 217, in default_restore_location
    result = fn(storage, location)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/serialization.py", line 187, in _cuda_deserialize
    return obj.cuda(device)
  File "/home/liuyewei/miniconda3/envs/meta-pruning/lib/python3.9/site-packages/torch/_utils.py", line 81, in _cuda
    untyped_storage = torch.UntypedStorage(
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[2025-04-27 22:30:26,367][train][INFO] - Before training : Train Acc=0.1077  Val Acc=0.1099
[2025-04-27 22:30:29,264][train][INFO] - Epoch 1/100, Val Acc=0.8801, Val Loss=0.4023, lr=0.0100
[2025-04-27 22:30:34,938][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Oli
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 10

[2025-04-27 22:30:35,092][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:30:35,092][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:30:35,092][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:30:45,121][train][INFO] - Epoch 1/100, Val Acc=0.8805, Val Loss=0.3661, lr=0.0100
[2025-04-27 22:30:46,223][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:30:48,058][train][INFO] - Epoch 2/100, Val Acc=0.8581, Val Loss=0.4681, lr=0.0100
[2025-04-27 22:31:00,807][train][INFO] - Epoch 1/100, Val Acc=0.5728, Val Loss=1.2273, lr=0.0100
[2025-04-27 22:31:03,446][train][INFO] - Epoch 2/100, Val Acc=0.8507, Val Loss=0.4734, lr=0.0100
[2025-04-27 22:31:07,033][train][INFO] - Epoch 3/100, Val Acc=0.8896, Val Loss=0.3670, lr=0.0100
[2025-04-27 22:31:15,769][train][INFO] - Epoch 2/100, Val Acc=0.6714, Val Loss=0.9443, lr=0.0100
[2025-04-27 22:32:29,654][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Oli
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 10

[2025-04-27 22:32:29,785][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:32:29,785][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:32:29,785][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:32:33,927][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Oli
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 20

[2025-04-27 22:32:34,123][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:32:34,123][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:32:34,123][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:32:40,769][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:32:45,581][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:32:47,726][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 10

[2025-04-27 22:32:47,861][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:32:47,861][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:32:47,861][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:32:52,632][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 20

[2025-04-27 22:32:52,769][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:32:52,769][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:32:52,769][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:32:58,751][train][INFO] - Before training : Train Acc=0.1237  Val Acc=0.1256
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:32:59,013][train][INFO] - Epoch 1/100, Val Acc=0.5728, Val Loss=1.2273, lr=0.0100
[2025-04-27 22:33:04,489][train][INFO] - Epoch 1/100, Val Acc=0.4540, Val Loss=1.4599, lr=0.0100
[2025-04-27 22:33:04,509][train][INFO] - Before training : Train Acc=0.1077  Val Acc=0.1099
[2025-04-27 22:33:12,740][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 10

[2025-04-27 22:33:12,886][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:33:12,886][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:33:12,886][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:33:15,947][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index: 20

[2025-04-27 22:33:16,099][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 22:33:16,099][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 22:33:16,099][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 22:33:16,370][train][INFO] - Epoch 1/100, Val Acc=0.8801, Val Loss=0.4023, lr=0.0100
[2025-04-27 22:33:17,732][train][INFO] - Epoch 2/100, Val Acc=0.6714, Val Loss=0.9443, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:33:23,109][train][INFO] - Epoch 2/100, Val Acc=0.5920, Val Loss=1.1771, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 22:33:23,612][train][INFO] - Epoch 1/100, Val Acc=0.8805, Val Loss=0.3661, lr=0.0100
[2025-04-27 22:33:25,837][train][INFO] - Before training : Train Acc=0.1174  Val Acc=0.1204
[2025-04-27 22:33:28,954][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 22:33:35,318][train][INFO] - Epoch 2/100, Val Acc=0.8581, Val Loss=0.4681, lr=0.0100
[2025-04-27 22:33:36,697][train][INFO] - Epoch 3/100, Val Acc=0.6978, Val Loss=0.8951, lr=0.0100
[2025-04-27 22:33:41,649][train][INFO] - Epoch 3/100, Val Acc=0.6275, Val Loss=1.0936, lr=0.0100
[2025-04-27 22:33:42,394][train][INFO] - Epoch 2/100, Val Acc=0.8507, Val Loss=0.4734, lr=0.0100
[2025-04-27 22:33:44,531][train][INFO] - Epoch 1/100, Val Acc=0.8437, Val Loss=0.5158, lr=0.0100
[2025-04-27 22:33:48,205][train][INFO] - Epoch 1/100, Val Acc=0.8299, Val Loss=0.5144, lr=0.0100
[2025-04-27 22:33:54,237][train][INFO] - Epoch 3/100, Val Acc=0.8896, Val Loss=0.3670, lr=0.0100
[2025-04-27 22:33:55,349][train][INFO] - Epoch 4/100, Val Acc=0.6646, Val Loss=1.0528, lr=0.0100
[2025-04-27 22:34:00,097][train][INFO] - Epoch 4/100, Val Acc=0.6680, Val Loss=0.9860, lr=0.0100
[2025-04-27 22:34:00,980][train][INFO] - Epoch 3/100, Val Acc=0.8403, Val Loss=0.5067, lr=0.0100
[2025-04-27 22:34:03,303][train][INFO] - Epoch 2/100, Val Acc=0.8427, Val Loss=0.5075, lr=0.0100
[2025-04-27 22:34:06,709][train][INFO] - Epoch 2/100, Val Acc=0.8536, Val Loss=0.4424, lr=0.0100
[2025-04-27 22:34:12,994][train][INFO] - Epoch 4/100, Val Acc=0.8851, Val Loss=0.3813, lr=0.0100
[2025-04-27 22:34:13,814][train][INFO] - Epoch 5/100, Val Acc=0.7360, Val Loss=0.8224, lr=0.0100
[2025-04-27 22:34:18,461][train][INFO] - Epoch 5/100, Val Acc=0.5802, Val Loss=1.4534, lr=0.0100
[2025-04-27 22:34:19,761][train][INFO] - Epoch 4/100, Val Acc=0.9005, Val Loss=0.3198, lr=0.0100
[2025-04-27 22:34:21,937][train][INFO] - Epoch 3/100, Val Acc=0.8780, Val Loss=0.4120, lr=0.0100
[2025-04-27 22:34:25,423][train][INFO] - Epoch 3/100, Val Acc=0.8724, Val Loss=0.3992, lr=0.0100
[2025-04-27 22:34:31,645][train][INFO] - Epoch 5/100, Val Acc=0.8959, Val Loss=0.3379, lr=0.0100
[2025-04-27 22:34:32,445][train][INFO] - Epoch 6/100, Val Acc=0.7534, Val Loss=0.7328, lr=0.0100
[2025-04-27 22:34:36,868][train][INFO] - Epoch 6/100, Val Acc=0.6676, Val Loss=0.9479, lr=0.0100
[2025-04-27 22:34:38,246][train][INFO] - Epoch 5/100, Val Acc=0.8892, Val Loss=0.3543, lr=0.0100
[2025-04-27 22:34:40,266][train][INFO] - Epoch 4/100, Val Acc=0.8680, Val Loss=0.4296, lr=0.0100
[2025-04-27 22:34:43,463][train][INFO] - Epoch 4/100, Val Acc=0.8684, Val Loss=0.4132, lr=0.0100
[2025-04-27 22:34:50,519][train][INFO] - Epoch 6/100, Val Acc=0.9019, Val Loss=0.3234, lr=0.0100
[2025-04-27 22:34:51,260][train][INFO] - Epoch 7/100, Val Acc=0.7959, Val Loss=0.6094, lr=0.0100
[2025-04-27 22:34:55,384][train][INFO] - Epoch 7/100, Val Acc=0.6601, Val Loss=1.0170, lr=0.0100
[2025-04-27 22:34:56,986][train][INFO] - Epoch 6/100, Val Acc=0.8918, Val Loss=0.3488, lr=0.0100
[2025-04-27 22:34:58,682][train][INFO] - Epoch 5/100, Val Acc=0.9049, Val Loss=0.3079, lr=0.0100
[2025-04-27 22:35:01,870][train][INFO] - Epoch 5/100, Val Acc=0.8852, Val Loss=0.3557, lr=0.0100
[2025-04-27 22:35:09,405][train][INFO] - Epoch 7/100, Val Acc=0.8984, Val Loss=0.3387, lr=0.0100
[2025-04-27 22:35:10,103][train][INFO] - Epoch 8/100, Val Acc=0.7980, Val Loss=0.5934, lr=0.0100
[2025-04-27 22:35:13,890][train][INFO] - Epoch 8/100, Val Acc=0.7484, Val Loss=0.7222, lr=0.0100
[2025-04-27 22:35:15,539][train][INFO] - Epoch 7/100, Val Acc=0.8990, Val Loss=0.3303, lr=0.0100
[2025-04-27 22:35:17,127][train][INFO] - Epoch 6/100, Val Acc=0.8870, Val Loss=0.3693, lr=0.0100
[2025-04-27 22:35:20,172][train][INFO] - Epoch 6/100, Val Acc=0.8960, Val Loss=0.3194, lr=0.0100
[2025-04-27 22:35:28,366][train][INFO] - Epoch 8/100, Val Acc=0.9005, Val Loss=0.3246, lr=0.0100
[2025-04-27 22:35:28,452][train][INFO] - Epoch 9/100, Val Acc=0.7852, Val Loss=0.6482, lr=0.0100
[2025-04-27 22:35:32,367][train][INFO] - Epoch 9/100, Val Acc=0.7382, Val Loss=0.7744, lr=0.0100
[2025-04-27 22:35:33,982][train][INFO] - Epoch 8/100, Val Acc=0.8996, Val Loss=0.3233, lr=0.0100
[2025-04-27 22:35:35,527][train][INFO] - Epoch 7/100, Val Acc=0.8878, Val Loss=0.3569, lr=0.0100
[2025-04-27 22:35:38,843][train][INFO] - Epoch 7/100, Val Acc=0.8825, Val Loss=0.3734, lr=0.0100
[2025-04-27 22:35:46,986][train][INFO] - Epoch 9/100, Val Acc=0.8994, Val Loss=0.3361, lr=0.0100
[2025-04-27 22:35:47,034][train][INFO] - Epoch 10/100, Val Acc=0.8244, Val Loss=0.5056, lr=0.0100
[2025-04-27 22:35:50,725][train][INFO] - Epoch 10/100, Val Acc=0.7326, Val Loss=0.7777, lr=0.0100
[2025-04-27 22:35:52,473][train][INFO] - Epoch 9/100, Val Acc=0.9003, Val Loss=0.3446, lr=0.0100
[2025-04-27 22:35:54,070][train][INFO] - Epoch 8/100, Val Acc=0.8913, Val Loss=0.3633, lr=0.0100
[2025-04-27 22:35:57,514][train][INFO] - Epoch 8/100, Val Acc=0.8911, Val Loss=0.3260, lr=0.0100
[2025-04-27 22:36:05,443][train][INFO] - Epoch 11/100, Val Acc=0.8167, Val Loss=0.5591, lr=0.0100
[2025-04-27 22:36:05,924][train][INFO] - Epoch 10/100, Val Acc=0.9075, Val Loss=0.3075, lr=0.0100
[2025-04-27 22:36:09,363][train][INFO] - Epoch 11/100, Val Acc=0.7203, Val Loss=0.8786, lr=0.0100
[2025-04-27 22:36:11,294][train][INFO] - Epoch 10/100, Val Acc=0.9012, Val Loss=0.3131, lr=0.0100
[2025-04-27 22:36:12,484][train][INFO] - Epoch 9/100, Val Acc=0.9119, Val Loss=0.2925, lr=0.0100
[2025-04-27 22:36:16,116][train][INFO] - Epoch 9/100, Val Acc=0.8947, Val Loss=0.3203, lr=0.0100
[2025-04-27 22:36:24,159][train][INFO] - Epoch 12/100, Val Acc=0.8334, Val Loss=0.5012, lr=0.0100
[2025-04-27 22:36:24,736][train][INFO] - Epoch 11/100, Val Acc=0.9039, Val Loss=0.3275, lr=0.0100
[2025-04-27 22:36:28,074][train][INFO] - Epoch 12/100, Val Acc=0.7234, Val Loss=0.8568, lr=0.0100
[2025-04-27 22:36:29,784][train][INFO] - Epoch 11/100, Val Acc=0.8976, Val Loss=0.3422, lr=0.0100
[2025-04-27 22:36:31,063][train][INFO] - Epoch 10/100, Val Acc=0.9004, Val Loss=0.3347, lr=0.0100
[2025-04-27 22:36:34,642][train][INFO] - Epoch 10/100, Val Acc=0.9053, Val Loss=0.2904, lr=0.0100
[2025-04-27 22:36:42,572][train][INFO] - Epoch 13/100, Val Acc=0.8480, Val Loss=0.4575, lr=0.0100
[2025-04-27 22:36:43,601][train][INFO] - Epoch 12/100, Val Acc=0.8997, Val Loss=0.3598, lr=0.0100
[2025-04-27 22:36:46,569][train][INFO] - Epoch 13/100, Val Acc=0.7889, Val Loss=0.6148, lr=0.0100
[2025-04-27 22:36:48,506][train][INFO] - Epoch 12/100, Val Acc=0.9050, Val Loss=0.3098, lr=0.0100
[2025-04-27 22:36:49,343][train][INFO] - Epoch 11/100, Val Acc=0.9049, Val Loss=0.3209, lr=0.0100
[2025-04-27 22:36:52,847][train][INFO] - Epoch 11/100, Val Acc=0.9024, Val Loss=0.3013, lr=0.0100
[2025-04-27 22:37:01,438][train][INFO] - Epoch 14/100, Val Acc=0.8374, Val Loss=0.4912, lr=0.0100
[2025-04-27 22:37:02,477][train][INFO] - Epoch 13/100, Val Acc=0.8994, Val Loss=0.3425, lr=0.0100
[2025-04-27 22:37:05,036][train][INFO] - Epoch 14/100, Val Acc=0.7642, Val Loss=0.7038, lr=0.0100
[2025-04-27 22:37:07,288][train][INFO] - Epoch 13/100, Val Acc=0.8975, Val Loss=0.3443, lr=0.0100
[2025-04-27 22:37:07,651][train][INFO] - Epoch 12/100, Val Acc=0.9008, Val Loss=0.3372, lr=0.0100
[2025-04-27 22:37:11,126][train][INFO] - Epoch 12/100, Val Acc=0.8991, Val Loss=0.3239, lr=0.0100
[2025-04-27 22:37:20,156][train][INFO] - Epoch 15/100, Val Acc=0.8248, Val Loss=0.5318, lr=0.0100
[2025-04-27 22:37:21,319][train][INFO] - Epoch 14/100, Val Acc=0.8906, Val Loss=0.3726, lr=0.0100
[2025-04-27 22:37:23,811][train][INFO] - Epoch 15/100, Val Acc=0.8067, Val Loss=0.5571, lr=0.0100
[2025-04-27 22:37:25,914][train][INFO] - Epoch 14/100, Val Acc=0.8950, Val Loss=0.3499, lr=0.0100
[2025-04-27 22:37:26,122][train][INFO] - Epoch 13/100, Val Acc=0.9000, Val Loss=0.3302, lr=0.0100
[2025-04-27 22:37:29,706][train][INFO] - Epoch 13/100, Val Acc=0.9030, Val Loss=0.3126, lr=0.0100
[2025-04-27 22:37:38,904][train][INFO] - Epoch 16/100, Val Acc=0.8465, Val Loss=0.4593, lr=0.0100
[2025-04-27 22:37:40,349][train][INFO] - Epoch 15/100, Val Acc=0.8989, Val Loss=0.3597, lr=0.0100
[2025-04-27 22:37:42,290][train][INFO] - Epoch 16/100, Val Acc=0.7741, Val Loss=0.6982, lr=0.0100
[2025-04-27 22:37:44,556][train][INFO] - Epoch 14/100, Val Acc=0.8955, Val Loss=0.3666, lr=0.0100
[2025-04-27 22:37:44,601][train][INFO] - Epoch 15/100, Val Acc=0.8988, Val Loss=0.3551, lr=0.0100
[2025-04-27 22:37:48,440][train][INFO] - Epoch 14/100, Val Acc=0.9065, Val Loss=0.2936, lr=0.0100
[2025-04-27 22:37:57,446][train][INFO] - Epoch 17/100, Val Acc=0.8471, Val Loss=0.4576, lr=0.0100
[2025-04-27 22:37:58,995][train][INFO] - Epoch 16/100, Val Acc=0.9051, Val Loss=0.3333, lr=0.0100
[2025-04-27 22:38:00,743][train][INFO] - Epoch 17/100, Val Acc=0.7814, Val Loss=0.6406, lr=0.0100
[2025-04-27 22:38:03,116][train][INFO] - Epoch 15/100, Val Acc=0.9042, Val Loss=0.3316, lr=0.0100
[2025-04-27 22:38:03,277][train][INFO] - Epoch 16/100, Val Acc=0.9091, Val Loss=0.3086, lr=0.0100
[2025-04-27 22:38:07,045][train][INFO] - Epoch 15/100, Val Acc=0.8974, Val Loss=0.3563, lr=0.0100
[2025-04-27 22:38:16,076][train][INFO] - Epoch 18/100, Val Acc=0.8327, Val Loss=0.5299, lr=0.0100
[2025-04-27 22:38:17,879][train][INFO] - Epoch 17/100, Val Acc=0.9035, Val Loss=0.3511, lr=0.0100
[2025-04-27 22:38:19,136][train][INFO] - Epoch 18/100, Val Acc=0.7642, Val Loss=0.6778, lr=0.0100
[2025-04-27 22:38:21,648][train][INFO] - Epoch 16/100, Val Acc=0.9093, Val Loss=0.3161, lr=0.0100
[2025-04-27 22:38:21,960][train][INFO] - Epoch 17/100, Val Acc=0.9046, Val Loss=0.3253, lr=0.0100
[2025-04-27 22:38:25,752][train][INFO] - Epoch 16/100, Val Acc=0.9082, Val Loss=0.2901, lr=0.0100
[2025-04-27 22:38:34,914][train][INFO] - Epoch 19/100, Val Acc=0.8353, Val Loss=0.4971, lr=0.0100
[2025-04-27 22:38:36,829][train][INFO] - Epoch 18/100, Val Acc=0.9094, Val Loss=0.3240, lr=0.0100
[2025-04-27 22:38:37,402][train][INFO] - Epoch 19/100, Val Acc=0.8000, Val Loss=0.5927, lr=0.0100
[2025-04-27 22:38:40,066][train][INFO] - Epoch 17/100, Val Acc=0.9097, Val Loss=0.3134, lr=0.0100
[2025-04-27 22:38:40,645][train][INFO] - Epoch 18/100, Val Acc=0.9130, Val Loss=0.2878, lr=0.0100
[2025-04-27 22:38:44,218][train][INFO] - Epoch 17/100, Val Acc=0.9025, Val Loss=0.3312, lr=0.0100
[2025-04-27 22:38:53,563][train][INFO] - Epoch 20/100, Val Acc=0.8608, Val Loss=0.4293, lr=0.0100
[2025-04-27 22:38:55,769][train][INFO] - Epoch 19/100, Val Acc=0.9107, Val Loss=0.3100, lr=0.0100
[2025-04-27 22:38:56,209][train][INFO] - Epoch 20/100, Val Acc=0.8046, Val Loss=0.5868, lr=0.0100
[2025-04-27 22:38:58,571][train][INFO] - Epoch 18/100, Val Acc=0.9056, Val Loss=0.3285, lr=0.0100
[2025-04-27 22:38:59,234][train][INFO] - Epoch 19/100, Val Acc=0.9055, Val Loss=0.3213, lr=0.0100
[2025-04-27 22:39:02,754][train][INFO] - Epoch 18/100, Val Acc=0.9029, Val Loss=0.3195, lr=0.0100
[2025-04-27 22:39:12,309][train][INFO] - Epoch 21/100, Val Acc=0.8459, Val Loss=0.4658, lr=0.0100
[2025-04-27 22:39:14,581][train][INFO] - Epoch 20/100, Val Acc=0.9015, Val Loss=0.3615, lr=0.0100
[2025-04-27 22:39:14,791][train][INFO] - Epoch 21/100, Val Acc=0.7883, Val Loss=0.6490, lr=0.0100
[2025-04-27 22:39:16,935][train][INFO] - Epoch 19/100, Val Acc=0.8977, Val Loss=0.3543, lr=0.0100
[2025-04-27 22:39:17,754][train][INFO] - Epoch 20/100, Val Acc=0.9099, Val Loss=0.3027, lr=0.0100
[2025-04-27 22:39:21,281][train][INFO] - Epoch 19/100, Val Acc=0.9109, Val Loss=0.2946, lr=0.0100
[2025-04-27 22:39:30,969][train][INFO] - Epoch 22/100, Val Acc=0.8491, Val Loss=0.4451, lr=0.0100
[2025-04-27 22:39:33,433][train][INFO] - Epoch 22/100, Val Acc=0.8095, Val Loss=0.5591, lr=0.0100
[2025-04-27 22:39:33,692][train][INFO] - Epoch 21/100, Val Acc=0.9125, Val Loss=0.3076, lr=0.0100
[2025-04-27 22:39:35,478][train][INFO] - Epoch 20/100, Val Acc=0.9142, Val Loss=0.2992, lr=0.0100
[2025-04-27 22:39:36,497][train][INFO] - Epoch 21/100, Val Acc=0.8830, Val Loss=0.3967, lr=0.0100
[2025-04-27 22:39:39,573][train][INFO] - Epoch 20/100, Val Acc=0.9099, Val Loss=0.2997, lr=0.0100
[2025-04-27 22:39:49,695][train][INFO] - Epoch 23/100, Val Acc=0.8693, Val Loss=0.3864, lr=0.0100
[2025-04-27 22:39:51,796][train][INFO] - Epoch 23/100, Val Acc=0.8059, Val Loss=0.5667, lr=0.0100
[2025-04-27 22:39:52,546][train][INFO] - Epoch 22/100, Val Acc=0.9099, Val Loss=0.3222, lr=0.0100
[2025-04-27 22:39:53,904][train][INFO] - Epoch 21/100, Val Acc=0.9082, Val Loss=0.3161, lr=0.0100
[2025-04-27 22:39:55,251][train][INFO] - Epoch 22/100, Val Acc=0.9067, Val Loss=0.3174, lr=0.0100
[2025-04-27 22:39:57,942][train][INFO] - Epoch 21/100, Val Acc=0.9065, Val Loss=0.3099, lr=0.0100
[2025-04-27 22:40:08,034][train][INFO] - Epoch 24/100, Val Acc=0.8627, Val Loss=0.4255, lr=0.0100
[2025-04-27 22:40:10,596][train][INFO] - Epoch 24/100, Val Acc=0.8212, Val Loss=0.5342, lr=0.0100
[2025-04-27 22:40:11,392][train][INFO] - Epoch 23/100, Val Acc=0.8977, Val Loss=0.3420, lr=0.0100
[2025-04-27 22:40:12,386][train][INFO] - Epoch 22/100, Val Acc=0.9122, Val Loss=0.2935, lr=0.0100
[2025-04-27 22:40:13,887][train][INFO] - Epoch 23/100, Val Acc=0.9048, Val Loss=0.3273, lr=0.0100
[2025-04-27 22:40:16,585][train][INFO] - Epoch 22/100, Val Acc=0.9034, Val Loss=0.3190, lr=0.0100
[2025-04-27 22:40:26,416][train][INFO] - Epoch 25/100, Val Acc=0.8687, Val Loss=0.4043, lr=0.0100
[2025-04-27 22:40:29,166][train][INFO] - Epoch 25/100, Val Acc=0.8200, Val Loss=0.5346, lr=0.0100
[2025-04-27 22:40:30,299][train][INFO] - Epoch 24/100, Val Acc=0.9105, Val Loss=0.3188, lr=0.0100
[2025-04-27 22:40:30,945][train][INFO] - Epoch 23/100, Val Acc=0.9100, Val Loss=0.3039, lr=0.0100
[2025-04-27 22:40:32,828][train][INFO] - Epoch 24/100, Val Acc=0.9073, Val Loss=0.3170, lr=0.0100
[2025-04-27 22:40:35,051][train][INFO] - Epoch 23/100, Val Acc=0.9101, Val Loss=0.2814, lr=0.0100
[2025-04-27 22:40:45,260][train][INFO] - Epoch 26/100, Val Acc=0.8678, Val Loss=0.3973, lr=0.0100
[2025-04-27 22:40:47,755][train][INFO] - Epoch 26/100, Val Acc=0.8417, Val Loss=0.4785, lr=0.0100
[2025-04-27 22:40:48,991][train][INFO] - Epoch 25/100, Val Acc=0.9001, Val Loss=0.3628, lr=0.0100
[2025-04-27 22:40:49,450][train][INFO] - Epoch 24/100, Val Acc=0.9108, Val Loss=0.3064, lr=0.0100
[2025-04-27 22:40:51,385][train][INFO] - Epoch 25/100, Val Acc=0.8928, Val Loss=0.3711, lr=0.0100
[2025-04-27 22:40:53,409][train][INFO] - Epoch 24/100, Val Acc=0.9080, Val Loss=0.3047, lr=0.0100
[2025-04-27 22:41:03,897][train][INFO] - Epoch 27/100, Val Acc=0.8592, Val Loss=0.4310, lr=0.0100
[2025-04-27 22:41:06,383][train][INFO] - Epoch 27/100, Val Acc=0.8231, Val Loss=0.5238, lr=0.0100
[2025-04-27 22:41:07,738][train][INFO] - Epoch 26/100, Val Acc=0.9061, Val Loss=0.3407, lr=0.0100
[2025-04-27 22:41:07,812][train][INFO] - Epoch 25/100, Val Acc=0.8977, Val Loss=0.3493, lr=0.0100
[2025-04-27 22:41:09,925][train][INFO] - Epoch 26/100, Val Acc=0.9043, Val Loss=0.3184, lr=0.0100
[2025-04-27 22:41:11,940][train][INFO] - Epoch 25/100, Val Acc=0.8928, Val Loss=0.3651, lr=0.0100
[2025-04-27 22:41:22,681][train][INFO] - Epoch 28/100, Val Acc=0.8530, Val Loss=0.4691, lr=0.0100
[2025-04-27 22:41:25,107][train][INFO] - Epoch 28/100, Val Acc=0.8247, Val Loss=0.5371, lr=0.0100
[2025-04-27 22:41:26,082][train][INFO] - Epoch 26/100, Val Acc=0.9128, Val Loss=0.2900, lr=0.0100
[2025-04-27 22:41:26,325][train][INFO] - Epoch 27/100, Val Acc=0.9131, Val Loss=0.3287, lr=0.0100
[2025-04-27 22:41:28,381][train][INFO] - Epoch 27/100, Val Acc=0.9099, Val Loss=0.3136, lr=0.0100
[2025-04-27 22:41:30,724][train][INFO] - Epoch 26/100, Val Acc=0.9038, Val Loss=0.3202, lr=0.0100
[2025-04-27 22:41:41,267][train][INFO] - Epoch 29/100, Val Acc=0.8604, Val Loss=0.4439, lr=0.0100
[2025-04-27 22:41:43,562][train][INFO] - Epoch 29/100, Val Acc=0.8314, Val Loss=0.5301, lr=0.0100
[2025-04-27 22:41:44,519][train][INFO] - Epoch 27/100, Val Acc=0.9114, Val Loss=0.3069, lr=0.0100
[2025-04-27 22:41:45,097][train][INFO] - Epoch 28/100, Val Acc=0.9061, Val Loss=0.3314, lr=0.0100
[2025-04-27 22:41:46,923][train][INFO] - Epoch 28/100, Val Acc=0.9153, Val Loss=0.3047, lr=0.0100
[2025-04-27 22:41:49,111][train][INFO] - Epoch 27/100, Val Acc=0.9019, Val Loss=0.3486, lr=0.0100
[2025-04-27 22:41:59,860][train][INFO] - Epoch 30/100, Val Acc=0.8563, Val Loss=0.4441, lr=0.0100
[2025-04-27 22:42:02,120][train][INFO] - Epoch 30/100, Val Acc=0.8109, Val Loss=0.5789, lr=0.0100
[2025-04-27 22:42:02,983][train][INFO] - Epoch 28/100, Val Acc=0.9095, Val Loss=0.3247, lr=0.0100
[2025-04-27 22:42:03,894][train][INFO] - Epoch 29/100, Val Acc=0.9114, Val Loss=0.3237, lr=0.0100
[2025-04-27 22:42:05,452][train][INFO] - Epoch 29/100, Val Acc=0.9056, Val Loss=0.3330, lr=0.0100
[2025-04-27 22:42:07,604][train][INFO] - Epoch 28/100, Val Acc=0.9076, Val Loss=0.3062, lr=0.0100
[2025-04-27 22:42:18,271][train][INFO] - Epoch 31/100, Val Acc=0.8671, Val Loss=0.4076, lr=0.0100
[2025-04-27 22:42:20,775][train][INFO] - Epoch 31/100, Val Acc=0.8326, Val Loss=0.4873, lr=0.0100
[2025-04-27 22:42:21,644][train][INFO] - Epoch 29/100, Val Acc=0.9060, Val Loss=0.3232, lr=0.0100
[2025-04-27 22:42:22,413][train][INFO] - Epoch 30/100, Val Acc=0.8957, Val Loss=0.3772, lr=0.0100
[2025-04-27 22:42:23,713][train][INFO] - Epoch 30/100, Val Acc=0.8981, Val Loss=0.3657, lr=0.0100
[2025-04-27 22:42:26,094][train][INFO] - Epoch 29/100, Val Acc=0.9088, Val Loss=0.3156, lr=0.0100
[2025-04-27 22:42:37,038][train][INFO] - Epoch 32/100, Val Acc=0.8606, Val Loss=0.4233, lr=0.0100
[2025-04-27 22:42:39,160][train][INFO] - Epoch 32/100, Val Acc=0.8375, Val Loss=0.5031, lr=0.0100
[2025-04-27 22:42:40,049][train][INFO] - Epoch 30/100, Val Acc=0.8906, Val Loss=0.3864, lr=0.0100
[2025-04-27 22:42:40,822][train][INFO] - Epoch 31/100, Val Acc=0.9082, Val Loss=0.3262, lr=0.0100
[2025-04-27 22:42:42,180][train][INFO] - Epoch 31/100, Val Acc=0.9113, Val Loss=0.3152, lr=0.0100
[2025-04-27 22:42:44,497][train][INFO] - Epoch 30/100, Val Acc=0.8829, Val Loss=0.4438, lr=0.0100
[2025-04-27 22:42:55,559][train][INFO] - Epoch 33/100, Val Acc=0.8531, Val Loss=0.4493, lr=0.0100
[2025-04-27 22:42:57,922][train][INFO] - Epoch 33/100, Val Acc=0.8190, Val Loss=0.5353, lr=0.0100
[2025-04-27 22:42:58,605][train][INFO] - Epoch 31/100, Val Acc=0.9126, Val Loss=0.3022, lr=0.0100
[2025-04-27 22:42:59,555][train][INFO] - Epoch 32/100, Val Acc=0.9046, Val Loss=0.3483, lr=0.0100
[2025-04-27 22:43:00,795][train][INFO] - Epoch 32/100, Val Acc=0.9094, Val Loss=0.3256, lr=0.0100
[2025-04-27 22:43:02,870][train][INFO] - Epoch 31/100, Val Acc=0.9087, Val Loss=0.3181, lr=0.0100
[2025-04-27 22:43:14,413][train][INFO] - Epoch 34/100, Val Acc=0.8738, Val Loss=0.4021, lr=0.0100
[2025-04-27 22:43:16,805][train][INFO] - Epoch 34/100, Val Acc=0.8098, Val Loss=0.5908, lr=0.0100
[2025-04-27 22:43:17,030][train][INFO] - Epoch 32/100, Val Acc=0.9052, Val Loss=0.3236, lr=0.0100
[2025-04-27 22:43:18,290][train][INFO] - Epoch 33/100, Val Acc=0.9150, Val Loss=0.3040, lr=0.0100
[2025-04-27 22:43:19,525][train][INFO] - Epoch 33/100, Val Acc=0.9112, Val Loss=0.3132, lr=0.0100
[2025-04-27 22:43:21,244][train][INFO] - Epoch 32/100, Val Acc=0.9127, Val Loss=0.2956, lr=0.0100
[2025-04-27 22:43:33,103][train][INFO] - Epoch 35/100, Val Acc=0.8723, Val Loss=0.3953, lr=0.0100
[2025-04-27 22:43:35,369][train][INFO] - Epoch 35/100, Val Acc=0.8340, Val Loss=0.4859, lr=0.0100
[2025-04-27 22:43:35,616][train][INFO] - Epoch 33/100, Val Acc=0.9050, Val Loss=0.3512, lr=0.0100
[2025-04-27 22:43:37,123][train][INFO] - Epoch 34/100, Val Acc=0.9170, Val Loss=0.2996, lr=0.0100
[2025-04-27 22:43:38,115][train][INFO] - Epoch 34/100, Val Acc=0.9019, Val Loss=0.3604, lr=0.0100
[2025-04-27 22:43:39,753][train][INFO] - Epoch 33/100, Val Acc=0.8954, Val Loss=0.3557, lr=0.0100
[2025-04-27 22:43:51,660][train][INFO] - Epoch 36/100, Val Acc=0.8683, Val Loss=0.3980, lr=0.0100
[2025-04-27 22:43:54,059][train][INFO] - Epoch 34/100, Val Acc=0.9079, Val Loss=0.3286, lr=0.0100
[2025-04-27 22:43:54,126][train][INFO] - Epoch 36/100, Val Acc=0.8363, Val Loss=0.4961, lr=0.0100
[2025-04-27 22:43:55,775][train][INFO] - Epoch 35/100, Val Acc=0.9071, Val Loss=0.3317, lr=0.0100
[2025-04-27 22:43:56,934][train][INFO] - Epoch 35/100, Val Acc=0.9090, Val Loss=0.3233, lr=0.0100
[2025-04-27 22:43:58,203][train][INFO] - Epoch 34/100, Val Acc=0.9051, Val Loss=0.3197, lr=0.0100
[2025-04-27 22:44:10,214][train][INFO] - Epoch 37/100, Val Acc=0.8608, Val Loss=0.4182, lr=0.0100
[2025-04-27 22:44:12,603][train][INFO] - Epoch 35/100, Val Acc=0.9133, Val Loss=0.3171, lr=0.0100
[2025-04-27 22:44:12,887][train][INFO] - Epoch 37/100, Val Acc=0.8338, Val Loss=0.4995, lr=0.0100
[2025-04-27 22:44:14,623][train][INFO] - Epoch 36/100, Val Acc=0.9067, Val Loss=0.3414, lr=0.0100
[2025-04-27 22:44:15,715][train][INFO] - Epoch 36/100, Val Acc=0.9089, Val Loss=0.3192, lr=0.0100
[2025-04-27 22:44:16,760][train][INFO] - Epoch 35/100, Val Acc=0.9084, Val Loss=0.3128, lr=0.0100
[2025-04-27 22:44:29,097][train][INFO] - Epoch 38/100, Val Acc=0.8720, Val Loss=0.3771, lr=0.0100
[2025-04-27 22:44:31,166][train][INFO] - Epoch 36/100, Val Acc=0.9079, Val Loss=0.3126, lr=0.0100
[2025-04-27 22:44:31,710][train][INFO] - Epoch 38/100, Val Acc=0.8424, Val Loss=0.4704, lr=0.0100
[2025-04-27 22:44:33,559][train][INFO] - Epoch 37/100, Val Acc=0.9163, Val Loss=0.2867, lr=0.0100
[2025-04-27 22:44:34,340][train][INFO] - Epoch 37/100, Val Acc=0.9137, Val Loss=0.2997, lr=0.0100
[2025-04-27 22:44:35,482][train][INFO] - Epoch 36/100, Val Acc=0.9053, Val Loss=0.3360, lr=0.0100
[2025-04-27 22:44:47,722][train][INFO] - Epoch 39/100, Val Acc=0.8584, Val Loss=0.4435, lr=0.0100
[2025-04-27 22:44:49,742][train][INFO] - Epoch 37/100, Val Acc=0.9105, Val Loss=0.3192, lr=0.0100
[2025-04-27 22:44:50,452][train][INFO] - Epoch 39/100, Val Acc=0.8458, Val Loss=0.4689, lr=0.0100
[2025-04-27 22:44:52,571][train][INFO] - Epoch 38/100, Val Acc=0.9076, Val Loss=0.3291, lr=0.0100
[2025-04-27 22:44:53,302][train][INFO] - Epoch 38/100, Val Acc=0.9102, Val Loss=0.3318, lr=0.0100
[2025-04-27 22:44:53,901][train][INFO] - Epoch 37/100, Val Acc=0.9100, Val Loss=0.2986, lr=0.0100
[2025-04-27 22:45:06,267][train][INFO] - Epoch 40/100, Val Acc=0.8735, Val Loss=0.3847, lr=0.0100
[2025-04-27 22:45:08,145][train][INFO] - Epoch 38/100, Val Acc=0.9127, Val Loss=0.2997, lr=0.0100
[2025-04-27 22:45:09,101][train][INFO] - Epoch 40/100, Val Acc=0.8416, Val Loss=0.4870, lr=0.0100
[2025-04-27 22:45:11,695][train][INFO] - Epoch 39/100, Val Acc=0.9028, Val Loss=0.3534, lr=0.0100
[2025-04-27 22:45:12,038][train][INFO] - Epoch 39/100, Val Acc=0.9105, Val Loss=0.3205, lr=0.0100
[2025-04-27 22:45:12,543][train][INFO] - Epoch 38/100, Val Acc=0.9045, Val Loss=0.3339, lr=0.0100
[2025-04-27 22:45:24,937][train][INFO] - Epoch 41/100, Val Acc=0.8722, Val Loss=0.4022, lr=0.0100
[2025-04-27 22:45:26,480][train][INFO] - Epoch 39/100, Val Acc=0.9051, Val Loss=0.3339, lr=0.0100
[2025-04-27 22:45:27,738][train][INFO] - Epoch 41/100, Val Acc=0.8514, Val Loss=0.4468, lr=0.0100
[2025-04-27 22:45:30,864][train][INFO] - Epoch 40/100, Val Acc=0.9073, Val Loss=0.3261, lr=0.0100
[2025-04-27 22:45:31,040][train][INFO] - Epoch 40/100, Val Acc=0.9046, Val Loss=0.3350, lr=0.0100
[2025-04-27 22:45:31,132][train][INFO] - Epoch 39/100, Val Acc=0.9095, Val Loss=0.3119, lr=0.0100
[2025-04-27 22:45:43,340][train][INFO] - Epoch 42/100, Val Acc=0.8687, Val Loss=0.4091, lr=0.0100
[2025-04-27 22:45:44,769][train][INFO] - Epoch 40/100, Val Acc=0.9102, Val Loss=0.3099, lr=0.0100
[2025-04-27 22:45:46,539][train][INFO] - Epoch 42/100, Val Acc=0.8565, Val Loss=0.4219, lr=0.0100
[2025-04-27 22:45:49,593][train][INFO] - Epoch 40/100, Val Acc=0.9081, Val Loss=0.3047, lr=0.0100
[2025-04-27 22:45:50,234][train][INFO] - Epoch 41/100, Val Acc=0.9074, Val Loss=0.3274, lr=0.0100
[2025-04-27 22:45:50,288][train][INFO] - Epoch 41/100, Val Acc=0.9093, Val Loss=0.3447, lr=0.0100
[2025-04-27 22:46:01,953][train][INFO] - Epoch 43/100, Val Acc=0.8676, Val Loss=0.4226, lr=0.0100
[2025-04-27 22:46:03,101][train][INFO] - Epoch 41/100, Val Acc=0.9207, Val Loss=0.2892, lr=0.0100
[2025-04-27 22:46:04,987][train][INFO] - Epoch 43/100, Val Acc=0.8358, Val Loss=0.5008, lr=0.0100
[2025-04-27 22:46:08,028][train][INFO] - Epoch 41/100, Val Acc=0.9114, Val Loss=0.3101, lr=0.0100
[2025-04-27 22:46:09,285][train][INFO] - Epoch 42/100, Val Acc=0.9093, Val Loss=0.3179, lr=0.0100
[2025-04-27 22:46:09,536][train][INFO] - Epoch 42/100, Val Acc=0.9126, Val Loss=0.3139, lr=0.0100
[2025-04-27 22:46:20,897][train][INFO] - Epoch 44/100, Val Acc=0.8619, Val Loss=0.4258, lr=0.0100
[2025-04-27 22:46:21,612][train][INFO] - Epoch 42/100, Val Acc=0.9123, Val Loss=0.3087, lr=0.0100
[2025-04-27 22:46:23,823][train][INFO] - Epoch 44/100, Val Acc=0.8483, Val Loss=0.4656, lr=0.0100
[2025-04-27 22:46:26,431][train][INFO] - Epoch 42/100, Val Acc=0.9081, Val Loss=0.3119, lr=0.0100
[2025-04-27 22:46:28,049][train][INFO] - Epoch 43/100, Val Acc=0.9096, Val Loss=0.3189, lr=0.0100
[2025-04-27 22:46:28,792][train][INFO] - Epoch 43/100, Val Acc=0.9207, Val Loss=0.2813, lr=0.0100
[2025-04-27 22:46:39,411][train][INFO] - Epoch 45/100, Val Acc=0.8768, Val Loss=0.3820, lr=0.0100
[2025-04-27 22:46:39,767][train][INFO] - Epoch 43/100, Val Acc=0.9053, Val Loss=0.3276, lr=0.0100
[2025-04-27 22:46:42,355][train][INFO] - Epoch 45/100, Val Acc=0.8219, Val Loss=0.5909, lr=0.0100
[2025-04-27 22:46:45,016][train][INFO] - Epoch 43/100, Val Acc=0.9021, Val Loss=0.3483, lr=0.0100
[2025-04-27 22:46:46,876][train][INFO] - Epoch 44/100, Val Acc=0.9072, Val Loss=0.3334, lr=0.0100
[2025-04-27 22:46:47,657][train][INFO] - Epoch 44/100, Val Acc=0.9180, Val Loss=0.3021, lr=0.0100
[2025-04-27 22:46:58,075][train][INFO] - Epoch 46/100, Val Acc=0.8651, Val Loss=0.4111, lr=0.0100
[2025-04-27 22:46:58,118][train][INFO] - Epoch 44/100, Val Acc=0.9112, Val Loss=0.3097, lr=0.0100
[2025-04-27 22:47:00,914][train][INFO] - Epoch 46/100, Val Acc=0.8465, Val Loss=0.4641, lr=0.0100
[2025-04-27 22:47:03,507][train][INFO] - Epoch 44/100, Val Acc=0.9175, Val Loss=0.2810, lr=0.0100
[2025-04-27 22:47:05,692][train][INFO] - Epoch 45/100, Val Acc=0.9090, Val Loss=0.3111, lr=0.0100
[2025-04-27 22:47:06,576][train][INFO] - Epoch 45/100, Val Acc=0.9103, Val Loss=0.3232, lr=0.0100
[2025-04-27 22:47:16,637][train][INFO] - Epoch 47/100, Val Acc=0.8710, Val Loss=0.4072, lr=0.0100
[2025-04-27 22:47:16,653][train][INFO] - Epoch 45/100, Val Acc=0.9023, Val Loss=0.3449, lr=0.0100
[2025-04-27 22:47:19,225][train][INFO] - Epoch 47/100, Val Acc=0.8346, Val Loss=0.5246, lr=0.0100
[2025-04-27 22:47:22,063][train][INFO] - Epoch 45/100, Val Acc=0.9132, Val Loss=0.3013, lr=0.0100
[2025-04-27 22:47:24,648][train][INFO] - Epoch 46/100, Val Acc=0.9060, Val Loss=0.3380, lr=0.0100
[2025-04-27 22:47:25,385][train][INFO] - Epoch 46/100, Val Acc=0.9054, Val Loss=0.3520, lr=0.0100
[2025-04-27 22:47:35,048][train][INFO] - Epoch 46/100, Val Acc=0.9051, Val Loss=0.3502, lr=0.0100
[2025-04-27 22:47:35,267][train][INFO] - Epoch 48/100, Val Acc=0.8639, Val Loss=0.4165, lr=0.0100
[2025-04-27 22:47:37,951][train][INFO] - Epoch 48/100, Val Acc=0.8487, Val Loss=0.4526, lr=0.0100
[2025-04-27 22:47:40,689][train][INFO] - Epoch 46/100, Val Acc=0.8927, Val Loss=0.3902, lr=0.0100
[2025-04-27 22:47:43,447][train][INFO] - Epoch 47/100, Val Acc=0.9109, Val Loss=0.2999, lr=0.0100
[2025-04-27 22:47:44,341][train][INFO] - Epoch 47/100, Val Acc=0.9078, Val Loss=0.3339, lr=0.0100
[2025-04-27 22:47:53,643][train][INFO] - Epoch 49/100, Val Acc=0.8712, Val Loss=0.4037, lr=0.0100
[2025-04-27 22:47:53,658][train][INFO] - Epoch 47/100, Val Acc=0.9052, Val Loss=0.3392, lr=0.0100
[2025-04-27 22:47:56,688][train][INFO] - Epoch 49/100, Val Acc=0.8412, Val Loss=0.4917, lr=0.0100
[2025-04-27 22:47:59,154][train][INFO] - Epoch 47/100, Val Acc=0.9088, Val Loss=0.3004, lr=0.0100
[2025-04-27 22:48:02,247][train][INFO] - Epoch 48/100, Val Acc=0.9122, Val Loss=0.3168, lr=0.0100
[2025-04-27 22:48:03,259][train][INFO] - Epoch 48/100, Val Acc=0.9118, Val Loss=0.3236, lr=0.0100
[2025-04-27 22:48:12,152][train][INFO] - Epoch 48/100, Val Acc=0.9089, Val Loss=0.3290, lr=0.0100
[2025-04-27 22:48:12,386][train][INFO] - Epoch 50/100, Val Acc=0.8608, Val Loss=0.4467, lr=0.0100
[2025-04-27 22:48:14,943][train][INFO] - Epoch 50/100, Val Acc=0.8170, Val Loss=0.5945, lr=0.0100
[2025-04-27 22:48:17,559][train][INFO] - Epoch 48/100, Val Acc=0.9116, Val Loss=0.2996, lr=0.0100
[2025-04-27 22:48:20,861][train][INFO] - Epoch 49/100, Val Acc=0.9079, Val Loss=0.3398, lr=0.0100
[2025-04-27 22:48:22,050][train][INFO] - Epoch 49/100, Val Acc=0.9128, Val Loss=0.3311, lr=0.0100
[2025-04-27 22:48:30,586][train][INFO] - Epoch 49/100, Val Acc=0.9124, Val Loss=0.3124, lr=0.0100
[2025-04-27 22:48:31,214][train][INFO] - Epoch 51/100, Val Acc=0.8554, Val Loss=0.4590, lr=0.0100
[2025-04-27 22:48:33,243][train][INFO] - Epoch 51/100, Val Acc=0.8183, Val Loss=0.5730, lr=0.0100
[2025-04-27 22:48:36,106][train][INFO] - Epoch 49/100, Val Acc=0.9088, Val Loss=0.3077, lr=0.0100
[2025-04-27 22:48:39,603][train][INFO] - Epoch 50/100, Val Acc=0.9016, Val Loss=0.3504, lr=0.0100
[2025-04-27 22:48:40,850][train][INFO] - Epoch 50/100, Val Acc=0.9129, Val Loss=0.3103, lr=0.0100
[2025-04-27 22:48:48,947][train][INFO] - Epoch 50/100, Val Acc=0.9121, Val Loss=0.3088, lr=0.0100
[2025-04-27 22:48:49,903][train][INFO] - Epoch 52/100, Val Acc=0.8800, Val Loss=0.3782, lr=0.0100
[2025-04-27 22:48:51,678][train][INFO] - Epoch 52/100, Val Acc=0.8472, Val Loss=0.4687, lr=0.0100
[2025-04-27 22:48:54,410][train][INFO] - Epoch 50/100, Val Acc=0.9117, Val Loss=0.2960, lr=0.0100
[2025-04-27 22:48:58,266][train][INFO] - Epoch 51/100, Val Acc=0.9085, Val Loss=0.3161, lr=0.0100
[2025-04-27 22:48:59,620][train][INFO] - Epoch 51/100, Val Acc=0.9174, Val Loss=0.2871, lr=0.0100
[2025-04-27 22:49:07,404][train][INFO] - Epoch 51/100, Val Acc=0.9041, Val Loss=0.3451, lr=0.0100
[2025-04-27 22:49:08,512][train][INFO] - Epoch 53/100, Val Acc=0.8620, Val Loss=0.4055, lr=0.0100
[2025-04-27 22:49:10,087][train][INFO] - Epoch 53/100, Val Acc=0.8501, Val Loss=0.4393, lr=0.0100
[2025-04-27 22:49:12,982][train][INFO] - Epoch 51/100, Val Acc=0.9101, Val Loss=0.3105, lr=0.0100
[2025-04-27 22:49:16,819][train][INFO] - Epoch 52/100, Val Acc=0.9142, Val Loss=0.2995, lr=0.0100
[2025-04-27 22:49:18,239][train][INFO] - Epoch 52/100, Val Acc=0.9123, Val Loss=0.3317, lr=0.0100
[2025-04-27 22:49:25,800][train][INFO] - Epoch 52/100, Val Acc=0.9158, Val Loss=0.3075, lr=0.0100
[2025-04-27 22:49:27,254][train][INFO] - Epoch 54/100, Val Acc=0.8678, Val Loss=0.4212, lr=0.0100
[2025-04-27 22:49:28,944][train][INFO] - Epoch 54/100, Val Acc=0.8394, Val Loss=0.4862, lr=0.0100
[2025-04-27 22:49:31,497][train][INFO] - Epoch 52/100, Val Acc=0.8989, Val Loss=0.3556, lr=0.0100
[2025-04-27 22:49:35,460][train][INFO] - Epoch 53/100, Val Acc=0.9105, Val Loss=0.3212, lr=0.0100
[2025-04-27 22:49:37,354][train][INFO] - Epoch 53/100, Val Acc=0.9155, Val Loss=0.3052, lr=0.0100
[2025-04-27 22:49:44,009][train][INFO] - Epoch 53/100, Val Acc=0.9112, Val Loss=0.3196, lr=0.0100
[2025-04-27 22:49:45,629][train][INFO] - Epoch 55/100, Val Acc=0.8618, Val Loss=0.4444, lr=0.0100
[2025-04-27 22:49:47,336][train][INFO] - Epoch 55/100, Val Acc=0.8440, Val Loss=0.4972, lr=0.0100
[2025-04-27 22:49:49,824][train][INFO] - Epoch 53/100, Val Acc=0.9007, Val Loss=0.3429, lr=0.0100
[2025-04-27 22:49:54,074][train][INFO] - Epoch 54/100, Val Acc=0.8973, Val Loss=0.3785, lr=0.0100
[2025-04-27 22:49:56,090][train][INFO] - Epoch 54/100, Val Acc=0.9120, Val Loss=0.3218, lr=0.0100
[2025-04-27 22:50:02,474][train][INFO] - Epoch 54/100, Val Acc=0.9011, Val Loss=0.3534, lr=0.0100
[2025-04-27 22:50:04,280][train][INFO] - Epoch 56/100, Val Acc=0.8669, Val Loss=0.4182, lr=0.0100
[2025-04-27 22:50:05,787][train][INFO] - Epoch 56/100, Val Acc=0.8539, Val Loss=0.4343, lr=0.0100
[2025-04-27 22:50:08,466][train][INFO] - Epoch 54/100, Val Acc=0.9105, Val Loss=0.3040, lr=0.0100
[2025-04-27 22:50:12,893][train][INFO] - Epoch 55/100, Val Acc=0.9113, Val Loss=0.3145, lr=0.0100
[2025-04-27 22:50:14,910][train][INFO] - Epoch 55/100, Val Acc=0.9135, Val Loss=0.3211, lr=0.0100
[2025-04-27 22:50:21,185][train][INFO] - Epoch 55/100, Val Acc=0.9084, Val Loss=0.3199, lr=0.0100
[2025-04-27 22:50:22,915][train][INFO] - Epoch 57/100, Val Acc=0.8911, Val Loss=0.3487, lr=0.0100
[2025-04-27 22:50:24,293][train][INFO] - Epoch 57/100, Val Acc=0.8575, Val Loss=0.4322, lr=0.0100
[2025-04-27 22:50:26,809][train][INFO] - Epoch 55/100, Val Acc=0.9074, Val Loss=0.3271, lr=0.0100
[2025-04-27 22:50:31,443][train][INFO] - Epoch 56/100, Val Acc=0.9168, Val Loss=0.2902, lr=0.0100
[2025-04-27 22:50:33,700][train][INFO] - Epoch 56/100, Val Acc=0.9099, Val Loss=0.3343, lr=0.0100
[2025-04-27 22:50:39,582][train][INFO] - Epoch 56/100, Val Acc=0.9068, Val Loss=0.3295, lr=0.0100
[2025-04-27 22:50:41,625][train][INFO] - Epoch 58/100, Val Acc=0.8773, Val Loss=0.3824, lr=0.0100
[2025-04-27 22:50:42,993][train][INFO] - Epoch 58/100, Val Acc=0.8655, Val Loss=0.4073, lr=0.0100
[2025-04-27 22:50:45,217][train][INFO] - Epoch 56/100, Val Acc=0.9112, Val Loss=0.3166, lr=0.0100
[2025-04-27 22:50:49,918][train][INFO] - Epoch 57/100, Val Acc=0.9041, Val Loss=0.3385, lr=0.0100
[2025-04-27 22:50:52,323][train][INFO] - Epoch 57/100, Val Acc=0.9038, Val Loss=0.3355, lr=0.0100
[2025-04-27 22:50:57,865][train][INFO] - Epoch 57/100, Val Acc=0.9066, Val Loss=0.3326, lr=0.0100
[2025-04-27 22:51:00,200][train][INFO] - Epoch 59/100, Val Acc=0.8683, Val Loss=0.4174, lr=0.0100
[2025-04-27 22:51:01,553][train][INFO] - Epoch 59/100, Val Acc=0.8457, Val Loss=0.4761, lr=0.0100
[2025-04-27 22:51:03,751][train][INFO] - Epoch 57/100, Val Acc=0.9165, Val Loss=0.2929, lr=0.0100
[2025-04-27 22:51:08,196][train][INFO] - Epoch 58/100, Val Acc=0.9105, Val Loss=0.3085, lr=0.0100
[2025-04-27 22:51:11,183][train][INFO] - Epoch 58/100, Val Acc=0.9061, Val Loss=0.3392, lr=0.0100
[2025-04-27 22:51:16,106][train][INFO] - Epoch 58/100, Val Acc=0.9177, Val Loss=0.2992, lr=0.0100
[2025-04-27 22:51:18,972][train][INFO] - Epoch 60/100, Val Acc=0.8690, Val Loss=0.4133, lr=0.0100
[2025-04-27 22:51:20,225][train][INFO] - Epoch 60/100, Val Acc=0.8492, Val Loss=0.4647, lr=0.0100
[2025-04-27 22:51:22,186][train][INFO] - Epoch 58/100, Val Acc=0.9099, Val Loss=0.3147, lr=0.0100
[2025-04-27 22:51:26,891][train][INFO] - Epoch 59/100, Val Acc=0.9072, Val Loss=0.3165, lr=0.0100
[2025-04-27 22:51:30,022][train][INFO] - Epoch 59/100, Val Acc=0.9085, Val Loss=0.3354, lr=0.0100
[2025-04-27 22:51:34,567][train][INFO] - Epoch 59/100, Val Acc=0.9168, Val Loss=0.3111, lr=0.0100
[2025-04-27 22:51:37,882][train][INFO] - Epoch 61/100, Val Acc=0.9085, Val Loss=0.2869, lr=0.0010
[2025-04-27 22:51:39,072][train][INFO] - Epoch 61/100, Val Acc=0.8894, Val Loss=0.3311, lr=0.0010
[2025-04-27 22:51:40,630][train][INFO] - Epoch 59/100, Val Acc=0.9147, Val Loss=0.3006, lr=0.0100
[2025-04-27 22:51:45,425][train][INFO] - Epoch 60/100, Val Acc=0.9078, Val Loss=0.3224, lr=0.0100
[2025-04-27 22:51:48,743][train][INFO] - Epoch 60/100, Val Acc=0.9112, Val Loss=0.3244, lr=0.0100
[2025-04-27 22:51:52,775][train][INFO] - Epoch 60/100, Val Acc=0.9028, Val Loss=0.3505, lr=0.0100
[2025-04-27 22:51:56,796][train][INFO] - Epoch 62/100, Val Acc=0.9072, Val Loss=0.2859, lr=0.0010
[2025-04-27 22:51:57,697][train][INFO] - Epoch 62/100, Val Acc=0.8899, Val Loss=0.3261, lr=0.0010
[2025-04-27 22:51:59,005][train][INFO] - Epoch 60/100, Val Acc=0.9085, Val Loss=0.3208, lr=0.0100
[2025-04-27 22:52:04,183][train][INFO] - Epoch 61/100, Val Acc=0.9304, Val Loss=0.2414, lr=0.0010
[2025-04-27 22:52:07,407][train][INFO] - Epoch 61/100, Val Acc=0.9342, Val Loss=0.2379, lr=0.0010
[2025-04-27 22:52:11,269][train][INFO] - Epoch 61/100, Val Acc=0.9322, Val Loss=0.2465, lr=0.0010
[2025-04-27 22:52:15,845][train][INFO] - Epoch 63/100, Val Acc=0.9103, Val Loss=0.2828, lr=0.0010
[2025-04-27 22:52:16,343][train][INFO] - Epoch 63/100, Val Acc=0.8906, Val Loss=0.3240, lr=0.0010
[2025-04-27 22:52:17,592][train][INFO] - Epoch 61/100, Val Acc=0.9280, Val Loss=0.2459, lr=0.0010
[2025-04-27 22:52:22,905][train][INFO] - Epoch 62/100, Val Acc=0.9346, Val Loss=0.2343, lr=0.0010
[2025-04-27 22:52:26,311][train][INFO] - Epoch 62/100, Val Acc=0.9365, Val Loss=0.2361, lr=0.0010
[2025-04-27 22:52:29,817][train][INFO] - Epoch 62/100, Val Acc=0.9335, Val Loss=0.2427, lr=0.0010
[2025-04-27 22:52:34,851][train][INFO] - Epoch 64/100, Val Acc=0.9121, Val Loss=0.2807, lr=0.0010
[2025-04-27 22:52:35,347][train][INFO] - Epoch 64/100, Val Acc=0.8914, Val Loss=0.3185, lr=0.0010
[2025-04-27 22:52:35,992][train][INFO] - Epoch 62/100, Val Acc=0.9307, Val Loss=0.2397, lr=0.0010
[2025-04-27 22:52:41,651][train][INFO] - Epoch 63/100, Val Acc=0.9330, Val Loss=0.2363, lr=0.0010
[2025-04-27 22:52:45,132][train][INFO] - Epoch 63/100, Val Acc=0.9367, Val Loss=0.2339, lr=0.0010
[2025-04-27 22:52:48,268][train][INFO] - Epoch 63/100, Val Acc=0.9345, Val Loss=0.2418, lr=0.0010
[2025-04-27 22:52:53,954][train][INFO] - Epoch 65/100, Val Acc=0.9119, Val Loss=0.2830, lr=0.0010
[2025-04-27 22:52:54,064][train][INFO] - Epoch 65/100, Val Acc=0.8959, Val Loss=0.3188, lr=0.0010
[2025-04-27 22:52:54,470][train][INFO] - Epoch 63/100, Val Acc=0.9320, Val Loss=0.2376, lr=0.0010
[2025-04-27 22:53:00,118][train][INFO] - Epoch 64/100, Val Acc=0.9343, Val Loss=0.2420, lr=0.0010
[2025-04-27 22:53:04,148][train][INFO] - Epoch 64/100, Val Acc=0.9367, Val Loss=0.2398, lr=0.0010
[2025-04-27 22:53:06,740][train][INFO] - Epoch 64/100, Val Acc=0.9337, Val Loss=0.2461, lr=0.0010
[2025-04-27 22:53:13,106][train][INFO] - Epoch 66/100, Val Acc=0.9117, Val Loss=0.2806, lr=0.0010
[2025-04-27 22:53:13,139][train][INFO] - Epoch 66/100, Val Acc=0.8943, Val Loss=0.3194, lr=0.0010
[2025-04-27 22:53:13,240][train][INFO] - Epoch 64/100, Val Acc=0.9317, Val Loss=0.2397, lr=0.0010
[2025-04-27 22:53:18,648][train][INFO] - Epoch 65/100, Val Acc=0.9331, Val Loss=0.2427, lr=0.0010
[2025-04-27 22:53:23,042][train][INFO] - Epoch 65/100, Val Acc=0.9396, Val Loss=0.2345, lr=0.0010
[2025-04-27 22:53:25,019][train][INFO] - Epoch 65/100, Val Acc=0.9343, Val Loss=0.2475, lr=0.0010
[2025-04-27 22:53:31,815][train][INFO] - Epoch 65/100, Val Acc=0.9326, Val Loss=0.2429, lr=0.0010
[2025-04-27 22:53:32,090][train][INFO] - Epoch 67/100, Val Acc=0.8938, Val Loss=0.3204, lr=0.0010
[2025-04-27 22:53:32,136][train][INFO] - Epoch 67/100, Val Acc=0.9104, Val Loss=0.2823, lr=0.0010
[2025-04-27 22:53:37,050][train][INFO] - Epoch 66/100, Val Acc=0.9354, Val Loss=0.2409, lr=0.0010
[2025-04-27 22:53:41,936][train][INFO] - Epoch 66/100, Val Acc=0.9377, Val Loss=0.2402, lr=0.0010
[2025-04-27 22:53:43,359][train][INFO] - Epoch 66/100, Val Acc=0.9339, Val Loss=0.2482, lr=0.0010
[2025-04-27 22:53:50,366][train][INFO] - Epoch 66/100, Val Acc=0.9345, Val Loss=0.2407, lr=0.0010
[2025-04-27 22:53:51,207][train][INFO] - Epoch 68/100, Val Acc=0.9126, Val Loss=0.2820, lr=0.0010
[2025-04-27 22:53:51,326][train][INFO] - Epoch 68/100, Val Acc=0.8936, Val Loss=0.3203, lr=0.0010
[2025-04-27 22:53:55,565][train][INFO] - Epoch 67/100, Val Acc=0.9336, Val Loss=0.2449, lr=0.0010
[2025-04-27 22:54:00,793][train][INFO] - Epoch 67/100, Val Acc=0.9404, Val Loss=0.2364, lr=0.0010
[2025-04-27 22:54:01,931][train][INFO] - Epoch 67/100, Val Acc=0.9351, Val Loss=0.2465, lr=0.0010
[2025-04-27 22:54:08,925][train][INFO] - Epoch 67/100, Val Acc=0.9361, Val Loss=0.2412, lr=0.0010
[2025-04-27 22:54:10,223][train][INFO] - Epoch 69/100, Val Acc=0.9121, Val Loss=0.2855, lr=0.0010
[2025-04-27 22:54:10,230][train][INFO] - Epoch 69/100, Val Acc=0.8936, Val Loss=0.3185, lr=0.0010
[2025-04-27 22:54:14,117][train][INFO] - Epoch 68/100, Val Acc=0.9345, Val Loss=0.2449, lr=0.0010
[2025-04-27 22:54:19,648][train][INFO] - Epoch 68/100, Val Acc=0.9381, Val Loss=0.2376, lr=0.0010
[2025-04-27 22:54:20,450][train][INFO] - Epoch 68/100, Val Acc=0.9359, Val Loss=0.2446, lr=0.0010
[2025-04-27 22:54:27,575][train][INFO] - Epoch 68/100, Val Acc=0.9343, Val Loss=0.2435, lr=0.0010
[2025-04-27 22:54:29,152][train][INFO] - Epoch 70/100, Val Acc=0.9120, Val Loss=0.2797, lr=0.0010
[2025-04-27 22:54:29,296][train][INFO] - Epoch 70/100, Val Acc=0.8955, Val Loss=0.3217, lr=0.0010
[2025-04-27 22:54:32,688][train][INFO] - Epoch 69/100, Val Acc=0.9347, Val Loss=0.2426, lr=0.0010
[2025-04-27 22:54:38,448][train][INFO] - Epoch 69/100, Val Acc=0.9400, Val Loss=0.2389, lr=0.0010
[2025-04-27 22:54:38,915][train][INFO] - Epoch 69/100, Val Acc=0.9370, Val Loss=0.2446, lr=0.0010
[2025-04-27 22:54:46,011][train][INFO] - Epoch 69/100, Val Acc=0.9350, Val Loss=0.2428, lr=0.0010
[2025-04-27 22:54:48,386][train][INFO] - Epoch 71/100, Val Acc=0.9133, Val Loss=0.2818, lr=0.0010
[2025-04-27 22:54:48,485][train][INFO] - Epoch 71/100, Val Acc=0.8941, Val Loss=0.3193, lr=0.0010
[2025-04-27 22:54:51,410][train][INFO] - Epoch 70/100, Val Acc=0.9354, Val Loss=0.2464, lr=0.0010
[2025-04-27 22:54:56,984][train][INFO] - Epoch 70/100, Val Acc=0.9404, Val Loss=0.2403, lr=0.0010
[2025-04-27 22:54:57,223][train][INFO] - Epoch 70/100, Val Acc=0.9367, Val Loss=0.2441, lr=0.0010
[2025-04-27 22:55:04,420][train][INFO] - Epoch 70/100, Val Acc=0.9357, Val Loss=0.2435, lr=0.0010
[2025-04-27 22:55:07,436][train][INFO] - Epoch 72/100, Val Acc=0.9111, Val Loss=0.2817, lr=0.0010
[2025-04-27 22:55:07,584][train][INFO] - Epoch 72/100, Val Acc=0.8932, Val Loss=0.3208, lr=0.0010
[2025-04-27 22:55:09,849][train][INFO] - Epoch 71/100, Val Acc=0.9360, Val Loss=0.2468, lr=0.0010
[2025-04-27 22:55:15,785][train][INFO] - Epoch 71/100, Val Acc=0.9368, Val Loss=0.2459, lr=0.0010
[2025-04-27 22:55:15,830][train][INFO] - Epoch 71/100, Val Acc=0.9392, Val Loss=0.2427, lr=0.0010
[2025-04-27 22:55:22,717][train][INFO] - Epoch 71/100, Val Acc=0.9362, Val Loss=0.2460, lr=0.0010
[2025-04-27 22:55:26,386][train][INFO] - Epoch 73/100, Val Acc=0.9119, Val Loss=0.2844, lr=0.0010
[2025-04-27 22:55:26,712][train][INFO] - Epoch 73/100, Val Acc=0.8934, Val Loss=0.3206, lr=0.0010
[2025-04-27 22:55:28,259][train][INFO] - Epoch 72/100, Val Acc=0.9354, Val Loss=0.2484, lr=0.0010
[2025-04-27 22:55:34,127][train][INFO] - Epoch 72/100, Val Acc=0.9385, Val Loss=0.2459, lr=0.0010
[2025-04-27 22:55:34,732][train][INFO] - Epoch 72/100, Val Acc=0.9400, Val Loss=0.2408, lr=0.0010
[2025-04-27 22:55:41,209][train][INFO] - Epoch 72/100, Val Acc=0.9350, Val Loss=0.2435, lr=0.0010
[2025-04-27 22:55:45,428][train][INFO] - Epoch 74/100, Val Acc=0.9106, Val Loss=0.2867, lr=0.0010
[2025-04-27 22:55:45,683][train][INFO] - Epoch 74/100, Val Acc=0.8957, Val Loss=0.3186, lr=0.0010
[2025-04-27 22:55:46,835][train][INFO] - Epoch 73/100, Val Acc=0.9352, Val Loss=0.2507, lr=0.0010
[2025-04-27 22:55:52,362][train][INFO] - Epoch 73/100, Val Acc=0.9374, Val Loss=0.2496, lr=0.0010
[2025-04-27 22:55:53,556][train][INFO] - Epoch 73/100, Val Acc=0.9391, Val Loss=0.2436, lr=0.0010
[2025-04-27 22:55:59,979][train][INFO] - Epoch 73/100, Val Acc=0.9357, Val Loss=0.2454, lr=0.0010
[2025-04-27 22:56:04,541][train][INFO] - Epoch 75/100, Val Acc=0.9099, Val Loss=0.2861, lr=0.0010
[2025-04-27 22:56:04,620][train][INFO] - Epoch 75/100, Val Acc=0.8940, Val Loss=0.3191, lr=0.0010
[2025-04-27 22:56:05,455][train][INFO] - Epoch 74/100, Val Acc=0.9366, Val Loss=0.2451, lr=0.0010
[2025-04-27 22:56:10,796][train][INFO] - Epoch 74/100, Val Acc=0.9390, Val Loss=0.2497, lr=0.0010
[2025-04-27 22:56:12,437][train][INFO] - Epoch 74/100, Val Acc=0.9405, Val Loss=0.2426, lr=0.0010
[2025-04-27 22:56:18,611][train][INFO] - Epoch 74/100, Val Acc=0.9359, Val Loss=0.2435, lr=0.0010
[2025-04-27 22:56:23,590][train][INFO] - Epoch 76/100, Val Acc=0.9103, Val Loss=0.2853, lr=0.0010
[2025-04-27 22:56:23,706][train][INFO] - Epoch 76/100, Val Acc=0.8953, Val Loss=0.3192, lr=0.0010
[2025-04-27 22:56:24,061][train][INFO] - Epoch 75/100, Val Acc=0.9362, Val Loss=0.2490, lr=0.0010
[2025-04-27 22:56:29,206][train][INFO] - Epoch 75/100, Val Acc=0.9401, Val Loss=0.2483, lr=0.0010
[2025-04-27 22:56:31,346][train][INFO] - Epoch 75/100, Val Acc=0.9398, Val Loss=0.2401, lr=0.0010
[2025-04-27 22:56:37,424][train][INFO] - Epoch 75/100, Val Acc=0.9378, Val Loss=0.2432, lr=0.0010
[2025-04-27 22:56:42,745][train][INFO] - Epoch 76/100, Val Acc=0.9369, Val Loss=0.2476, lr=0.0010
[2025-04-27 22:56:42,809][train][INFO] - Epoch 77/100, Val Acc=0.9098, Val Loss=0.2861, lr=0.0010
[2025-04-27 22:56:42,862][train][INFO] - Epoch 77/100, Val Acc=0.8952, Val Loss=0.3193, lr=0.0010
[2025-04-27 22:56:47,647][train][INFO] - Epoch 76/100, Val Acc=0.9389, Val Loss=0.2467, lr=0.0010
[2025-04-27 22:56:50,126][train][INFO] - Epoch 76/100, Val Acc=0.9404, Val Loss=0.2396, lr=0.0010
[2025-04-27 22:56:55,892][train][INFO] - Epoch 76/100, Val Acc=0.9376, Val Loss=0.2444, lr=0.0010
[2025-04-27 22:57:01,074][train][INFO] - Epoch 77/100, Val Acc=0.9366, Val Loss=0.2523, lr=0.0010
[2025-04-27 22:57:01,982][train][INFO] - Epoch 78/100, Val Acc=0.8930, Val Loss=0.3272, lr=0.0010
[2025-04-27 22:57:02,085][train][INFO] - Epoch 78/100, Val Acc=0.9131, Val Loss=0.2849, lr=0.0010
[2025-04-27 22:57:05,865][train][INFO] - Epoch 77/100, Val Acc=0.9387, Val Loss=0.2496, lr=0.0010
[2025-04-27 22:57:08,913][train][INFO] - Epoch 77/100, Val Acc=0.9408, Val Loss=0.2415, lr=0.0010
[2025-04-27 22:57:14,218][train][INFO] - Epoch 77/100, Val Acc=0.9373, Val Loss=0.2470, lr=0.0010
[2025-04-27 22:57:19,636][train][INFO] - Epoch 78/100, Val Acc=0.9368, Val Loss=0.2490, lr=0.0010
[2025-04-27 22:57:21,153][train][INFO] - Epoch 79/100, Val Acc=0.8931, Val Loss=0.3254, lr=0.0010
[2025-04-27 22:57:21,289][train][INFO] - Epoch 79/100, Val Acc=0.9116, Val Loss=0.2881, lr=0.0010
[2025-04-27 22:57:24,173][train][INFO] - Epoch 78/100, Val Acc=0.9398, Val Loss=0.2490, lr=0.0010
[2025-04-27 22:57:27,814][train][INFO] - Epoch 78/100, Val Acc=0.9411, Val Loss=0.2407, lr=0.0010
[2025-04-27 22:57:32,525][train][INFO] - Epoch 78/100, Val Acc=0.9375, Val Loss=0.2467, lr=0.0010
[2025-04-27 22:57:38,268][train][INFO] - Epoch 79/100, Val Acc=0.9373, Val Loss=0.2537, lr=0.0010
[2025-04-27 22:57:39,934][train][INFO] - Epoch 80/100, Val Acc=0.8922, Val Loss=0.3293, lr=0.0010
[2025-04-27 22:57:40,298][train][INFO] - Epoch 80/100, Val Acc=0.9084, Val Loss=0.2942, lr=0.0010
[2025-04-27 22:57:42,765][train][INFO] - Epoch 79/100, Val Acc=0.9392, Val Loss=0.2481, lr=0.0010
[2025-04-27 22:57:46,493][train][INFO] - Epoch 79/100, Val Acc=0.9411, Val Loss=0.2418, lr=0.0010
[2025-04-27 22:57:50,987][train][INFO] - Epoch 79/100, Val Acc=0.9363, Val Loss=0.2486, lr=0.0010
[2025-04-27 22:57:57,088][train][INFO] - Epoch 80/100, Val Acc=0.9368, Val Loss=0.2505, lr=0.0010
[2025-04-27 22:57:58,694][train][INFO] - Epoch 81/100, Val Acc=0.8950, Val Loss=0.3234, lr=0.0010
[2025-04-27 22:57:59,452][train][INFO] - Epoch 81/100, Val Acc=0.9116, Val Loss=0.2864, lr=0.0010
[2025-04-27 22:58:01,076][train][INFO] - Epoch 80/100, Val Acc=0.9412, Val Loss=0.2508, lr=0.0010
[2025-04-27 22:58:05,476][train][INFO] - Epoch 80/100, Val Acc=0.9421, Val Loss=0.2391, lr=0.0010
[2025-04-27 22:58:09,706][train][INFO] - Epoch 80/100, Val Acc=0.9371, Val Loss=0.2451, lr=0.0010
[2025-04-27 22:58:15,682][train][INFO] - Epoch 81/100, Val Acc=0.9360, Val Loss=0.2533, lr=0.0010
[2025-04-27 22:58:17,424][train][INFO] - Epoch 82/100, Val Acc=0.8927, Val Loss=0.3260, lr=0.0010
[2025-04-27 22:58:18,339][train][INFO] - Epoch 82/100, Val Acc=0.9105, Val Loss=0.2893, lr=0.0010
[2025-04-27 22:58:19,507][train][INFO] - Epoch 81/100, Val Acc=0.9379, Val Loss=0.2535, lr=0.0010
[2025-04-27 22:58:24,230][train][INFO] - Epoch 81/100, Val Acc=0.9412, Val Loss=0.2473, lr=0.0010
[2025-04-27 22:58:28,269][train][INFO] - Epoch 81/100, Val Acc=0.9366, Val Loss=0.2473, lr=0.0010
[2025-04-27 22:58:34,427][train][INFO] - Epoch 82/100, Val Acc=0.9365, Val Loss=0.2517, lr=0.0010
[2025-04-27 22:58:36,270][train][INFO] - Epoch 83/100, Val Acc=0.8967, Val Loss=0.3215, lr=0.0010
[2025-04-27 22:58:37,153][train][INFO] - Epoch 83/100, Val Acc=0.9112, Val Loss=0.2928, lr=0.0010
[2025-04-27 22:58:38,040][train][INFO] - Epoch 82/100, Val Acc=0.9404, Val Loss=0.2499, lr=0.0010
[2025-04-27 22:58:43,112][train][INFO] - Epoch 82/100, Val Acc=0.9398, Val Loss=0.2452, lr=0.0010
[2025-04-27 22:58:47,009][train][INFO] - Epoch 82/100, Val Acc=0.9393, Val Loss=0.2489, lr=0.0010
[2025-04-27 22:58:53,130][train][INFO] - Epoch 83/100, Val Acc=0.9373, Val Loss=0.2530, lr=0.0010
[2025-04-27 22:58:55,208][train][INFO] - Epoch 84/100, Val Acc=0.8959, Val Loss=0.3239, lr=0.0010
[2025-04-27 22:58:56,139][train][INFO] - Epoch 84/100, Val Acc=0.9104, Val Loss=0.2911, lr=0.0010
[2025-04-27 22:58:56,538][train][INFO] - Epoch 83/100, Val Acc=0.9394, Val Loss=0.2487, lr=0.0010
[2025-04-27 22:59:01,903][train][INFO] - Epoch 83/100, Val Acc=0.9412, Val Loss=0.2411, lr=0.0010
[2025-04-27 22:59:05,620][train][INFO] - Epoch 83/100, Val Acc=0.9373, Val Loss=0.2518, lr=0.0010
[2025-04-27 22:59:11,739][train][INFO] - Epoch 84/100, Val Acc=0.9382, Val Loss=0.2503, lr=0.0010
[2025-04-27 22:59:13,911][train][INFO] - Epoch 85/100, Val Acc=0.8976, Val Loss=0.3190, lr=0.0010
[2025-04-27 22:59:15,101][train][INFO] - Epoch 85/100, Val Acc=0.9092, Val Loss=0.2930, lr=0.0010
[2025-04-27 22:59:15,103][train][INFO] - Epoch 84/100, Val Acc=0.9383, Val Loss=0.2504, lr=0.0010
[2025-04-27 22:59:20,714][train][INFO] - Epoch 84/100, Val Acc=0.9414, Val Loss=0.2423, lr=0.0010
[2025-04-27 22:59:24,019][train][INFO] - Epoch 84/100, Val Acc=0.9367, Val Loss=0.2504, lr=0.0010
[2025-04-27 22:59:30,466][train][INFO] - Epoch 85/100, Val Acc=0.9382, Val Loss=0.2522, lr=0.0010
[2025-04-27 22:59:32,683][train][INFO] - Epoch 86/100, Val Acc=0.8937, Val Loss=0.3255, lr=0.0010
[2025-04-27 22:59:33,421][train][INFO] - Epoch 85/100, Val Acc=0.9402, Val Loss=0.2500, lr=0.0010
[2025-04-27 22:59:33,793][train][INFO] - Epoch 86/100, Val Acc=0.9071, Val Loss=0.3012, lr=0.0010
[2025-04-27 22:59:39,522][train][INFO] - Epoch 85/100, Val Acc=0.9415, Val Loss=0.2399, lr=0.0010
[2025-04-27 22:59:42,609][train][INFO] - Epoch 85/100, Val Acc=0.9381, Val Loss=0.2492, lr=0.0010
[2025-04-27 22:59:49,095][train][INFO] - Epoch 86/100, Val Acc=0.9382, Val Loss=0.2539, lr=0.0010
[2025-04-27 22:59:51,715][train][INFO] - Epoch 87/100, Val Acc=0.8977, Val Loss=0.3219, lr=0.0010
[2025-04-27 22:59:51,926][train][INFO] - Epoch 86/100, Val Acc=0.9377, Val Loss=0.2537, lr=0.0010
[2025-04-27 22:59:52,604][train][INFO] - Epoch 87/100, Val Acc=0.9109, Val Loss=0.2958, lr=0.0010
[2025-04-27 22:59:58,226][train][INFO] - Epoch 86/100, Val Acc=0.9411, Val Loss=0.2446, lr=0.0010
[2025-04-27 23:00:01,197][train][INFO] - Epoch 86/100, Val Acc=0.9384, Val Loss=0.2472, lr=0.0010
[2025-04-27 23:00:07,899][train][INFO] - Epoch 87/100, Val Acc=0.9368, Val Loss=0.2526, lr=0.0010
[2025-04-27 23:00:10,167][train][INFO] - Epoch 87/100, Val Acc=0.9395, Val Loss=0.2532, lr=0.0010
[2025-04-27 23:00:10,501][train][INFO] - Epoch 88/100, Val Acc=0.8951, Val Loss=0.3232, lr=0.0010
[2025-04-27 23:00:11,522][train][INFO] - Epoch 88/100, Val Acc=0.9112, Val Loss=0.2892, lr=0.0010
[2025-04-27 23:00:17,216][train][INFO] - Epoch 87/100, Val Acc=0.9408, Val Loss=0.2444, lr=0.0010
[2025-04-27 23:00:19,573][train][INFO] - Epoch 87/100, Val Acc=0.9391, Val Loss=0.2519, lr=0.0010
[2025-04-27 23:00:26,613][train][INFO] - Epoch 88/100, Val Acc=0.9399, Val Loss=0.2507, lr=0.0010
[2025-04-27 23:00:28,501][train][INFO] - Epoch 88/100, Val Acc=0.9400, Val Loss=0.2555, lr=0.0010
[2025-04-27 23:00:29,469][train][INFO] - Epoch 89/100, Val Acc=0.8940, Val Loss=0.3257, lr=0.0010
[2025-04-27 23:00:30,089][train][INFO] - Epoch 89/100, Val Acc=0.9111, Val Loss=0.2944, lr=0.0010
[2025-04-27 23:00:36,066][train][INFO] - Epoch 88/100, Val Acc=0.9416, Val Loss=0.2424, lr=0.0010
[2025-04-27 23:00:38,058][train][INFO] - Epoch 88/100, Val Acc=0.9378, Val Loss=0.2532, lr=0.0010
[2025-04-27 23:00:45,385][train][INFO] - Epoch 89/100, Val Acc=0.9388, Val Loss=0.2517, lr=0.0010
[2025-04-27 23:00:46,786][train][INFO] - Epoch 89/100, Val Acc=0.9383, Val Loss=0.2555, lr=0.0010
[2025-04-27 23:00:48,168][train][INFO] - Epoch 90/100, Val Acc=0.8965, Val Loss=0.3231, lr=0.0010
[2025-04-27 23:00:49,030][train][INFO] - Epoch 90/100, Val Acc=0.9122, Val Loss=0.2943, lr=0.0010
[2025-04-27 23:00:54,798][train][INFO] - Epoch 89/100, Val Acc=0.9401, Val Loss=0.2443, lr=0.0010
[2025-04-27 23:00:56,306][train][INFO] - Epoch 89/100, Val Acc=0.9384, Val Loss=0.2510, lr=0.0010
[2025-04-27 23:01:04,147][train][INFO] - Epoch 90/100, Val Acc=0.9386, Val Loss=0.2561, lr=0.0010
[2025-04-27 23:01:05,185][train][INFO] - Epoch 90/100, Val Acc=0.9391, Val Loss=0.2554, lr=0.0010
[2025-04-27 23:01:07,174][train][INFO] - Epoch 91/100, Val Acc=0.8986, Val Loss=0.3186, lr=0.0001
[2025-04-27 23:01:08,082][train][INFO] - Epoch 91/100, Val Acc=0.9129, Val Loss=0.2910, lr=0.0001
[2025-04-27 23:01:13,645][train][INFO] - Epoch 90/100, Val Acc=0.9408, Val Loss=0.2415, lr=0.0010
[2025-04-27 23:01:14,636][train][INFO] - Epoch 90/100, Val Acc=0.9383, Val Loss=0.2509, lr=0.0010
[2025-04-27 23:01:22,898][train][INFO] - Epoch 91/100, Val Acc=0.9384, Val Loss=0.2528, lr=0.0001
[2025-04-27 23:01:23,321][train][INFO] - Epoch 91/100, Val Acc=0.9397, Val Loss=0.2542, lr=0.0001
[2025-04-27 23:01:26,065][train][INFO] - Epoch 92/100, Val Acc=0.8980, Val Loss=0.3175, lr=0.0001
[2025-04-27 23:01:26,993][train][INFO] - Epoch 92/100, Val Acc=0.9123, Val Loss=0.2909, lr=0.0001
[2025-04-27 23:01:32,428][train][INFO] - Epoch 91/100, Val Acc=0.9417, Val Loss=0.2404, lr=0.0001
[2025-04-27 23:01:32,991][train][INFO] - Epoch 91/100, Val Acc=0.9376, Val Loss=0.2515, lr=0.0001
[2025-04-27 23:01:41,574][train][INFO] - Epoch 92/100, Val Acc=0.9393, Val Loss=0.2539, lr=0.0001
[2025-04-27 23:01:41,669][train][INFO] - Epoch 92/100, Val Acc=0.9399, Val Loss=0.2549, lr=0.0001
[2025-04-27 23:01:45,067][train][INFO] - Epoch 93/100, Val Acc=0.8988, Val Loss=0.3184, lr=0.0001
[2025-04-27 23:01:45,817][train][INFO] - Epoch 93/100, Val Acc=0.9124, Val Loss=0.2902, lr=0.0001
[2025-04-27 23:01:51,010][train][INFO] - Epoch 92/100, Val Acc=0.9414, Val Loss=0.2416, lr=0.0001
[2025-04-27 23:01:51,489][train][INFO] - Epoch 92/100, Val Acc=0.9383, Val Loss=0.2505, lr=0.0001
[2025-04-27 23:02:00,027][train][INFO] - Epoch 93/100, Val Acc=0.9392, Val Loss=0.2554, lr=0.0001
[2025-04-27 23:02:00,276][train][INFO] - Epoch 93/100, Val Acc=0.9390, Val Loss=0.2535, lr=0.0001
[2025-04-27 23:02:03,613][train][INFO] - Epoch 94/100, Val Acc=0.8973, Val Loss=0.3194, lr=0.0001
[2025-04-27 23:02:04,609][train][INFO] - Epoch 94/100, Val Acc=0.9098, Val Loss=0.2910, lr=0.0001
[2025-04-27 23:02:09,707][train][INFO] - Epoch 93/100, Val Acc=0.9413, Val Loss=0.2414, lr=0.0001
[2025-04-27 23:02:10,168][train][INFO] - Epoch 93/100, Val Acc=0.9382, Val Loss=0.2524, lr=0.0001
[2025-04-27 23:02:18,370][train][INFO] - Epoch 94/100, Val Acc=0.9399, Val Loss=0.2571, lr=0.0001
[2025-04-27 23:02:18,872][train][INFO] - Epoch 94/100, Val Acc=0.9384, Val Loss=0.2561, lr=0.0001
[2025-04-27 23:02:22,432][train][INFO] - Epoch 95/100, Val Acc=0.8982, Val Loss=0.3174, lr=0.0001
[2025-04-27 23:02:23,388][train][INFO] - Epoch 95/100, Val Acc=0.9111, Val Loss=0.2934, lr=0.0001
[2025-04-27 23:02:28,485][train][INFO] - Epoch 94/100, Val Acc=0.9412, Val Loss=0.2438, lr=0.0001
[2025-04-27 23:02:28,593][train][INFO] - Epoch 94/100, Val Acc=0.9386, Val Loss=0.2522, lr=0.0001
[2025-04-27 23:02:36,778][train][INFO] - Epoch 95/100, Val Acc=0.9390, Val Loss=0.2583, lr=0.0001
[2025-04-27 23:02:37,342][train][INFO] - Epoch 95/100, Val Acc=0.9374, Val Loss=0.2588, lr=0.0001
[2025-04-27 23:02:41,296][train][INFO] - Epoch 96/100, Val Acc=0.8991, Val Loss=0.3179, lr=0.0001
[2025-04-27 23:02:42,326][train][INFO] - Epoch 96/100, Val Acc=0.9113, Val Loss=0.2900, lr=0.0001
[2025-04-27 23:02:47,086][train][INFO] - Epoch 95/100, Val Acc=0.9419, Val Loss=0.2446, lr=0.0001
[2025-04-27 23:02:47,171][train][INFO] - Epoch 95/100, Val Acc=0.9386, Val Loss=0.2543, lr=0.0001
[2025-04-27 23:02:55,044][train][INFO] - Epoch 96/100, Val Acc=0.9399, Val Loss=0.2551, lr=0.0001
[2025-04-27 23:02:55,968][train][INFO] - Epoch 96/100, Val Acc=0.9392, Val Loss=0.2540, lr=0.0001
[2025-04-27 23:03:00,155][train][INFO] - Epoch 97/100, Val Acc=0.8982, Val Loss=0.3167, lr=0.0001
[2025-04-27 23:03:01,103][train][INFO] - Epoch 97/100, Val Acc=0.9107, Val Loss=0.2915, lr=0.0001
[2025-04-27 23:03:05,626][train][INFO] - Epoch 96/100, Val Acc=0.9377, Val Loss=0.2530, lr=0.0001
[2025-04-27 23:03:05,950][train][INFO] - Epoch 96/100, Val Acc=0.9422, Val Loss=0.2422, lr=0.0001
[2025-04-27 23:03:13,567][train][INFO] - Epoch 97/100, Val Acc=0.9406, Val Loss=0.2541, lr=0.0001
[2025-04-27 23:03:14,780][train][INFO] - Epoch 97/100, Val Acc=0.9389, Val Loss=0.2537, lr=0.0001
[2025-04-27 23:03:18,894][train][INFO] - Epoch 98/100, Val Acc=0.8983, Val Loss=0.3172, lr=0.0001
[2025-04-27 23:03:19,832][train][INFO] - Epoch 98/100, Val Acc=0.9105, Val Loss=0.2927, lr=0.0001
[2025-04-27 23:03:24,254][train][INFO] - Epoch 97/100, Val Acc=0.9392, Val Loss=0.2501, lr=0.0001
[2025-04-27 23:03:24,787][train][INFO] - Epoch 97/100, Val Acc=0.9418, Val Loss=0.2416, lr=0.0001
[2025-04-27 23:03:32,086][train][INFO] - Epoch 98/100, Val Acc=0.9403, Val Loss=0.2544, lr=0.0001
[2025-04-27 23:03:33,297][train][INFO] - Epoch 98/100, Val Acc=0.9382, Val Loss=0.2530, lr=0.0001
[2025-04-27 23:03:37,625][train][INFO] - Epoch 99/100, Val Acc=0.8981, Val Loss=0.3168, lr=0.0001
[2025-04-27 23:03:38,838][train][INFO] - Epoch 99/100, Val Acc=0.9108, Val Loss=0.2908, lr=0.0001
[2025-04-27 23:03:42,538][train][INFO] - Epoch 98/100, Val Acc=0.9378, Val Loss=0.2509, lr=0.0001
[2025-04-27 23:03:43,646][train][INFO] - Epoch 98/100, Val Acc=0.9419, Val Loss=0.2419, lr=0.0001
[2025-04-27 23:03:50,568][train][INFO] - Epoch 99/100, Val Acc=0.9392, Val Loss=0.2544, lr=0.0001
[2025-04-27 23:03:51,918][train][INFO] - Epoch 99/100, Val Acc=0.9394, Val Loss=0.2524, lr=0.0001
[2025-04-27 23:03:56,229][train][INFO] - Epoch 100/100, Val Acc=0.8977, Val Loss=0.3173, lr=0.0001
[2025-04-27 23:03:57,508][train][INFO] - Epoch 100/100, Val Acc=0.9107, Val Loss=0.2907, lr=0.0001
[2025-04-27 23:04:01,136][train][INFO] - Epoch 99/100, Val Acc=0.9387, Val Loss=0.2492, lr=0.0001
[2025-04-27 23:04:01,775][train][INFO] - After training : Train Acc=0.9534  Val Acc=0.8991
[2025-04-27 23:04:01,784][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:04:02,607][train][INFO] - Epoch 99/100, Val Acc=0.9419, Val Loss=0.2420, lr=0.0001
[2025-04-27 23:04:03,100][train][INFO] - After training : Train Acc=0.9697  Val Acc=0.9133
[2025-04-27 23:04:03,110][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:04:09,209][train][INFO] - Epoch 100/100, Val Acc=0.9398, Val Loss=0.2544, lr=0.0001
[2025-04-27 23:04:10,697][train][INFO] - Epoch 100/100, Val Acc=0.9386, Val Loss=0.2538, lr=0.0001
[2025-04-27 23:04:15,045][train][INFO] - After training : Train Acc=0.9996  Val Acc=0.9412
[2025-04-27 23:04:15,053][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:04:16,377][train][INFO] - After training : Train Acc=0.9998  Val Acc=0.9399
[2025-04-27 23:04:16,392][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:04:18,362][train][INFO] - Epoch 100/100, Val Acc=0.9381, Val Loss=0.2523, lr=0.0001
[2025-04-27 23:04:20,110][train][INFO] - Epoch 100/100, Val Acc=0.9425, Val Loss=0.2436, lr=0.0001
[2025-04-27 23:04:24,490][train][INFO] - After training : Train Acc=0.9993  Val Acc=0.9393
[2025-04-27 23:04:24,497][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:04:26,258][train][INFO] - After training : Train Acc=0.9998  Val Acc=0.9425
[2025-04-27 23:04:26,267][Visualize acc speed up curve][INFO] - Start visualizing
[2025-04-27 23:05:40,581][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:05:42,145][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:05:45,028][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:05:46,128][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:05:56,022][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:06:01,142][Visualize acc speed up curve][INFO] - Model 1/2 visualized
[2025-04-27 23:07:16,926][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:07:17,478][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:07:21,965][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:07:22,462][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:07:35,920][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:07:36,463][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:07:41,488][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:07:41,958][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:08:39,861][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:08:40,637][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:08:47,851][Visualize acc speed up curve][INFO] - Model 2/2 visualized
[2025-04-27 23:08:48,307][Visualize acc speed up curve][INFO] - End visualizing
[2025-04-27 23:19:31,633][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 25
- 30
- 35
- 40

[2025-04-27 23:19:31,788][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:19:31,788][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:19:31,788][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:19:42,453][train][INFO] - Before training : Train Acc=0.1079  Val Acc=0.1102
[2025-04-27 23:19:49,899][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Newton
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 45
- 50
- 55
- 60

[2025-04-27 23:19:50,045][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:19:50,045][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:19:50,045][get_dataset_model_loader][INFO] - ==================================================
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:19:57,202][train][INFO] - Epoch 1/100, Val Acc=0.8452, Val Loss=0.4773, lr=0.0100
[2025-04-27 23:20:01,847][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 23:20:14,925][train][INFO] - Epoch 2/100, Val Acc=0.8392, Val Loss=0.5032, lr=0.0100
[2025-04-27 23:20:21,019][train][INFO] - Epoch 1/100, Val Acc=0.6440, Val Loss=1.1414, lr=0.0100
[2025-04-27 23:20:33,326][train][INFO] - Epoch 3/100, Val Acc=0.8558, Val Loss=0.4578, lr=0.0100
[2025-04-27 23:20:39,550][train][INFO] - Epoch 2/100, Val Acc=0.7350, Val Loss=0.8146, lr=0.0100
[2025-04-27 23:20:51,409][train][INFO] - Epoch 4/100, Val Acc=0.8554, Val Loss=0.4680, lr=0.0100
[2025-04-27 23:20:56,164][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Oli
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 1
- 2
- 3
- 4
- 5

[2025-04-27 23:20:56,320][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:20:56,320][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:20:56,320][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 23:20:58,262][train][INFO] - Epoch 3/100, Val Acc=0.7976, Val Loss=0.6230, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:21:05,687][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Oli
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 6
- 7
- 8
- 9
- 10

[2025-04-27 23:21:05,835][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:21:05,835][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:21:05,835][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 23:21:07,496][train][INFO] - Before training : Train Acc=0.9049  Val Acc=0.8664
[2025-04-27 23:21:10,332][train][INFO] - Epoch 5/100, Val Acc=0.8911, Val Loss=0.3452, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:21:17,161][train][INFO] - Epoch 4/100, Val Acc=0.8024, Val Loss=0.6024, lr=0.0100
[2025-04-27 23:21:18,222][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 23:21:24,230][train][INFO] - Epoch 1/100, Val Acc=0.9001, Val Loss=0.3412, lr=0.0100
[2025-04-27 23:21:29,333][train][INFO] - Epoch 6/100, Val Acc=0.8875, Val Loss=0.3679, lr=0.0100
[2025-04-27 23:21:36,228][train][INFO] - Epoch 5/100, Val Acc=0.8166, Val Loss=0.5574, lr=0.0100
[2025-04-27 23:21:37,029][train][INFO] - Epoch 1/100, Val Acc=0.3612, Val Loss=2.0154, lr=0.0100
[2025-04-27 23:21:42,825][train][INFO] - Epoch 2/100, Val Acc=0.8976, Val Loss=0.3562, lr=0.0100
[2025-04-27 23:21:47,617][train][INFO] - Epoch 7/100, Val Acc=0.9006, Val Loss=0.3216, lr=0.0100
[2025-04-27 23:21:49,054][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 20
- 24
- 28
- 32
- 36

[2025-04-27 23:21:49,203][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:21:49,203][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:21:49,203][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 23:21:55,193][train][INFO] - Epoch 6/100, Val Acc=0.8654, Val Loss=0.4015, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:21:55,859][train][INFO] - Epoch 2/100, Val Acc=0.5874, Val Loss=1.3031, lr=0.0100
[2025-04-27 23:22:00,936][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 23:22:01,697][train][INFO] - Epoch 3/100, Val Acc=0.9095, Val Loss=0.3185, lr=0.0100
[2025-04-27 23:22:06,175][root][INFO] - 

task:
  _recursive_: true
  model_name: resnet56
  task_name: ${task.model_name}_on_${task.dataset.dataset_name}
  dataset:
    dataset_name: CIFAR10
    num_workers: 4
    big_batch: 500
    small_batch: 128
    dataset_path: ../dataset/${task.dataset.dataset_name}
    seed: ${seed}
  big_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.big_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  small_batch_dataset:
    dataset_name: ${task.dataset.dataset_name}
    batch_size: ${task.dataset.small_batch}
    num_workers: ${task.dataset.num_workers}
    dataset_path: ${task.dataset.dataset_path}
    seed: ${seed}
  dataset_model:
    dataset_model_name: resnet56_on_CIFAR10
    train_split: 0.8
    dataset_model_path: ../dataset_model/${task.dataset_model.dataset_model_name}_level_${level}
    seed: ${seed}
  meta_train:
    epochs: 100
    lr: 0.001
    lr_decay_milestones: '5'
    lr_decay_gamma: 0.1
    weight_decay: 0.0005
    method: ${method}
    pruner_reg:
    - 10
    - 10
    save_every_epoch: 1
    warm_up: 0
    level: ${level}
    model_name: ${task.model_name}
    dataset_name: ${task.dataset.dataset_name}
    save_path: save/metanetwork/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    use_meta_eval: false
    meta_eval:
      speed_up_threshold: 0.925
      epochs: ${task.pruning.finetune.after_metanetwork.epochs}
      lr: ${task.pruning.finetune.after_metanetwork.lr}
      lr_decay_milestones: ${task.pruning.finetune.after_metanetwork.lr_decay_milestones}
      lr_decay_gamma: ${task.pruning.finetune.after_metanetwork.lr_decay_gamma}
      weight_decay: ${task.pruning.finetune.after_metanetwork.weight_decay}
  metanetwork:
    '0':
      _target_: nn.GNN.MyGNN
      _recursive_: true
      num_layer: 8
      hiddim: 64
      in_node_dim: 8
      in_edge_dim: 9
      node_res_ratio: 0.01
      edge_res_ratio: 0.01
  visualize:
    max_speed_up: 5.0
    marker: o
    save_path: save/visualize/${task.dataset_model.dataset_model_name}/${name}/level_${level}/
    ylim:
    - 0.0
    - 1.0
  pruning:
    pruning_index: 2.86
    level: ${level}
    method: ${method}
    finetune:
      after_pruning:
        epochs: 140
        lr: 0.01
        lr_decay_milestones: 80,120
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
      after_metanetwork:
        epochs: 100
        lr: 0.01
        lr_decay_milestones: 60,90
        lr_decay_gamma: 0.1
        weight_decay: 0.0005
__recursive_: true
name: Paris
level: 0
run: visualize
method: group_sl
log: true
reproduce_index: 1
seed: 7
index:
- 42
- 46
- 50
- 54
- 56

[2025-04-27 23:22:06,329][get_dataset_model_loader][INFO] - train dataset model size: 8
[2025-04-27 23:22:06,329][get_dataset_model_loader][INFO] - val dataset model size: 2
[2025-04-27 23:22:06,329][get_dataset_model_loader][INFO] - ==================================================
[2025-04-27 23:22:06,725][train][INFO] - Epoch 8/100, Val Acc=0.9038, Val Loss=0.2995, lr=0.0100
[2025-04-27 23:22:14,223][train][INFO] - Epoch 7/100, Val Acc=0.8563, Val Loss=0.4223, lr=0.0100
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[2025-04-27 23:22:14,782][train][INFO] - Epoch 1/100, Val Acc=0.8299, Val Loss=0.5144, lr=0.0100
[2025-04-27 23:22:14,802][train][INFO] - Epoch 3/100, Val Acc=0.6912, Val Loss=0.9149, lr=0.0100
[2025-04-27 23:22:19,884][train][INFO] - Before training : Train Acc=0.1000  Val Acc=0.1000
[2025-04-27 23:22:20,600][train][INFO] - Epoch 4/100, Val Acc=0.9110, Val Loss=0.3064, lr=0.0100
[2025-04-27 23:22:25,473][train][INFO] - Epoch 9/100, Val Acc=0.8982, Val Loss=0.3331, lr=0.0100
[2025-04-27 23:22:32,173][train][INFO] - Epoch 2/100, Val Acc=0.8536, Val Loss=0.4424, lr=0.0100
[2025-04-27 23:22:33,254][train][INFO] - Epoch 8/100, Val Acc=0.8484, Val Loss=0.4512, lr=0.0100
[2025-04-27 23:22:33,370][train][INFO] - Epoch 4/100, Val Acc=0.7266, Val Loss=0.8529, lr=0.0100
[2025-04-27 23:22:38,804][train][INFO] - Epoch 1/100, Val Acc=0.6613, Val Loss=1.0028, lr=0.0100
[2025-04-27 23:22:39,452][train][INFO] - Epoch 5/100, Val Acc=0.9146, Val Loss=0.2961, lr=0.0100
